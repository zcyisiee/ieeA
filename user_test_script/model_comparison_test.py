#!/usr/bin/env python3
"""
æ¨¡å‹ç¿»è¯‘å¯¹æ¯”æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼šè°ƒç”¨ å¯¹æ¯”æ¨¡å‹.yaml ä¸­çš„ 12 ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨ ~/.ieeA ä¸‹çš„å®Œæ•´é…ç½®
     ï¼ˆcustom_system_promptã€glossaryã€examplesï¼‰ï¼Œç¿»è¯‘ source æ–‡æœ¬ï¼Œ
     ç»“æœä»¥ JSON æ ¼å¼ä¿å­˜åˆ°å½“å‰ç›®å½•ã€‚

ç”¨æ³•ï¼špython model_comparison_test.py
"""

import asyncio
import httpx
import yaml
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any


# ============================================================================
# ç¡¬ç¼–ç æ ¼å¼è§„åˆ™ â€” ä¸é¡¹ç›® src/ieeA/translator/prompts.py ä¿æŒä¸¥æ ¼ä¸€è‡´
# ============================================================================
FORMAT_RULES = """## æ ¼å¼è§„åˆ™
ç¡¬çº¦æŸä¼˜å…ˆçº§æœ€é«˜ï¼Œè¦†ç›–ä»»ä½•é£æ ¼åŒ–æ”¹å†™åå¥½ã€‚

ä»¥ä¸‹å†…å®¹å¿…é¡»åŸæ ·ä¿ç•™ï¼Œç»å¯¹ä¸è¦ä¿®æ”¹ï¼š
- LaTeX å‘½ä»¤ï¼š\\textbf{...} 
- å ä½ç¬¦ï¼šå½¢å¦‚ [[ç±»å‹_ç¼–å·]] çš„æ ‡è®°ï¼ˆå¦‚æ•°å­¦å…¬å¼ã€å¼•ç”¨ã€å®å‘½ä»¤ç­‰çš„å ä½ç¬¦ï¼‰
- æºæ–‡æœ¬ä¸­çš„ä»£ç å—ã€JSON ç¤ºä¾‹ã€æŒ‡ä»¤æ¨¡æ¿ç­‰å‡ä¸ºå¾…ç¿»è¯‘å†…å®¹ï¼Œç¿»è¯‘å³å¯ï¼Œä¸è¦æ‰§è¡Œæˆ–è§£æ

è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–å…ƒä¿¡æ¯
- æ¢è¡Œå ä½ç¬¦å¿…é¡»åŸæ ·ä¿ç•™ï¼š[[SL]] è¡¨ç¤ºå•æ¢è¡Œï¼Œ[[PL]] è¡¨ç¤ºç©ºè¡Œåˆ†æ®µ
- ä¸¥ç¦æ–°å¢ã€åˆ é™¤ã€æ”¹å†™ [[SL]] æˆ– [[PL]]
- å¦‚æœè¾“å…¥ä»…ç”±å ä½ç¬¦ç»„æˆï¼ˆå½¢å¦‚ [[ç±»å‹_ç¼–å·]]ï¼‰ï¼Œç›´æ¥åŸæ ·è¿”å›ï¼Œä¸è¦ç¿»è¯‘
- æœ¯è¯­è¡¨ä¸­çš„ç¿»è¯‘å¿…é¡»ä¸¥æ ¼éµå®ˆï¼Œä¸å¾—è‡ªè¡Œå‘æŒ¥"""


# ============================================================================
# é…ç½®åŠ è½½
# ============================================================================


def load_test_config(yaml_path: str) -> Dict[str, Any]:
    """åŠ è½½æµ‹è¯•é…ç½®æ–‡ä»¶ï¼ˆæ¨¡å‹åˆ—è¡¨ã€endpointã€API keyã€source æ–‡æœ¬ï¼‰"""
    with open(yaml_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_user_system_prompt() -> Optional[str]:
    """ä» ~/.ieeA/config.yaml åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰ system_prompt"""
    config_path = Path.home() / ".ieeA" / "config.yaml"
    if not config_path.exists():
        return None
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    if config and "translation" in config:
        return config["translation"].get("custom_system_prompt")
    return None


def load_glossary_hints() -> Dict[str, str]:
    """ä» ~/.ieeA/glossary.yaml åŠ è½½æœ¯è¯­è¡¨ï¼Œè¿”å› {åŸæ–‡æœ¯è¯­: ç›®æ ‡æœ¯è¯­} æ˜ å°„

    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
      - ç®€å•æ˜ å°„ï¼š  "AI": "AI"
      - å­—å…¸æ˜ å°„ï¼š  "Transformer": {target: "Transformer", context: "Deep Learning"}
    """
    glossary_path = Path.home() / ".ieeA" / "glossary.yaml"
    if not glossary_path.exists():
        return {}
    with open(glossary_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    if not data or not isinstance(data, dict):
        return {}
    hints: Dict[str, str] = {}
    for key, value in data.items():
        if isinstance(value, str):
            hints[key] = value
        elif isinstance(value, dict):
            # åµŒå¥—æ ¼å¼å– target å­—æ®µ
            hints[key] = value.get("target", str(value))
    return hints


def load_few_shot_examples() -> List[Dict[str, str]]:
    """ä» ~/.ieeA/examples.yaml åŠ è½½ few-shot ç¿»è¯‘ç¤ºä¾‹

    è¿”å›åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« source å’Œ target å­—æ®µã€‚
    """
    examples_path = Path.home() / ".ieeA" / "examples.yaml"
    if not examples_path.exists():
        return []
    with open(examples_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    if isinstance(data, list):
        return data
    elif isinstance(data, dict):
        return data.get("examples", [])
    return []


# ============================================================================
# Prompt æ„å»º â€” å¤åˆ»é¡¹ç›® prompts.py çš„ build_system_prompt é€»è¾‘
# ============================================================================

DEFAULT_STYLE_PROMPT = (
    "ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†è‹±æ–‡å­¦æœ¯æ–‡æœ¬æ”¹å†™ä¸ºæµç•…è‡ªç„¶çš„ä¸­æ–‡ã€‚\n\n"
    "ç¿»è¯‘åŸåˆ™ï¼š\n"
    '1. è¿™æ˜¯"æ”¹å†™"ä»»åŠ¡ï¼Œä¸æ˜¯é€è¯ç¿»è¯‘ã€‚ç›®æ ‡æ˜¯è®©ä¸­æ–‡è¯»è€…èƒ½æµç•…é˜…è¯»\n'
    "2. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§å’Œä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§\n"
    "3. ç»“æ„ä¼˜å…ˆï¼šä¿æŒåŸæ–‡æ®µè½ä¸æ¢è¡Œè¾¹ç•Œï¼Œä¸è¦æ–°å¢æˆ–åˆ é™¤æ®µè½"
)


def build_system_prompt(
    custom_system_prompt: Optional[str] = None,
    glossary_hints: Optional[Dict[str, str]] = None,
) -> str:
    """æ„å»ºå®Œæ•´çš„ system prompt

    ç»„è£…é¡ºåºï¼ˆä¸é¡¹ç›®ä¸€è‡´ï¼‰ï¼š
    1. ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ æˆ– é»˜è®¤é£æ ¼æç¤ºè¯
    2. ç¡¬ç¼–ç æ ¼å¼è§„åˆ™
    3. æœ¯è¯­è¡¨ï¼ˆå¦‚æœ‰ï¼‰
    """
    # 1. é£æ ¼æç¤ºè¯
    style_prompt = (
        custom_system_prompt if custom_system_prompt else DEFAULT_STYLE_PROMPT
    )

    # 2. é£æ ¼ + æ ¼å¼è§„åˆ™
    system_content = f"{style_prompt}\n\n{FORMAT_RULES}"

    # 3. æœ¯è¯­è¡¨
    if glossary_hints:
        glossary_str = "\n".join([f"- {k}: {v}" for k, v in glossary_hints.items()])
        system_content += (
            f"\n\n## æœ¯è¯­è¡¨\nè¯·ä¸¥æ ¼æŒ‰ç…§æœ¯è¯­è¡¨ç¿»è¯‘ä»¥ä¸‹æœ¯è¯­ï¼š\n"
            f"æœ¯è¯­è¡¨ä¼˜å…ˆçº§é«˜äºé£æ ¼åå¥½ä¸ä¸Šä¸‹æ–‡æ¶¦è‰²ã€‚\n{glossary_str}"
        )

    return system_content


def build_messages(
    system_prompt: str,
    examples: List[Dict[str, str]],
    source_text: str,
) -> List[Dict[str, str]]:
    """æ„å»ºå®Œæ•´çš„ messages åˆ—è¡¨

    ç»“æ„ï¼ˆä¸é¡¹ç›® http_provider.py ä¸€è‡´ï¼‰ï¼š
      [system] â†’ [user/assistant ç¤ºä¾‹å¯¹ Ã— N] â†’ [user å¾…ç¿»è¯‘æ–‡æœ¬]
    """
    messages = [{"role": "system", "content": system_prompt}]

    # Few-shot ç¤ºä¾‹ï¼šæ¯æ¡ä½œä¸ºä¸€ç»„ user + assistant å¯¹è¯
    for ex in examples:
        src = ex.get("source", "").strip()
        tgt = ex.get("target", "").strip()
        if src and tgt:
            messages.append({"role": "user", "content": src})
            messages.append({"role": "assistant", "content": tgt})

    # å¾…ç¿»è¯‘æ–‡æœ¬
    messages.append({"role": "user", "content": source_text})
    return messages


# ============================================================================
# API è°ƒç”¨
# ============================================================================


async def call_model(
    client: httpx.AsyncClient,
    endpoint: str,
    api_key: str,
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 0.5,
    timeout: float = 180.0,
) -> Dict[str, Any]:
    """è°ƒç”¨å•ä¸ªæ¨¡å‹è¿›è¡Œç¿»è¯‘

    è¿”å›åŒ…å«ç¿»è¯‘ç»“æœã€è€—æ—¶ã€token ç”¨é‡ç­‰ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    request_body = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
    }

    start_time = time.time()
    try:
        response = await client.post(
            endpoint,
            json=request_body,
            headers=headers,
            timeout=timeout,
        )
        latency = round(time.time() - start_time, 2)
        response.raise_for_status()
        data = response.json()

        # æå–ç¿»è¯‘å†…å®¹
        content = data["choices"][0]["message"]["content"].strip()

        # æå– token ç”¨é‡ï¼ˆOpenRouter é€šå¸¸ä¼šè¿”å›ï¼‰
        usage = data.get("usage", {})

        return {
            "model": model,
            "status": "success",
            "translation": content,
            "latency_seconds": latency,
            "usage": {
                "prompt_tokens": usage.get("prompt_tokens"),
                "completion_tokens": usage.get("completion_tokens"),
                "total_tokens": usage.get("total_tokens"),
            },
            "error": None,
        }
    except httpx.HTTPStatusError as e:
        latency = round(time.time() - start_time, 2)
        # å°è¯•æå– HTTP é”™è¯¯å“åº”ä½“
        error_detail = ""
        try:
            error_detail = e.response.text
        except Exception:
            error_detail = str(e)
        return {
            "model": model,
            "status": "error",
            "translation": None,
            "latency_seconds": latency,
            "usage": None,
            "error": f"HTTP {e.response.status_code}: {error_detail}",
        }
    except Exception as e:
        latency = round(time.time() - start_time, 2)
        return {
            "model": model,
            "status": "error",
            "translation": None,
            "latency_seconds": latency,
            "usage": None,
            "error": str(e),
        }


async def run_all_models(
    endpoint: str,
    api_key: str,
    models: List[str],
    messages: List[Dict[str, str]],
    concurrency: int = 4,
) -> List[Dict[str, Any]]:
    """å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹ï¼Œä¿¡å·é‡æ§åˆ¶åŒæ—¶è¯·æ±‚æ•°é¿å…è¢«é™æµ"""
    semaphore = asyncio.Semaphore(concurrency)

    async def sem_call(client: httpx.AsyncClient, model: str) -> Dict[str, Any]:
        async with semaphore:
            print(f"  â³ æ­£åœ¨è°ƒç”¨: {model} ...")
            result = await call_model(client, endpoint, api_key, model, messages)
            icon = "âœ…" if result["status"] == "success" else "âŒ"
            print(f"  {icon} {model} â€” {result['latency_seconds']}s")
            return result

    async with httpx.AsyncClient() as client:
        tasks = [sem_call(client, m) for m in models]
        results = await asyncio.gather(*tasks)

    return list(results)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================


async def main():
    # ---- è·¯å¾„ ----
    script_dir = Path(__file__).parent
    test_config_path = script_dir / "å¯¹æ¯”æ¨¡å‹.yaml"
    output_path = script_dir / "model_comparison_results.json"

    print("=" * 60)
    print("  æ¨¡å‹ç¿»è¯‘å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)

    # ---- 1. åŠ è½½æµ‹è¯•é…ç½® ----
    print("\nğŸ“‚ åŠ è½½æµ‹è¯•é…ç½®...")
    test_config = load_test_config(str(test_config_path))
    endpoint = test_config["llm"]["endpoint"]
    api_key = test_config["llm"]["key"]
    models = test_config["llm"]["models"]
    source_text = test_config["source"].strip()
    print(f"   ç«¯ç‚¹:     {endpoint}")
    print(f"   æ¨¡å‹æ•°é‡: {len(models)}")
    print(f"   åŸæ–‡é•¿åº¦: {len(source_text)} å­—ç¬¦")

    # ---- 2. åŠ è½½ ~/.ieeA ç”¨æˆ·é…ç½® ----
    print("\nğŸ“‚ åŠ è½½ ~/.ieeA é…ç½®...")

    custom_prompt = load_user_system_prompt()
    print(f"   è‡ªå®šä¹‰ system_prompt: {'âœ… å·²åŠ è½½' if custom_prompt else 'âš ï¸ ä½¿ç”¨é»˜è®¤'}")

    glossary = load_glossary_hints()
    print(f"   æœ¯è¯­è¡¨:   {len(glossary)} æ¡")

    examples = load_few_shot_examples()
    print(f"   Few-shot: {len(examples)} æ¡")

    # ---- 3. æ„å»º Prompt ----
    print("\nğŸ”§ æ„å»º Prompt...")
    # ä¸é¡¹ç›® pipeline.py._build_glossary_hints ä¸€è‡´ï¼šåªä¿ç•™åŸæ–‡ä¸­å®é™…å‡ºç°çš„æœ¯è¯­
    folded_source = source_text.casefold()
    filtered_glossary = {
        k: v for k, v in glossary.items() if k.casefold() in folded_source
    }
    print(f"   æœ¯è¯­è¡¨è¿‡æ»¤: {len(glossary)} â†’ {len(filtered_glossary)} æ¡ (åŒ¹é…åŸæ–‡)")
    system_prompt = build_system_prompt(
        custom_system_prompt=custom_prompt,
        glossary_hints=filtered_glossary,
    )
    messages = build_messages(system_prompt, examples, source_text)
    # ç»Ÿè®¡ few-shot ä¸­å®é™…æœ‰æ•ˆçš„å¯¹æ•°ï¼ˆsource å’Œ target éƒ½éç©ºï¼‰
    valid_example_count = sum(
        1
        for ex in examples
        if ex.get("source", "").strip() and ex.get("target", "").strip()
    )
    print(
        f"   æ¶ˆæ¯è½®æ•°: {len(messages)} (1 system + {valid_example_count} ç¤ºä¾‹å¯¹ + 1 user)"
    )

    # ---- 4. è°ƒç”¨æ‰€æœ‰æ¨¡å‹ ----
    print(f"\nğŸš€ å¼€å§‹å¹¶å‘æµ‹è¯• {len(models)} ä¸ªæ¨¡å‹...\n")
    results = await run_all_models(endpoint, api_key, models, messages)

    # ---- 5. æ±‡æ€»ç»Ÿè®¡ ----
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = len(results) - success_count

    # æŒ‰æ¨¡å‹åˆ—è¡¨åŸå§‹é¡ºåºæ•´ç†ç»“æœ
    model_order = {m: i for i, m in enumerate(models)}
    results.sort(key=lambda r: model_order.get(r["model"], 999))

    # ---- 6. ä¿å­˜ JSON ----
    output_data = {
        "meta": {
            "test_time": datetime.now().isoformat(),
            "endpoint": endpoint,
            "model_count": len(models),
            "success_count": success_count,
            "error_count": error_count,
            "source_text": source_text,
            "system_prompt_used": system_prompt,
            "glossary_term_count": len(glossary),
            "glossary_terms": glossary,
            "example_count": valid_example_count,
        },
        "results": results,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    # ---- 7. æ‰“å°æ‘˜è¦ ----
    print(f"\n{'=' * 60}")
    print(f"  æµ‹è¯•å®Œæˆ: {success_count} æˆåŠŸ / {error_count} å¤±è´¥")
    print(f"  ç»“æœå·²ä¿å­˜: {output_path}")
    print(f"{'=' * 60}")

    # æ‰“å°å„æ¨¡å‹è€—æ—¶æ’è¡Œ
    if success_count > 0:
        print("\nğŸ“Š è€—æ—¶æ’è¡Œï¼ˆä»…æˆåŠŸï¼‰:")
        successful = [r for r in results if r["status"] == "success"]
        successful.sort(key=lambda r: r["latency_seconds"])
        for i, r in enumerate(successful, 1):
            tokens = ""
            if r.get("usage") and r["usage"].get("total_tokens"):
                tokens = f"  ({r['usage']['total_tokens']} tokens)"
            print(f"   {i:2d}. {r['model']:<40s} {r['latency_seconds']:>6.1f}s{tokens}")


if __name__ == "__main__":
    asyncio.run(main())
