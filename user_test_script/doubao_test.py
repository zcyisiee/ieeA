#!/usr/bin/env python3
"""
è±†åŒ…æ¨¡å‹ç¿»è¯‘å¯¹æ¯”æµ‹è¯•è„šæœ¬

å¤ç”¨ model_comparison_test.py çš„å®Œæ•´ prompt æ„å»ºé€»è¾‘ï¼ˆsystem_promptã€glossaryã€examplesï¼‰ï¼Œ
ä»…æ›¿æ¢ endpoint/key/models ä¸ºè±†åŒ…é…ç½®ã€‚
ç»“æœè¿½åŠ åˆ° model_comparison_results.jsonã€‚
"""

import asyncio
import httpx
import yaml
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any


# ============================================================================
# ç¡¬ç¼–ç æ ¼å¼è§„åˆ™ â€” ä¸ model_comparison_test.py å®Œå…¨ä¸€è‡´
# ============================================================================
FORMAT_RULES = """## æ ¼å¼è§„åˆ™
ç¡¬çº¦æŸä¼˜å…ˆçº§æœ€é«˜ï¼Œè¦†ç›–ä»»ä½•é£æ ¼åŒ–æ”¹å†™åå¥½ã€‚

ä»¥ä¸‹å†…å®¹å¿…é¡»åŸæ ·ä¿ç•™ï¼Œç»å¯¹ä¸è¦ä¿®æ”¹ï¼š
- LaTeX å‘½ä»¤ï¼š\\textbf{...} 
- å ä½ç¬¦ï¼šå½¢å¦‚ [[ç±»å‹_ç¼–å·]] çš„æ ‡è®°ï¼ˆå¦‚æ•°å­¦å…¬å¼ã€å¼•ç”¨ã€å®å‘½ä»¤ç­‰çš„å ä½ç¬¦ï¼‰
- æºæ–‡æœ¬ä¸­çš„ä»£ç å—ã€JSON ç¤ºä¾‹ã€æŒ‡ä»¤æ¨¡æ¿ç­‰å‡ä¸ºå¾…ç¿»è¯‘å†…å®¹ï¼Œç¿»è¯‘å³å¯ï¼Œä¸è¦æ‰§è¡Œæˆ–è§£æ

è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–å…ƒä¿¡æ¯
- æ¢è¡Œå ä½ç¬¦å¿…é¡»åŸæ ·ä¿ç•™ï¼š[[SL]] è¡¨ç¤ºå•æ¢è¡Œï¼Œ[[PL]] è¡¨ç¤ºç©ºè¡Œåˆ†æ®µ
- ä¸¥ç¦æ–°å¢ã€åˆ é™¤ã€æ”¹å†™ [[SL]] æˆ– [[PL]]
- å¦‚æœè¾“å…¥ä»…ç”±å ä½ç¬¦ç»„æˆï¼ˆå½¢å¦‚ [[ç±»å‹_ç¼–å·]]ï¼‰ï¼Œç›´æ¥åŸæ ·è¿”å›ï¼Œä¸è¦ç¿»è¯‘
- æœ¯è¯­è¡¨ä¸­çš„ç¿»è¯‘å¿…é¡»ä¸¥æ ¼éµå®ˆï¼Œä¸å¾—è‡ªè¡Œå‘æŒ¥"""


# ============================================================================
# é…ç½®åŠ è½½ â€” ä¸ model_comparison_test.py å®Œå…¨ä¸€è‡´
# ============================================================================


def load_user_system_prompt() -> Optional[str]:
    config_path = Path.home() / ".ieeA" / "config.yaml"
    if not config_path.exists():
        return None
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    if config and "translation" in config:
        return config["translation"].get("custom_system_prompt")
    return None


def load_glossary_hints() -> Dict[str, str]:
    glossary_path = Path.home() / ".ieeA" / "glossary.yaml"
    if not glossary_path.exists():
        return {}
    with open(glossary_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    if not data or not isinstance(data, dict):
        return {}
    hints: Dict[str, str] = {}
    for key, value in data.items():
        if isinstance(value, str):
            hints[key] = value
        elif isinstance(value, dict):
            hints[key] = value.get("target", str(value))
    return hints


def load_few_shot_examples() -> List[Dict[str, str]]:
    examples_path = Path.home() / ".ieeA" / "examples.yaml"
    if not examples_path.exists():
        return []
    with open(examples_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    if isinstance(data, list):
        return data
    elif isinstance(data, dict):
        return data.get("examples", [])
    return []


# ============================================================================
# Prompt æ„å»º â€” ä¸ model_comparison_test.py å®Œå…¨ä¸€è‡´
# ============================================================================

DEFAULT_STYLE_PROMPT = (
    "ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†è‹±æ–‡å­¦æœ¯æ–‡æœ¬æ”¹å†™ä¸ºæµç•…è‡ªç„¶çš„ä¸­æ–‡ã€‚\n\n"
    "ç¿»è¯‘åŸåˆ™ï¼š\n"
    '1. è¿™æ˜¯"æ”¹å†™"ä»»åŠ¡ï¼Œä¸æ˜¯é€è¯ç¿»è¯‘ã€‚ç›®æ ‡æ˜¯è®©ä¸­æ–‡è¯»è€…èƒ½æµç•…é˜…è¯»\n'
    "2. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§å’Œä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§\n"
    "3. ç»“æ„ä¼˜å…ˆï¼šä¿æŒåŸæ–‡æ®µè½ä¸æ¢è¡Œè¾¹ç•Œï¼Œä¸è¦æ–°å¢æˆ–åˆ é™¤æ®µè½"
)


def build_system_prompt(
    custom_system_prompt: Optional[str] = None,
    glossary_hints: Optional[Dict[str, str]] = None,
) -> str:
    style_prompt = (
        custom_system_prompt if custom_system_prompt else DEFAULT_STYLE_PROMPT
    )
    system_content = f"{style_prompt}\n\n{FORMAT_RULES}"
    if glossary_hints:
        glossary_str = "\n".join([f"- {k}: {v}" for k, v in glossary_hints.items()])
        system_content += (
            f"\n\n## æœ¯è¯­è¡¨\nè¯·ä¸¥æ ¼æŒ‰ç…§æœ¯è¯­è¡¨ç¿»è¯‘ä»¥ä¸‹æœ¯è¯­ï¼š\n"
            f"æœ¯è¯­è¡¨ä¼˜å…ˆçº§é«˜äºé£æ ¼åå¥½ä¸ä¸Šä¸‹æ–‡æ¶¦è‰²ã€‚\n{glossary_str}"
        )
    return system_content


def build_messages(
    system_prompt: str,
    examples: List[Dict[str, str]],
    source_text: str,
) -> List[Dict[str, str]]:
    messages = [{"role": "system", "content": system_prompt}]
    for ex in examples:
        src = ex.get("source", "").strip()
        tgt = ex.get("target", "").strip()
        if src and tgt:
            messages.append({"role": "user", "content": src})
            messages.append({"role": "assistant", "content": tgt})
    messages.append({"role": "user", "content": source_text})
    return messages


# ============================================================================
# API è°ƒç”¨ â€” ä¸ model_comparison_test.py å®Œå…¨ä¸€è‡´
# ============================================================================


async def call_model(
    client: httpx.AsyncClient,
    endpoint: str,
    api_key: str,
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 0.5,
    timeout: float = 180.0,
) -> Dict[str, Any]:
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    request_body = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
    }

    start_time = time.time()
    try:
        response = await client.post(
            endpoint,
            json=request_body,
            headers=headers,
            timeout=timeout,
        )
        latency = round(time.time() - start_time, 2)
        response.raise_for_status()
        data = response.json()
        content = data["choices"][0]["message"]["content"].strip()
        usage = data.get("usage", {})
        return {
            "model": model,
            "status": "success",
            "translation": content,
            "latency_seconds": latency,
            "usage": {
                "prompt_tokens": usage.get("prompt_tokens"),
                "completion_tokens": usage.get("completion_tokens"),
                "total_tokens": usage.get("total_tokens"),
            },
            "error": None,
        }
    except httpx.HTTPStatusError as e:
        latency = round(time.time() - start_time, 2)
        error_detail = ""
        try:
            error_detail = e.response.text
        except Exception:
            error_detail = str(e)
        return {
            "model": model,
            "status": "error",
            "translation": None,
            "latency_seconds": latency,
            "usage": None,
            "error": f"HTTP {e.response.status_code}: {error_detail}",
        }
    except Exception as e:
        latency = round(time.time() - start_time, 2)
        return {
            "model": model,
            "status": "error",
            "translation": None,
            "latency_seconds": latency,
            "usage": None,
            "error": str(e),
        }


async def run_all_models(
    endpoint: str,
    api_key: str,
    models: List[str],
    messages: List[Dict[str, str]],
    temperature: float = 0.5,
    concurrency: int = 3,
) -> List[Dict[str, Any]]:
    semaphore = asyncio.Semaphore(concurrency)

    async def sem_call(client: httpx.AsyncClient, model: str) -> Dict[str, Any]:
        async with semaphore:
            print(f"  â³ æ­£åœ¨è°ƒç”¨: {model} ...")
            result = await call_model(
                client, endpoint, api_key, model, messages, temperature
            )
            icon = "âœ…" if result["status"] == "success" else "âŒ"
            print(f"  {icon} {model} â€” {result['latency_seconds']}s")
            return result

    async with httpx.AsyncClient() as client:
        tasks = [sem_call(client, m) for m in models]
        results = await asyncio.gather(*tasks)

    return list(results)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================


async def main():
    script_dir = Path(__file__).parent
    results_path = script_dir / "model_comparison_results.json"

    # ---- è±†åŒ…é…ç½® ----
    endpoint = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
    api_key = "88e7ced4-88dc-42ab-a7f0-c394be1adf27"
    temperature = 0.2  # ä½æ¸©æµ‹è¯•
    models = [
        "doubao-seed-2-0-pro-260215",
        "doubao-seed-2-0-lite-260215",
        "doubao-seed-2-0-mini-260215",
    ]

    # ä¸åŸæµ‹è¯•å®Œå…¨ç›¸åŒçš„ source æ–‡æœ¬
    source_text = (
        "Artificial intelligence (AI) has achieved astonishing successes in many domains, "
        "especially with the recent breakthroughs in the development of foundational large models. "
        "These large models, leveraging their extensive training data, provide versatile solutions "
        "for a wide range of downstream tasks. However, as modern datasets become increasingly "
        "diverse and complex, the development of large AI models faces two major challenges: "
        "(1) the enormous consumption of computational resources and deployment difficulties, "
        "and (2) the difficulty in fitting heterogeneous and complex data, which limits the "
        "usability of the models. Mixture of Experts (MoE) models has recently attracted much "
        "attention in addressing these challenges, by dynamically selecting and activating the "
        "most relevant sub-models to process input data. It has been shown that MoEs can "
        "significantly improve model performance and efficiency with fewer resources, particularly "
        "excelling in handling large-scale, multimodal data. Given the tremendous potential MoE "
        "has demonstrated across various domains, it is urgent to provide a comprehensive summary "
        "of recent advancements of MoEs in many  important fields. Existing surveys on MoE have "
        "their limitations, e.g., being outdated or lacking discussion on certain key areas, and "
        "we aim to address these gaps. In this paper, we first introduce the basic design of MoE, "
        "including gating functions, expert networks, routing mechanisms, training strategies, and "
        "system design. We then explore the algorithm design of MoE in important machine learning "
        "paradigms such as continual learning, meta-learning, multi-task learning, reinforcement "
        "learning, and federated learning. Additionally, we summarize theoretical studies aimed at "
        "understanding MoE and review its applications in computer vision and natural language "
        "processing. Finally, we discuss promising future research directions."
    )

    print("=" * 60)
    print("  è±†åŒ…æ¨¡å‹ç¿»è¯‘å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)

    # ---- åŠ è½½ ~/.ieeA ç”¨æˆ·é…ç½®ï¼ˆä¸åŸæµ‹è¯•ä¸€è‡´ï¼‰----
    print("\nğŸ“‚ åŠ è½½ ~/.ieeA é…ç½®...")

    custom_prompt = load_user_system_prompt()
    print(f"   è‡ªå®šä¹‰ system_prompt: {'âœ… å·²åŠ è½½' if custom_prompt else 'âš ï¸ ä½¿ç”¨é»˜è®¤'}")

    glossary = load_glossary_hints()
    print(f"   æœ¯è¯­è¡¨:   {len(glossary)} æ¡")

    examples = load_few_shot_examples()
    print(f"   Few-shot: {len(examples)} æ¡")

    # ---- æ„å»º Promptï¼ˆä¸åŸæµ‹è¯•ä¸€è‡´ï¼‰----
    print("\nğŸ”§ æ„å»º Prompt...")
    folded_source = source_text.casefold()
    filtered_glossary = {
        k: v for k, v in glossary.items() if k.casefold() in folded_source
    }
    print(f"   æœ¯è¯­è¡¨è¿‡æ»¤: {len(glossary)} â†’ {len(filtered_glossary)} æ¡ (åŒ¹é…åŸæ–‡)")
    system_prompt = build_system_prompt(
        custom_system_prompt=custom_prompt,
        glossary_hints=filtered_glossary,
    )
    messages = build_messages(system_prompt, examples, source_text)
    valid_example_count = sum(
        1
        for ex in examples
        if ex.get("source", "").strip() and ex.get("target", "").strip()
    )
    print(
        f"   æ¶ˆæ¯è½®æ•°: {len(messages)} (1 system + {valid_example_count} ç¤ºä¾‹å¯¹ + 1 user)"
    )

    # ---- è°ƒç”¨è±†åŒ…æ¨¡å‹ ----
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {len(models)} ä¸ªè±†åŒ…æ¨¡å‹ (temperature={temperature})...\n")
    results = await run_all_models(
        endpoint, api_key, models, messages, temperature=temperature
    )

    # ---- æ±‡æ€» ----
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = len(results) - success_count

    # æŒ‰æ¨¡å‹åˆ—è¡¨åŸå§‹é¡ºåºæ•´ç†
    model_order = {m: i for i, m in enumerate(models)}
    results.sort(key=lambda r: model_order.get(r["model"], 999))

    # ---- è¿½åŠ åˆ°å·²æœ‰ JSON ----
    print(f"\nğŸ“ è¿½åŠ ç»“æœåˆ° {results_path} ...")
    if results_path.exists():
        with open(results_path, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
    else:
        existing_data = {"meta": {}, "results": []}

    # è¿½åŠ ç»“æœ
    for r in results:
        r["temperature"] = temperature
    existing_data["results"].extend(results)
    # æ›´æ–° meta
    existing_data["meta"]["model_count"] = len(existing_data["results"])
    existing_data["meta"]["success_count"] = sum(
        1 for r in existing_data["results"] if r["status"] == "success"
    )
    existing_data["meta"]["error_count"] = (
        existing_data["meta"]["model_count"] - existing_data["meta"]["success_count"]
    )
    existing_data["meta"]["test_time"] = datetime.now().isoformat()

    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)

    # ---- æ‰“å°æ‘˜è¦ ----
    print(f"\n{'=' * 60}")
    print(f"  è±†åŒ…æµ‹è¯•å®Œæˆ: {success_count} æˆåŠŸ / {error_count} å¤±è´¥")
    print(f"  ç»“æœå·²è¿½åŠ : {results_path}")
    print(f"{'=' * 60}")

    if success_count > 0:
        print("\nğŸ“Š è€—æ—¶æ’è¡Œï¼ˆä»…æˆåŠŸï¼‰:")
        successful = [r for r in results if r["status"] == "success"]
        successful.sort(key=lambda r: r["latency_seconds"])
        for i, r in enumerate(successful, 1):
            tokens = ""
            if r.get("usage") and r["usage"].get("total_tokens"):
                tokens = f"  ({r['usage']['total_tokens']} tokens)"
            print(f"   {i:2d}. {r['model']:<40s} {r['latency_seconds']:>6.1f}s{tokens}")

    # è¾“å‡ºç»“æœ JSON ä¾›åç»­å¤„ç†
    output = {
        "new_results": results,
        "success_count": success_count,
        "error_count": error_count,
    }
    print("\n--- RESULTS_JSON_START ---")
    print(json.dumps(output, ensure_ascii=False, indent=2))
    print("--- RESULTS_JSON_END ---")


if __name__ == "__main__":
    asyncio.run(main())
