llm:
  endpoint: https://openrouter.ai/api/v1/chat/completions
  key: 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
  models: [
            anthropic/claude-opus-4.5, anthropic/claude-sonnet-4.5, anthropic/claude-haiku-4.5,
            openai/gpt-5.2-chat, openai/gpt-5-mini, openai/gpt-5-nano,
            google/gemini-3-pro-preview, google/gemini-3-flash-preview, qwen/qwen3-max-thinking,
            qwen/qwen3.5-plus-02-15, moonshotai/kimi-k2.5, deepseek/deepseek-v3.2
          ]
source: 
  "Artificial intelligence (AI) has achieved astonishing successes in many domains, especially with the recent breakthroughs in the development of foundational large models. These large models, leveraging their extensive training data, provide versatile solutions for a wide range of downstream tasks. However, as modern datasets become increasingly diverse and complex, the development of large AI models faces two major challenges: (1) the enormous consumption of computational resources and deployment difficulties, and (2) the difficulty in fitting heterogeneous and complex data, which limits the usability of the models. Mixture of Experts (MoE) models has recently attracted much attention in addressing these challenges, by dynamically selecting and activating the most relevant sub-models to process input data. It has been shown that MoEs can significantly improve model performance and efficiency with fewer resources, particularly excelling in handling large-scale, multimodal data. Given the tremendous potential MoE has demonstrated across various domains, it is urgent to provide a comprehensive summary of recent advancements of MoEs in many  important fields. Existing surveys on MoE have their limitations, e.g., being outdated or lacking discussion on certain key areas, and we aim to address these gaps. In this paper, we first introduce the basic design of MoE, including gating functions, expert networks, routing mechanisms, training strategies, and system design. We then explore the algorithm design of MoE in important machine learning paradigms such as continual learning, meta-learning, multi-task learning, reinforcement learning, and federated learning. Additionally, we summarize theoretical studies aimed at understanding MoE and review its applications in computer vision and natural language processing. Finally, we discuss promising future research directions."