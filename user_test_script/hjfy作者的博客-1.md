# 消耗 3 亿 token，我用大模型翻译了 1 万篇 arXiv 论文

最近我用大模型翻译了 1 万篇 arXiv 论文，消耗了 3 亿 token，并上线了一个 SaaS 服务：「幻觉翻译」 https://hjfy.top/ ，可以通过使用它来高效阅读论文，本文将介绍具体实现过程。

### 现有 arXiv 论文翻译方案

翻译 arXiv 论文有以下几种方案：

方案一：将 PDF 转成 Markdown 再翻译，比如使用 MinerU 之类的工具，转 Markdown 的好处是可以方便二次编辑，但 Markdown 本身表现能力弱，导致丢失太多信息了，比如 Markdown 本身不支持复杂表格，大部分工具的转换效果都很差，比如下面是一个产品的效果，左侧是原先的表格，右侧是转换后的表格，转换后完全没法看。

表格转换 Markdown 后的效果

最近比较火的方法是使用多模态大模型进行解析，大概是先转成图片然后使用大模型的 OCR 能力实现的，我也测试过了据说效果最好的 Gemini，但它有个缺点是转换后的 Markdown 没有图片，所以只适合用来提取文本信息，而且我测试时发现经常无故丢失大段文本，因此这个方案不可行。

方案二：将 PDF 转成 Word 后翻译，相比 Markdown，Word 可以更好保留原先的布局，但由于 PDF 文件本身的复杂性，这些软件通常都会遇到排版和公式错乱问题，下图是某个国外知名的翻译软件效果

表格转 Word 后的效果

如果是复杂公式更没法看了，比如下面是某篇论文的翻译结果

复杂公式的效果

虽然这个软件号称效果最好，但从我个人使用经验看，几乎没有一个 PDF 是能正常看的，必须忍受大量错乱的布局，而且价格很贵，28$ 的版本每月限制 20 个文件。

如果要自己实现的话，PDF 转 Word 是很费时的功能，实现成本很高，而市面上唯一可用的第三方库是 Solid Converter，它的价格很贵，据说每年百万，因此效果不好还贵，这个方案也不可行。

方案三：基于 arXiv 官网提供的 HTML 版本翻译，arXiv 为大部分论文都提供了基于 LaTeXML 编译的网页版本，这样就能使用浏览器中的翻译软件翻译了，但 LaTeXML 很多模块不支持，比如 forest 宏不支持，下面使用 forest 的论文效果，左侧是原本的树形图，右侧是 LaTeXML 的显示效果，基本没法看。

左侧是原文，右侧是转成 HTML 的效果

方案四：基于 arXiv 的 LaTex 源码翻译，这种做法能最大程度保留论文格式，包括所有公式及引用，是目前唯一能满足我所有需求的方式，有极少数产品是这么做的，但它们有以下两个问题：

- 经常解析失败，成功率很低，即便是商业产品也失败率很高，我个人使用的经验看成功率可能不到 60%：

- 贵或有数量限制，现有产品通常有两种收费模式：

- 月费，比如几十元但只能翻译 20 篇，这种产品用起来真让人纠结。

- 按字数收费，看起来更合理？然而这类产品往往贵得夸张，下面是某个产品翻译一篇论文的价格，在国内都够买本书了，而实际消耗的 token 数量大概不到 10 万，按 GPT-4o-mini 算其实只需 $0.06。

因此在评估了现有方案之后我决定自己实现，接下来是实现过程。

### 自己实现

第一次尝试：基于开源项目改

在自己实现前先参考开源项目，因为之前已经有人实现过类似功能了，然而尝试之后发现成功率很低，而且这些开源项目基本都是基于正则实现的，难以实现准确判断，大多是没有类型标注的 Python 代码，改造困难，所以放弃了。

第二次尝试：完全使用大模型来翻译

考虑到《苦涩的教训》，我尝试的第一个方案是直接使用大模型翻译，目前大部分大模型上下文都很长，至少也有 128K，因此可以直接将 LaTex 文件进行简单分割，直接让大模型进行全文翻译。

然而这个方案效果很差，由于幻觉问题，大模型经常生成无法编译的 LaTex 文件，比如多出未知宏，或者少转义了特殊字符等，尝试了几篇论文发现没有一篇能正常编译，所以放弃了这个方案。

第三次尝试：使用开源 LaTex 解析

既然完整 LaTex 解析容易出错，那就通过解析来简化，让大模型只看到纯文本，这样就不会犯错了，但我实在不想写语法解析，因此先找开源的 LaTex 解析库，评估之后选择了看起来很成熟的 unified-latex。

于是使用 unified-latex 实现解析，提取其中的纯文本让大模型进行翻译，这个方案看起来很完美，然而实现完后发现了两个致命问题，一是 unified-latex 的实现不完整，稍微奇怪点的 LaTex 就不支持了，导致解析后无法还原回去，二是这个方案的翻译效果太差，举个例子，比如下面这段简单的文本

```
Where $x$ is
```

解析后会变成三个节点

```
[{
  "type": "string",
  "content": "Where "
}, {
  "type": "inlineMath",
  "content": [
    {
      "type": "string",
      "content": "x"
    }
  ]
}, {
  "type": "string",
  "content": " is"
}]
```

然后让大模型逐个翻译「Where 」、「x」和「 is」，但问题来了，大模型只看到「Where 」，它肯定会翻译成「哪里」，而这里的意思是「其中」，因此要翻译效果好必须看到完整段落，所以解析成纯文本也是不可行的，这个方案也放弃了。

第四次尝试：自己实现 LaTex 解析

为了解决前面翻译效果差问题，我不得不自己实现 LaTex 解析，而且还实现了两种解析，一种是半解析，它只提取段落级别文本，忽略不需要翻译的部分，另一种是完整语法树解析，用于检查及修改。

LaTex 解析的实现比我预想的要复杂，它和 Markdown 之类文档格式不同，LaTex 本质上是一种「脚本语言」，有 IF 等控制语法，甚至不能像普通语言那样先做语法解析再执行，而是必须边解释边执行，因为可以随时定义新的命令，比如不少人会通过下面的语句来定义公式开始和结束，遇到这个命令时要当成对应的其它命令：

```
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
```

完整实现 LaTex 解析还需要解析第三方库，这实在太复杂了，因此具体实现时我做了很多取巧，只覆盖绝大多数人使用的语法。

通过检查来应对大模型幻觉问题

有了自己实现的解析就能让大模型进行段落级别翻译了，然而即便是一小段文本，用大模型翻译依旧容易出错，翻译后可编译的成功率还是很低。

怎么办呢？解决方法是加了大量的后检查机制，使用前面提到的完整解析模型，包括检查关键宏是否遗漏，以及大模型是否可能翻错了，对于一些容易出错的地方，还会直接做屏蔽，比如发现大模型经常擅自修改公式，于是干脆直接屏蔽，改成一个特殊标记，大模型看不到就不会犯错。

最烧钱的问题：arXiv 下载

解决了大模型导致的编译报错问题后成功率明显提升，于是我开始加大翻译测试量，然而这时遇到了整个项目最烧钱的问题：arXiv 下载。

虽然本文标题说消耗了 3 亿 token，但实际主要使用的是 qwen-turbo-latest 和 doubao-1-5-lite 模型，这两个大模型的成本是一百万 token 输入 3 毛输出 6 毛，因此 3 亿 token 其实只花了不到 140 元，便宜死了，甚至远比效果更差的传统翻译便宜，比如百度翻译 1 亿字要 4000 元。

花费最多的反而是最不起眼的 arXiv 下载问题，因为 arXiv 有限流策略，加上服务部署在海外，导致下载速度很慢，经常出现的情况是整个任务 10 分钟里有 8 分钟在下载，手动测试的时候等得我心烦。

于是我使用 arXiv 的 s3 付费接口下载了 2023 年至今的论文源码，这竟然花费了 174$，因为大概有 2T 数据。

不过这钱烧得还是值的，除了大幅提升效率，还能进行后面将提到的数据分析。

难以解决的 LaTex 编译问题

最后也是整个项目最难解决的问题：LaTex 编译失败，编译失败的原因有很多，常见的有以下几种：

- 某些包和中文 ctex 环境冲突。

- 某些包或写法和 xelatex/lulatex 编译器冲突。

- 论文使用的包是旧版的，而我使用的是 Tex 2025 版本，有些参数没了或改了导致出错。

这些问题没法预先知道，只能通过大量测试来发现，整个过程非常耗体力，因为 LaTex 的报错信息经常不准确，尤其是 Hook 类的，通过报错无法定位问题在哪，我不得不使用最原始的排查方法：折半删除，直到找到最小复现条件。

通过这种原始方法测试了 1000 多文件之后，错误率依旧有 15% 左右，怎么办？没办法，继续加大测试量，终于在 5000 多文件之后，错误率降低到 5% 左右了，而且在整个过程中我将自己手动修复方法都固化在代码中了，这样曾经遇到过的问题都能自动修复，大幅减轻了体力，我每天大概能修复  60 个，因此 5% 的错误率意味着一天能翻译 1000 篇，翻译速度还可以。

如何找到最有价值的 1 万篇论文

接下来的问题是如何找到 1 万篇最有价值的论文进行翻译，arXiv 上论文总数是 270 万，其中只有少数是值得反复阅读的。

最简单的排序方法是根据引用数量，然而我尝试使用 Semantic Scholar 的接口没成功，所以是通过对前面从 s3 下载的 2023 年至今的 47 万篇论文进行简单解析，提取其中引用 arXiv 论文来计数的，虽然不能准确反映引用数量，但这个排序应该很接近。

其中引用最高是 1412.6980（Adam 优化器），有 29512 引用，但排序从 1 万之后引用数量就降低到 64 了，10 万之后就是个位数了，所以预计只需要翻译 10 万篇，就能覆盖绝大多数人这辈子要看的 arXiv 论文了。

然而之前我个人测试的论文基本都是 AI 相关的，按这个引用数量排序后前 1 万中多出了许多物理学及天体相关的论文， 其中还有好多书，比如 0912.0201 是一本关于天文望远镜的书，有 500 页，2103.05419 是讲电子对撞机的，有 900 页，可能缺少相关术语翻译效果不好。

这些新论文的加入导致导致翻译成功率又显著降低了，尤其是 REVTeX 4 这个物理学中的常见模板的表格报错让我头疼了好久。

不过最终还是完成了引用数量最高的 1 万篇论文翻译，并在这个过程中大幅提升了成功率，让这个产品基本可用了（不过目前 SaaS 版本会莫名奇妙退出，导致报错率远大于我自己的电脑，可能是因为使用函数计算导致）。

统计了一下整个项目有 2 万多行 TypeScript 代码，500 多测试用例，比原本预计的工作量要大得多，虽然不复杂，但很费体力。

好在最终的翻译效果我很满意，下面是几个示例

翻译示例

保留所有文本样式、布局，引用，下面是之前那篇 Word 翻译效果很差的复杂公式

复杂公式的效果

可以看到公式都完美保留了，感觉就像原本是中文写的论文，其它还可以通过下面几个地址实际体验：

- https://hjfy.top/arxiv/2501.14787

- https://hjfy.top/arxiv/2503.06072

- https://hjfy.top/arxiv/2412.05265

从我个人使用情况看，这个工具可以大幅提升阅读效率，尤其是对自己不熟悉的领域，中文读起来亲切很多，比如前面提到的电子对撞机如果是英文我肯定不会看，但翻译后我还能假装看了一会。

PDF 翻译还是不能没有

虽然幻觉翻译目前重点是 arXiv 源码翻译，但是还有大量论文只提供 PDF 文件，因此目前站点还提供了 PDF 翻译功能，是基于 marker 转成 Markdown 文件后翻译的，说实话效果很差，所以我目前在实现基于 PDF 指令进行翻译的功能，只有这种做法能完美保留原始布局和样式，具体细节这里先不展开了，等实现完再介绍。

最后，我觉得对用户最友好的翻译软件其实是离线软件，用户只需配置大模型 API 就能使用，还可以使用本地大模型，这样就能彻底实现翻译自由，不用每月花几十元还限制只能翻译 20 篇了，因此等功能完善之后我打算开发桌面版本，让每个人都拥有翻译自由。

---

原文链接：https://zhuanlan.zhihu.com/p/1905569596599169419
作者：吴多益
