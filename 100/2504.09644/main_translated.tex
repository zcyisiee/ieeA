\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025
\PassOptionsToPackage{numbers, compress}{natbib}

% ready for submission
% \usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{colortbl}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow,hhline}
\usepackage{caption}

\usepackage{multirow}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{subcaption}
% \usepackage{subfig}

\usepackage{enumitem}

% \usepackage{hyperref}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{graphics}
\usepackage[export]{adjustbox}
\usepackage{bbding}

\usepackage{enumitem}

\newcommand{\tian}[1]{\textcolor{black}{#1}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\zhan}[1]{\textcolor{blue}{#1}}
\newcommand{\model}{\textit{SegEarth-R1}}
\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\etc}{\textit{etc.}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}


\title{SegEarth-R1：基于大型语言模型的地理空间像素推理}

\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother

\author{
  \vspace{-25pt}\\
  \textbf{Kaiyu Li$^{1,*}$,
  \quad Zepeng Xin$^{1,*}$,
  \quad Li Pang$^1$,
  \quad Chao Pang$^{2}$,
  \quad Yupeng Deng$^{3}$} \\
  \textbf{ Jing Yao$^{3}$,
  \quad Guisong Xia$^{2}$,
  \quad Deyu Meng$^1$,
  \quad Zhi Wang$^1$,
  \quad Xiangyong Cao$^{1,\dag}$}\\
  % \vspace{3pt}
  $^1$Xi'an Jiaotong University ~~\quad $^2$Wuhan University ~~\quad $^3$Chinese Academy of Sciences\\
  \vspace{-25pt}
}
  
\definecolor{mygray}{gray}{.91}


% Auto-injected Chinese Support
\usepackage{xeCJK}
\setCJKmainfont{Songti SC}
\setCJKsansfont{PingFang SC}
\setCJKmonofont{STFangsong}

\begin{document}

\maketitle

% \begin{figure}[ht]
% \vspace{-10pt}
% \begin{center}
% 	\includegraphics[width=1.0\linewidth]{figures/task.pdf}
% \end{center}
% \vspace{-7pt}
% \caption{\small xxx.}
% \vspace{-2pt}
% \label{fig:abs}
% \end{figure}

\renewcommand{\thefootnote}{*}
\footnotetext[1]{Equal contribution}
\renewcommand{\thefootnote}{\dag}
\footnotetext[1]{Corresponding author: caoxiangyong@mail.xjtu.edu.cn}
\renewcommand{\thefootnote}{\arabic{footnote}}

% 语言引导的分割任务

\begin{abstract}
遥感在理解环境动态、城市规划与灾害管理等方面已变得至关重要。然而，传统的遥感工作流通常依赖显式的分割或检测方法，这些方法在应对需要基于空间语境、领域知识与隐含用户意图进行推理的复杂隐式查询时往往力不从心。基于此动机，我们提出了一个新任务，即 \ie geospatial pixel reasoning，该任务支持隐式查询与推理，并输出目标区域的掩码。为推动该任务的发展，我们构建并发布了首个大规模基准数据集 EarthReason，包含 5,434 幅人工标注的图像掩码以及超过 30,000 对隐式问答对。 

此外，我们提出了 SegEarth-R1，一种简单却高效的语言引导分割基线，集成了分层视觉编码器、用于指令解析的大型语言模型 (LLM) 以及为空间关联定制的掩码生成器。SegEarth-R1 的设计包含若干领域专用的改进，包括为处理超高分辨率遥感影像而采用的激进视觉令牌压缩、用于融合语言与多尺度特征的描述投影模块，以及可直接查询描述嵌入的精简掩码预测流程。大量实验表明，SegEarth-R1 在推理与指称分割任务上均取得了最先进的性能，显著优于传统和基于 LLM 的分割方法。我们的数据与代码将发布于 \url{https://github.com/earth-insights/SegEarth-R1}。

% 最近多模态大型语言模型 (MLLM) 在自然图像领域展现出潜力，但由于航拍影像具有极端尺度变化、倾斜的目标朝向以及密集的小尺度特征等独特特性，将其直接应用于遥感存在挑战。为应对这些挑战，我们引入了 SegEarth-R1——一个用于 geospatial pixel reasoning 的新框架，集成了分层视觉编码器、用于指令解析的大型语言模型 (LLM) 以及为空间相关性设计的定制掩码生成器。SegEarth-R1 融入了领域特化的改进，包括为处理超高分辨率图像的激进视觉令牌压缩、用于融合语言与多尺度特征的描述投影模块，以及直接查询描述嵌入的简化掩码预测流水线。

% 我们还提出了 EarthReason，这是首个面向 geospatial pixel reasoning 的大规模基准，包含 5,434 幅人工标注的遥感影像和超过 30,000 个隐式问答对。大量实验表明，SegEarth-R1 在推理与指称分割任务上取得了最先进的表现，显著优于传统分割方法和现成的 MLLM。我们的贡献包括提出遥感推理分割任务、发布 EarthReason 数据集，以及设计 SegEarth-R1，从而推进了基于 LLM 的分割在遥感领域的能力。
\end{abstract}

\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/title.pdf}
   \caption{\small 语义分割、指称分割与地理空间像素推理的比较。（左）来自 LoveDA~\cite{wang2021loveda} 和 RRSIS-D~\cite{liu2024rotated} 数据集的样例。（右）来自 EarthReason 数据集的样例。以往任务受限于固定的分类体系和显式指令，而地理空间像素推理则支持复杂的隐式指令，并要求模型具备更强的推理能力。}
   \label{fig:data_sample}
   \vspace{-1em}
\end{figure}


\section{引言}
\label{sec:intro}

通过遥感进行的地球观测已成为现代地理空间分析的基石，为环境动态、城市规划和灾害管理提供了前所未有的洞见~\cite{rolf2024mission, lu2025vision}。卫星与航拍影像为监测从森林砍伐模式到海岸侵蚀等行星尺度现象提供了独特视角。然而，要把这些原始像素数据转化为可操作的结论，单靠传统计算机视觉技术并不够；需要能够对空间语境、领域知识和隐含用户意图进行推理的模型。传统的遥感工作流程主要依赖显式任务，\eg, 语义分割和指代分割~\cite{long2021creating, christie2018functional, yuan2024rrsis}，它们在固定的分类体系内运行并且依赖精确的用户指令。尽管在定义明确的场景中这些方法效果良好，但在处理复杂的隐含查询时却显得力不从心——例如，基于坡度、植被覆盖和与基础设施的接近程度来识别高滑坡风险区域。这类任务要求对异质空间模式、对象关系及环境元数据进行隐式推理，已超出标准分割或检测流程的能力范围。

受此启发，我们提出了一个新任务，\ie，geospatial pixel reasoning，允许进行隐式查询与推理并生成目标区域的掩码。为推动该任务的研究，我们构建并发布了第一个大规模基准数据集 EarthReason，包含 5,434 对人工标注的遥感图像-掩码对，这些样本来自多种分类来源，覆盖 28 个场景类别，空间分辨率范围为 0.5m 到 153m。每幅图像都配有多个隐式推理问题，要求模型基于上下文信息与领域知识推断目标掩码，而非依赖明确的目标名称。此外，EarthReason 通过引入空目标样例和不同的空间尺度，推动模型在复杂的真实遥感场景中实现更强的泛化能力。

% To address these challenges, we introduce the geospatial pixel reasoning task and curate the first large-scale benchmark, EarthReason, for geospatial pixel reasoning. EarthReason comprises 5,434 manually annotated remote sensing image-mask pairs drawn from diverse classification sources, covering 28 scene categories at spatial resolutions ranging from 0.5m to 153m. Each image is paired with multiple implicit reasoning questions that demand the model to infer target masks based on contextual and domain-specific knowledge, rather than explicit object names. Further, by incorporating empty-target cases and varying spatial scales, EarthReason pushes models to generalize across complex, real-world remote sensing scenarios.

近期在多模态大语言模型 MLLM 方面的进展在自然图像领域表现出显著效果，例如 LISA~\cite{lai2024lisa} 和 PixelLM~\cite{ren2024pixellm} 等模型借助 large language models (LLM)~\cite{touvron2023llama, chiang2023vicuna, yang2024qwen2} 来解析丰富的文本提示并生成像素级输出。这类框架在诸如推理分割~\cite{lai2024lisa} 的任务上尤为擅长——此类任务中目标掩模并未被直接指定，而需从细微的语言线索中推断出来。遗憾的是，将这些方法直接迁移到地理空间像素推理并非易事，因为遥感影像存在极端的尺度变化、密集的小尺度目标以及超高分辨率，这些特性违背了自然图像的常见假设。此外，区别于自然图像，遥感查询通常还需要考虑显著的空间关联性。例如，识别“informal settlements”通常需要同时检测屋顶材料的不规则性、道路网络的碎片化程度以及与合法土地使用区的空间邻接关系。

为了解决这些挑战，我们提出了 SegEarth-R1，一种简单而有效的语言引导分割模型。该模型融合了分层视觉编码器、用于指令解析的 LLM 以及为空间相关性定制的掩码生成器。此外，若干组件还针对遥感影像的特性进行了适配。具体而言，我们提出了用于处理超高分辨率图像的激进视觉令牌压缩策略、用于将语言与多尺度特征融合的描述投影模块，以及直接查询描述嵌入的精简掩码预测流程。尽管架构较为简洁，SegEarth-R1 在 EarthReason 和指称分割数据集上均取得了领先表现，显著优于传统以及基于 LLM 的分割方法。

综上所述，我们的贡献如下：
\begin{itemize}
[leftmargin=8pt]
\item 我们提出了地理空间像素推理任务，该任务要求模型通过对空间上下文和领域知识的推理，从隐含的自然语言查询中推断出分割掩码。
\item 我们构建并发布了首个大规模基准数据集，包含 5,434 对图像-掩码、28 个类别，以及超过 30,000 个隐式问答对，以促进地理空间像素推理的研究。
\item 我们提出了一种基于 LLM 的分割模型 SegEarth-R1，为遥感分割引入了新的能力，并包含若干领域特定的设计。
\item 大量实验表明，Compared to traditional methods and other LLM-based methods, SegEarth-R1 在推理与指称分割任务上达到了最先进的性能。
\end{itemize}



\section{相关工作}
\label{sec:related_work}

% 1. 参考分割
% 2. 推理分割
% 3. 基于LLM/MLLM的分割方法

% \subsection{}


\subsection{指称分割}
% 自然图像->遥感图像

指代分割旨在根据自然语言描述对图像中的目标进行分割，要求语言表达与视觉内容之间精确对齐。早期方法采用了 CNN-RNN/LSTM 框架~\cite{hu2016segmentation, liu2017recurrent, li2018referring, margffoy2018dynamic, shi2018key, huang2020referring} 来提取视觉特征并编码文本查询，但由于局部感受野有限且跨模态交互不足，这些方法难以处理复杂表达~\cite{ji2024survey}。为了解决这些局限，注意力机制~\cite{vaswani2017attention} 成为关键技术~\cite{ding2021vision, yang2022lavt, wu2022language, hu2023beyond, xu2023bridging, nag2024safari, wu2024towards, shang2024prompt}。VLT~\cite{ding2021vision} 基于图文交互动态生成自适应查询向量，通过跨模态注意力实现精确定位。LAVT~\cite{yang2022lavt} 更进一步，在 Swin Transformer~\cite{liu2021swin} 主干中融入分层的视觉-语言融合，利用像素-词语注意力精炼多尺度特征以达到细粒度语义对齐。在遥感领域，为特定实例指定分割有助于提升解读效率和用户交互性。最近，Yuan \textit{et al.}~\cite{yuan2024rrsis} 首次将指代分割引入卫星影像。随后，基于 LAVT 架构的 RMSIN~\cite{liu2024rotated} 也通过加入自适应旋转卷积来应对尺度和方向的变化~\cite{yang2022lavt}。FIANet~\cite{lei2024exploring} 与 CroBIM~\cite{dong2024cross} 引入了更为精细的跨模态交互以对齐特征。RSSep~\cite{ho2024rssep} 则将指代分割重新表述为序列到序列任务，预测多边形边界以处理尺度变化和边缘模糊~\cite{liu2023polyformer}。然而，现有方法虽能有效执行明确指令下的目标分割，却缺乏对隐含意图的推理能力。本文提出的地理空间像素推理任务超越了指代分割，依托 LLM 的推理能力来解读细微指令并精确分割目标。


% \subsection{推理分割}
\subsection{基于 LLM 的分割}

最近在 LLM 方面的进展显著扩展了其将像素级分割与语言推理相结合的能力~\cite{xiao2023florence, wang2023visionllm, wu2025visionllm, beyer2024paligemma, steiner2024paligemma, zhang2024next, yuan2025sa2va, he2024multi}。例如，Florence-2~\cite{xiao2023florence} 通过带有任务指令的序列到序列框架，将文本、检测与分割统一起来。为应对真实世界分割场景的复杂性，部分研究侧重于架构专门化与指令感知的适配。LISA~\cite{lai2024lisa, yang2023lisa++} 通过引入 \texttt{[SEG]} token 将 LLM 与诸如 SAM~\cite{kirillov2023segment} 的分割解码器连接起来，奠定了语言引导掩膜预测的范式。后续工作对该范式进行了增强：GSVA~\cite{xia2024gsva} 提出共享权重的 \texttt{[SEG]} token 与 \texttt{[REJ]} token 以处理多目标与空目标场景~\cite{liu2023gres, ren2024pixellm, zhang2024groundhog}，而 GLaMM~\cite{rasheed2024glamm} 则通过整体分割实现了像素级的对话能力~\cite{zhou2024instruction}。与此同时，也有工作致力于架构统一——PSALM~\cite{zhang2024psalm} 为多任务分割建立了灵活的输入方案，OMG-LLaVA~\cite{zhang2025omg} 将通用分割主干与 LLM 结合以实现像素级推理。面向视频理解的拓展由 VISA~\cite{yan2024visa} 与 InstructSeg~\cite{wei2024instructseg} 提出，二者引入了时序推理。值得注意的是，Text4Seg~\cite{lan2024text4seg} 将分割重新定义为使用语义描述符的文本生成问题，从而无需额外的解码器。在遥感领域，受上述范式的启发~\cite{lai2024lisa, lan2024text4seg}，一些统一模型如 RSUniVLM~\cite{liu2024rsunivlm}、GeoGround~\cite{zhou2024geoground} 与 GeoPix~\cite{ou2025geopix} 被赋予了分割能力。尽管这些模型基于 LLM，但它们仅聚焦于显式文本引导的分割。此外，GeoPixel~\cite{shabbir2025geopixel} 将 grounded conversation generation~\cite{rasheed2024glamm} 引入遥感，但仍未提供推理能力。我们的 SegEarth-R1 同样遵循基于 LLM 的分割范式，但与以往方法不同：SegEarth-R1 首次支持从隐式查询中对目标区域进行推理，其各组件也针对遥感场景的挑战进行了专项设计。

% the components in SegEarth-R1 are well-designed for the challenges in remote sensing.


\section{基准地理空间像素推理数据集——EarthReason}

\begin{table*}
  \caption{EarthReason 与其他相关数据集的比较。  
其中 {\color{white!50!black}gray} 渲染表示自然图像数据集。  
``Seg''、``Det''、``VG''、``Cls'' 分别表示分割、检测、视觉定位和分类数据集。}
  \label{table_data}
  \centering
  \scalebox{0.63}{
  \begin{tabular}{@{}lcccccccccc@{}}
    \toprule[1pt]
数据集 & 掩码标签 & 推理查询 & 空间分辨率 & 图像大小 & 图像数量 & 图像来源 & 类别数 \\
    \midrule[1pt]
{\color{white!50!black}ReasonSeg} \cite{lai2024lisa} & {\color{white!50!black}\checkmark} & {\color{white!50!black}\checkmark} & {\color{white!50!black}-} & {\color{white!50!black}-} & {\color{white!50!black}1,218} & \makecell[c]{{\color{white!50!black}OpenImages (Seg) \&} \\ {\color{white!50!black}ScanNetv2 (Seg)}} & {\color{white!50!black}-} \\
    {\color{white!50!black}LLM-Seg40K} \cite{wang2024llm} & {\color{white!50!black}\checkmark} & {\color{white!50!black}\checkmark} & {\color{white!50!black}-} & {\color{white!50!black}-} & {\color{white!50!black}14,000} & \makecell[c]{{\color{white!50!black}LVIS (Seg) \&} \\ {\color{white!50!black}EgoObjects (Seg)}} & {\color{white!50!black}-} \\
    \midrule[1pt]
EarthVQA~\cite{wang2024earthvqa} & \ding{55} & \checkmark & 0.3m & $1024^2$ & 6,000 & LoveDA (Seg) & 14 \\
    RegSegRS~\cite{yuan2024rrsis} & \checkmark & \ding{55} & 0.5m-30m & $800^2$ & 4,420 & SkyScapes (Seg) & 14 \\
    RRSIS-D~\cite{liu2024rotated} & \checkmark & \ding{55} & 0.13m & $512^2$ & 17,402 & RSVGD (VG) \& DIOR (OD) & 20 \\
    % VRSBench~\cite{li2024vrsbench} & \checkmark & \ding{55} & 
RISBench~\cite{dong2024cross} & \checkmark & \ding{55} & 0.1m-30m & $512^2$ & 52,472 & DOTAv2(OD) \& DIOR (OD) & 26 \\
    \midrule
EarthReason & \checkmark & \checkmark & 0.5m-153m & $123^2$-$7617^2$ & 5,434 & AID (Cls) \& fMoW (Cls) & 28  \\
    \bottomrule[1pt]
  \end{tabular}}
\end{table*}

% Million-AID

\subsection{与相关数据集的比较}


我们分析了三类与地理空间像素推理相关的任务和数据集，\ie, 自然图像推理分割、遥感视觉问答（VQA）和遥感指代分割，如 Table~\ref{table_data} 所示。RefSegRS~\cite{yuan2024rrsis} 和 RRSIS-D~\cite{liu2024rotated} 提供了早期包含图像—文本—掩码三元组的基准。RISBench~\cite{dong2024cross}，迄今为止最大的 RRSIS 数据集，通过半自动流程引入了 52,472 个带有定向边界框和像素级掩码的三元组。这些数据集弥补了早期以文本为主的数据集（\eg, RSICD~\cite{lu2017exploring}, EarthVQA~\cite{wang2024earthvqa}, \etc）的局限，使多模态模型得以进行更全面的评估。与此前的指代分割数据集相比，EarthReason 具有以下特点： \textbf{(1)} EarthReason 中的掩码标签并非由查询显式指定，而是需要进一步推理以确定目标，从而对模型的推理能力提出了更高要求。 \textbf{(2)} EarthReason 使用更原始的数据来源。此前相关数据集通常直接转换自现有的分割数据集~\cite{azimi2019skyscapes, wang2021loveda}或基于 SAM 处理的检测数据集~\cite{zhan2023rsvg, li2020object, ding2021object}，而 EarthReason 则采用来自分类数据集~\cite{long2021creating, christie2018functional} 的图像并由人工标注，这在统一分割任务的联合训练中能带来更多的数据增益。 \textbf{(3)} EarthReason 拥有更多样的空间分辨率和图像尺寸，有利于解决遥感图像中固有的对象尺度跨越问题~\cite{rolf2024mission}。与第一个自然图像推理分割数据集 ReasonSeg 相比，EarthReason 的数据量多出 $4.46\times$。因此，我们认为 EarthReason 作为遥感领域首个地理空间像素推理数据集，能够对该任务开展初步探索。

\subsection{数据集生成流程}
我们的基准数据集 EarthReason 按照以下三个步骤生成：图像收集、问答对生成和对象掩码标注。

\noindent
\textbf{Image Collection.} 如上所述，为了在未来构建用于遥感的统一分割模型时避免潜在的数据泄露，我们从现有的分类数据中收集图像。虽然这增加了标注成本，但也促使场景更为多样。具体而言，我们首先从 Million-AID~\cite{long2021creating} 数据集中筛选出更适合推理的 28 个类别，并为每个类别采样约 200 张图像。随后我们发现 Million-AID 的图像所涵盖的实际地理范围有限。因此，我们还从 fMoW~\cite{christie2018functional} 数据集中收集了 800 张图像，以增强模型在复杂场景下的推理能力。进一步地，为缓解虚构幻觉问题~\cite{pang2024vhm}，我们额外加入了 200 张空目标图像（\ie，隐含目标不在图像中）。最后，剔除了一些低质量图像，最终得到共计 5,434 张图像。


\noindent
\textbf{Question-Answer Pair Generation.} 我们使用 GPT-4o\footnote{\url{https://platform.openai.com/docs/models/gpt-4o}} 来构建问答对。鉴于其出色的视觉理解能力，我们将遥感影像及相应的场景类别（由 Million-AID 和 fMoW 提供）作为提示的一部分，以生成与影像密切相关的问题与答案。此类提示的示例见附录~\ref{sec:appendix_ann}。此外，遵循~\cite{lai2024lisa} 的做法，为了提高问题与答案的多样性，我们采用 GPT-3.5 对指令性问题和答案进行改写，参见附录图~\ref{fig:data_gen2}。

\noindent
\textbf{Object Mask Labeling.} 不同于先前的指称和推理分割数据集（这些数据集使用现成的掩码或边界框），我们从头对图像进行标注。具体而言，我们聘请了多名遥感与视觉领域的专家，每位专家分配数百张图像进行标注，并在标注完成后对标注结果进行交叉验证。对于简单目标（\eg, lake），使用 SAM-H~\cite{kirillov2023segment} 辅助标注；对于复杂目标（\eg, wind turbine），对多边形的每个点进行精细标注。关于掩码质量的说明见附录~\ref{sec:appendix_ann}。

% 参考分割和地理空间像素推理的比较。(a) RRSIS-D和RefSegRS数据集中的样本。(b) EarthReason数据集中的样本。
% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1.0\linewidth]{figures/task.pdf}
%    \caption{\small 指称分割与地理空间像素推理的比较。（左）来自 RRSIS-D~\cite{liu2024rotated} 和 RefSegRS~\cite{yuan2024rrsis} 数据集的样本。（右）来自 EarthReason 数据集的样本。}
%    \label{fig:data_sample}
%    \vspace{-1em}
% \end{figure}

% \subsection{数据集统计}
\noindent
\textbf{Dataset Statistics.}
EarthReason 数据集被划分为训练集、验证集和测试集，分别包含 2,371、1,135 和 1,928 张图像。训练集中每张图像平均标注了 6 个问题和 3 个对应答案。问题的平均长度为 20.86 个词，答案的平均长度为 26.76 个词。为评估模型的泛化能力，故意将若干语义类别保留在验证集和测试集中，确保这些类别在训练阶段未被见到。更多数据集细节见附录~\ref{sec:appendix_ann_stat}。


\section{基线地理空间像素推理方法——SegEarth-R1}

与自然图像相比，遥感影像具有独特特性，这要求在像素级地理空间推理任务中采用专门的架构设计。在本工作中，我们提出了 SegEarth-R1——一个简单但功能强大的像素级地理空间推理基线，它在融入领域特定适配的同时，有效发挥了 LLM 的能力。如图 Figure~\ref{fig:method} 所示，我们的架构由三大核心模块组成：用于图像特征提取的视觉编码器、用于指令解析与语义关联的 LLM，以及用于空间相关建模与掩膜预测的掩膜生成器。每一模块均包含针对遥感影像独特挑战的关键设计考量。

\subsection{分层视觉编码器}

\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/method2.pdf}
   \caption{\small 所提出的 SegEarth-R1 架构概览。对于输入图像 $X_v$ 和文本描述 $X_q$，采用分层视觉编码器和所提出的连接器来提取并压缩视觉 tokens。随后，将视觉 tokens \includegraphics[scale=0.004,valign=c]{figures/visual_token.png} 和描述嵌入 \includegraphics[scale=0.004,valign=c]{figures/text_token.png} 输入到 LLM 中，用于指令解析和语义关联。最后，将描述嵌入直接映射为查询向量，用于空间相关性计算及分割掩码生成。}
   \label{fig:method}
   \vspace{-1em}
\end{figure}


卫星与航拍目标面临两大关键挑战：(1) 尺度极端变化，从亚米级目标到公里级地形构造~\cite{rolf2024mission}；(2) 小目标密集分布，需进行高分辨率分析~\cite{li2024segearth}。由于固定尺度的特征提取以及通过激进的patch合并导致的信息压缩，MLLM~\cite{lai2024lisa, yang2023lisa++, kirillov2023segment, xia2024gsva}中常用的基于ViT的编码器（\eg，CLIP~\cite{radford2021learning}和SAM~\cite{kirillov2023segment, ravi2024sam}中的图像编码器）在此类场景下表现欠佳。为缓解这些局限，依据~\cite{zhang2024psalm}，SegEarth-R1采用了带有渐进式特征层次构建的Swin Transformer~\cite{liu2021swin}主干。该架构通过受控下采样生成多尺度特征图 $v_h, h\in [1,4]$，其分辨率为原图的{1/4, 1/8, 1/16, 1/32}，既保留了对小目标有利的高分辨率细节，又在更深层次捕捉了上下文语义。

% The shifted window mechanism further enables cross-window connectivity without sacrificing computational efficiency, making it particularly suitable for analyzing large-scale geospatial images.

\subsection{大型语言模型与输入模式}

SegEarth-R1 采用 MLLM 范式~\cite{liu2023visual, li2023blip}，通过将视觉标记和文本指令联合嵌入到统一的 LLM 输入空间来实现多模态推理。与自然图像不同，遥感数据具有超高分辨率覆盖~\cite{ji2023ultra, wang2025xlrs}，在通过十亿级别的 LLM 处理时带来了计算上的挑战。因此，我们希望压缩视觉标记以缓解计算开销，并仅在 LLM 中进行简单的语义关联。

\subsubsection{视觉标记} 


\textbf{Redundancy Analysis.} 图像冗余度量了图像中可压缩且非信息性数据的比例。为了评估对遥感影像实施激进视觉令牌压缩的可行性，我们从像素级统计冗余和空间结构冗余两个层面对冗余性进行了分析。

% To confirm the feasibility of over-compressing visual tokens on remote sensing images, we analyze the redundancy of remote sensing images from two aspects.

\begin{itemize}
[leftmargin=8pt]\item 根据信息论~\cite{shannon1948mathematical}，熵衡量图像的平均不确定性或信息量，而最大熵对应像素值均匀分布的理想情形（\ie，无冗余）。因此，从熵的角度，图像的冗余可以定义为~\cite{gonzales1987digital}：
\begin{equation}
\begin{aligned}
  R_e = 1 - \frac{-\sum_{l=0}^{L-1} p(l) \log _{2} p(l)}{\log _{2} L},
  \label{eq:entropy}
\end{aligned}
\end{equation}
其中 $L$ 表示不同强度级别的数量（\eg，对于 8 位灰度图像为 $L=256$），而 $p(l)$ 表示像素强度值 $l$ 的概率质量函数。
\end{itemize}


\begin{itemize}
[leftmargin=8pt]\item 除了像素级的统计冗余之外，结构自相似性反映了由重复模式（\eg 纹理、几何特征）引起的空间冗余。为量化这一点，我们采用结构相似性指数矩阵（SSIM）~\cite{wang2004image}来衡量补丁之间的相似性。对于被划分为 $N$ 个补丁的图像，SSIM 矩阵 $\mathbf{M} \in \mathbb{R}^{N \times N}$ 定义为：
\begin{equation}
\begin{aligned}
\mathbf{M}(i,j) = \frac{(2\mu_i\mu_j + C_1)(2\sigma_{ij} + C_2)}{(\mu_i^2 + \mu_j^2 + C_1)(\sigma_i^2 + \sigma_j^2 + C_2)}, \quad \forall i,j \in {1, ..., N}
\label{eq:ssim}
\end{aligned}
\end{equation}
其中 $\mu_{i}$、$\sigma_{i}$ 分别表示第 $i$ 个补丁的均值和方差，$\sigma_{ij}$ 表示第 $i$ 与第 $j$ 个补丁之间的协方差，$ C_{1}$、$C_{2}$ 为稳定性常数。然后，结构自相似冗余 $R_{s}$ 通过对 $\mathbf{M}$ 的非对角元素求平均得到：
\begin{equation}
\begin{aligned}
R_s = \frac{1}{N(N-1)} \sum_{i \neq j} \mathbf{M}(i,j).
\label{eq:ssim_redundancy}
\end{aligned}
\end{equation}
\end{itemize}


\begin{wrapfigure}{r}{0.6\textwidth} 
  \centering
  \vspace{-5pt}
  \subfloat[pixel-level redundancy]{\label{fig:redundancy_a}
    \includegraphics[width=0.28\textwidth]{figures/redundancy_a.pdf}
  }
  \subfloat[spatial structure redundancy]{\label{fig:redundancy_b}
    \includegraphics[width=0.28\textwidth]{figures/redundancy_b.pdf}
  }
  \caption{\small 对遥感数据集与自然图像的冗余性分析表明，前者具有更高的冗余性。}
  \label{fig:simg_pair}
  \vspace{-1em}
\end{wrapfigure}

我们为冗余性分析评估了六个基准数据集，涵盖自然影像（COCO~\cite{caesar2018coco}、ADE20K~\cite{zhou2017scene}、PASCAL~\cite{everingham2010pascal}）和遥感影像（LoveDA~\cite{wang2021loveda}、DeepGlobe~\cite{demir2018deepglobe}、xBD~\cite{gupta2019xbd}）。如图~\ref{fig:simg_pair}所示，我们的分析揭示了两个关键发现：1) 遥感影像在熵冗余方面比自然影像高出 1.9$\sim$3.3$\times$，表明其在像素层面具有更强的可压缩性；2) 遥感数据的平均自相似性比自然影像高出 42.6\%，进一步证实了重复纹理和几何模式更为普遍。该洞见为在遥感影像的语义层面采用激进的 token 压缩提供了充分依据。


\textbf{Token Compression Connector.}
在现代 MLLM 中，诸如 Q-Former~\cite{li2023blip} 和 MLP~\cite{liu2023visual} 的连接器被设计用于将视觉 tokens 转换到多模态空间。然而，一些工作~\cite{cha2024honeybee, yao2024deco} 指出 Q-Former 可能导致视觉信息丢失且难以训练。因此，在 SegEarth-R1 中，我们遵循 LLaVA~\cite{liu2023visual} 中的 MLP 连接器风格，采用一种简单但有效的连接器，\ie，堆叠的卷积块与 Layer Normalization (LN)。其中，卷积块用于空间下采样以压缩特征图尺寸，LN 则用于稳定跨模态训练。具体地，我们的连接器可以表述为：
\begin{equation}
\begin{aligned}
v_{out}=(Conv \circ LN)^{d}(v_4),
\label{eq:connector}
\end{aligned}
\end{equation}
其中 $\circ$ 表示函数复合算子，$d$ 表示堆叠层数。


\subsubsection{文本指令}


尽管地理像素推理中的指令是隐含的、且比指称分割包含更多文字，但它们仍然保持相同的数据格式。因此，可以很容易地使用如下模板将其转换为问答对：``\textbf{USER}: 这是图像 <IMAGE>，请根据以下指令进行地理像素推理：<DESCRIPTION>。 \textbf{ASSISTANT}: <ANSWER>''。对于 referring segmentation 任务，指令中的任务名称改为``referring segmentation''。


\subsection{考虑空间相关性的掩膜生成}

一些最近基于 LLM 的分割模型~\cite{zhang2024psalm, wei2024instructseg} 采用 Mask2Former~\cite{cheng2022masked} 范式作为掩码生成器。它们在 transformer 解码器中使用 $T$ 个可学习的掩码 token（通常 $T$=100）作为查询，生成带有对应分数的 $T$ 个候选掩码，然后通过二分匹配将这些掩码分配给描述嵌入\footnote{embeddings of <DESCRIPTION> in text instruction.}。与更倾向于基于目标自身属性进行推理的自然图像推理分割不同，在地理空间像素推理中，模型必须更多地基于图像内的空间布局和对象间的关联进行理解和推断（\eg，识别 Figure~\ref{fig:method} 中的地震撤离区就需要分析道路与建筑之间的拓扑关系）。此外，我们认为掩码查询机制~\cite{cheng2021per, cheng2022masked} 在语言引导的分割任务中显得不够灵活且存在冗余；实际上只需根据指令生成可变数量的掩码即可。基于上述动机，我们提出直接将描述嵌入作为掩码生成器的查询，并将其与图像的空间特征显式关联。


\begin{wrapfigure}{r}{0.25\textwidth}
    \vspace{-15pt}
    \centering
    \includegraphics[width=\linewidth]{figures/pool.pdf} 
    \vspace{-10pt}
    \caption{\small $D$-投影器.}
    \label{fig:pool}
    \vspace{-10pt}
\end{wrapfigure}


描述嵌入的长度随用户指令而变化，而在我们的地理空间像素推理或指称分割设置中，分割结果可以由单个二值掩码表示。因此，我们引入了描述投影模块（$D$-Projector），用于将整个描述转换为单一向量，如图~\ref{fig:pool}所示。具体地，先将描述嵌入求平均得到全局向量，然后通过交叉注意力与展平的多尺度视觉特征进行交互，随后通过跳跃连接和线性层将其映射为查询向量。接着，该查询向量被送入 Mask2Former 的 Transformer 解码器，后者由堆叠的掩蔽注意力、自注意力和 FFN 组成。值得注意的是，由于已移除 mask query 机制，生成的掩码数与查询数相同，因此不再需要得分预测和二分匹配。最后，用 focal loss~\cite{lin2017focal} 与 dice loss~\cite{milletari2016v} 的线性组合作为对预测掩码的监督。


\section{实验}

\subsection{实验设置}

\noindent
\textbf{Datasets and Tasks.} 除了 EarthReason 上的地理空间像素推理外，我们还通过对模型进行明确的短查询来评估其基本地理定位能力。我们采用了两个基准的指称图像分割数据集：RefSegRS~\cite{yuan2024rrsis} 和 RRSIS-D~\cite{liu2024rotated}。这些数据集分别包含 14 和 20 个语义类别，文本描述主要侧重于方向、颜色和大小等直接视觉属性。所有模型均在各自的训练集上训练，并在对应的验证集和测试集上进行评估。

% Notably, to evaluate the model's out-of-domain capability, we placed several categories exclusively in the validation and test sets, making them unseen during training. Further details can be found in the supplementary materials.



\noindent
\textbf{Evaluation metrics.} 参照~\cite{lai2024lisa}，我们采用两项评估指标：\textbf{gIoU}（每幅图像的 IoU 平均）和 cIoU（累积交并比）。后者因其稳定性而被优先采用。

\noindent
\textbf{Network Architecture.} 除非另有说明，SegEarth-R1 使用 phi-1.5 (1.3B)~\cite{li2023textbooks} 作为 LLM，并采用 Swin-B 作为视觉编码器。令牌压缩连接器的层数配置为 $d=2$。掩码生成器遵循 Mask2Former 架构，但如上所述移除了掩码令牌。

\noindent
\textbf{Implementation details.} 在训练过程中，我们使用 bf16 精度并冻结视觉编码器。LLM 从 Phi-1.5 初始化，而 Swin-B 编码器和掩码生成器均采用 Mask2Former 的预训练权重初始化。所有图像被调整为 $1024 \times 1024$，通过对短边填充以保持原始宽高比。我们采用 AdamW 优化器，初始学习率为 $1 \times 10^{-4}$，使用余弦学习率调度且不使用权重衰减。在各数据集上统一使用批量大小 16，训练步数分别设为 7,610（RRSIS-D）、5,400（RefSegRS）和 2,220（EarthReason）。所有实验均在两块 NVIDIA A100 80GB GPU 上进行。

\subsection{地理空间像素推理结果}


\begin{wraptable}{l}{0.7\textwidth}
\vspace{-1em}
    \caption{\small SegEarth-R1（我们的方法）与先前相关工作的地理空间像素推理结果比较。}
    \label{tab:earthreason}
    \centering
    \scalebox{0.75}{
    \begin{tabular}{@{}l|c|c|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Method} & \multirow{2}{*}{Visual Encoder} & \multirow{2}{*}{LLM Type} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & & & Val & Test & Val & Test \\
    \midrule[1pt]
LISA~\cite{lai2024lisa} & CLIP-L & Vicuna-7B~\cite{chiang2023vicuna} & 57.39 & 59.10 & 61.04 & 60.88 \\
    PixelLM~\cite{ren2024pixellm} & CLIP-L & Vicuna-7B~\cite{chiang2023vicuna} & 57.79 & 59.22 & 57.94 & 60.01 \\
    % Text4Seg~\cite{lan2024text4seg} &  &  &  &  &  &  \\
PSALM~\cite{zhang2024psalm}，骨干为 Swin-B，使用 phi-1.5 (1.3B)~\cite{li2023textbooks}，各项得分分别为 62.03、64.61、66.61 和 68.30。\\
\textit{SegEarth-R1}，骨干为 Swin-B，使用 phi-1.5 (1.3B)~\cite{li2023textbooks}，各项得分分别为 \textbf{64.13}、\textbf{68.25}、\textbf{68.60} 和 \textbf{70.75}。\\
    \bottomrule[1pt]
\end{tabular}}
\vspace{-1em}
\end{wraptable}

我们在 EarthReason 数据集上对 SOTA LLM-based 方法与 SegEarth-R1 进行了比较评估。如 Table~\ref{tab:earthreason} 所示，所有模型均仅在 EarthReason 的训练集上训练，以确保比较公平。LISA 与 PixelLM 的表现相当；然而，尽管它们利用了更大的 LLM 或 MLLM，其预测的分割掩码质量仍然不理想。这主要归因于它们依赖 CLIP 作为视觉编码器，CLIP 往往削弱了对小尺度地理目标的表征。作为 SegEarth-R1 的基线之一，PSALM 相较于 LISA 和 PixelLM 取得了显著提升。尽管如此，PSALM 并未充分结合基于 LLM 的分割与 Mask2Former 范式，也缺乏对航拍图像（overhead images）的专门考量。SegEarth-R1 在两项指标上均取得了最优结果，在测试集上分别比 PSALM 高出 3.64\% 和 2.45\%。值得注意的是，SegEarth-R1 在 LLM 中使用更少的视觉 tokens，并减少了掩码生成器中的 queries 数量，从而降低了推理成本。


\subsection{指称分割结果}


\begin{wraptable}{r}{0.55\textwidth}
    \vspace{-1em}
    \caption{\small 在 RRSIS-D 数据集上，SegEarth-R1 与以往相关工作在指代分割任务上的结果比较。}
    \label{tab:RRSIS_D}
    \centering
    \scalebox{0.65}{
    \begin{tabular}{@{}l|c|cc|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Method} & & \multicolumn{2}{c|}{P@0.5} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & & Val & Test & Val & Test & Val & Test \\
    \midrule[1pt]
\multicolumn{8}{@{}l}{\textbf{\textit{Traditional method:}}} \\
    RRN~\cite{li2018referring} & {\tiny CVPR'18} & 51.09 & 51.07 & 66.53 & 66.43 & 46.06 & 45.64 \\
    CSMC~\cite{ye2019cross} & {\tiny CVPR'19} & 55.68 & 55.32 & 69.39 & 69.39 & 48.85 & 48.54 \\
    LSCM~\cite{hui2020linguistic} & {\tiny ECCV'20} & 57.12 & 56.02 & 69.05 & 69.28 & 50.36 & 49.92 \\
    CMPC~\cite{huang2020referring} & {\tiny CVPR'20} & 57.93 & 55.83 & 69.22 & 69.39 & 50.41 & 49.24 \\
    BRINet~\cite{hu2020bi} & {\tiny CVPR'20} & 58.79 & 56.90 & 70.73 & 69.88 & 51.14 & 49.65 \\
    CMPC+~\cite{liu2021cross} & {\tiny TPAMI'20} & 59.19 & 57.65 & 70.14 & 68.64 & 51.41 & 50.24 \\
    LGCE~\cite{yuan2024rrsis} & {\tiny TGRS'24} & 68.10 & 67.65 & 76.68 & 76.34 & 60.16 & 59.37 \\
    RIS-DMMI~\cite{hu2023beyond} & {\tiny CVPR'23} & 70.40 & 68.74 & 77.01 & 76.20 & 60.72 & 60.12 \\
    LAVT~\cite{yang2022lavt} & {\tiny CVPR'22} & 69.54 & 69.52 & 77.59 & 77.19 & 61.46 & 61.04 \\
    RMSIN~\cite{liu2024rotated} & {\tiny CVPR'24} & 74.66 & 74.26 & 78.27 & 77.79 & 65.10 & 64.20 \\
    \midrule[1pt]
\multicolumn{8}{@{}l}{\textbf{\textit{LLM-based method:}}} \\
    LISA~\cite{lai2024lisa} & {\tiny CVPR'24} & 27.07 & 24.51 & - & - & 27.84 & 26.78 \\
    PixelLM~\cite{ren2024pixellm} & {\tiny CVPR'24} & 33.46 & 28.81 & - & - & 33.89 & 31.65 \\
    NEXT-Chat~\cite{zhang2023next} & {\tiny arXiv'23} & 28.97 & 26.37 & - & - & 26.98 & 24.98 \\
    GeoGround~\cite{zhou2024geoground} & {\tiny arXiv'25} & 68.69 & 67.50 & - & - & 61.10 & 60.50 \\
    \midrule[1pt]
    \multicolumn{2}{@{}l|}{\model} & \textbf{78.62} & \textbf{76.96} & \textbf{78.92} & \textbf{78.01} & \textbf{67.56} & \textbf{66.40} \\
    \bottomrule[1pt]
    \end{tabular}}
    % \vspace{-1em}
\end{wraptable}


SegEarth-R1 还支持基础的显式语言引导分割。如 Table~\ref{tab:RRSIS_D} 所示，我们将其性能与现有的 SOTA 传统方法（不基于 LLM）以及近期基于 LLM 的方法进行了比较。值得注意的是，在 SegEarth-R1 之前，基于 LLM 的方法在指称分割任务上一直落后于传统方法。例如，先进的 GeoGround~\cite{zhou2024geoground} 在 RRSIS-D 数据集上的 gIoU 比 RMSIN~\cite{liu2024rotated} 低 3.7\%。相比之下，作为一种通用的基于 LLM 的语言引导分割方法，SegEarth-R1 首次在指称分割任务上超越了传统方法，提升了 2.2\%，该结果凸显了 SegEarth-R1 更强的泛化能力与实际应用潜力。在 RefSegRS 数据集上，SegEarth-R1 的提升更加显著：在验证集和测试集上分别较 RMSIN 提升了 8.33\% 和 9.87\%，详见 Table~\ref{tab:RefSegRS}。


\begin{table}[h]
    \caption{\small SegEarth-R1 与先前相关工作在 RefSegRS 数据集上的指称分割结果。}
    \label{tab:RefSegRS}
    \centering
    \scalebox{0.66}{
    \begin{tabular}{@{}l|c|cc|cc|cc|cc|cc|cc|cc@{}}
    \toprule[1pt]
\multirow{2}{*}{Method} & & \multicolumn{2}{c|}{P@0.5} & \multicolumn{2}{c|}{P@0.6} & \multicolumn{2}{c|}{P@0.7} & \multicolumn{2}{c|}{P@0.8} & \multicolumn{2}{c|}{P@0.9} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    % \hline
    & & Val & Test & Val & Test & Val & Test & Val & Test & Val & Test & Val & Test & Val & Test \\
    \midrule[1pt]
    % \multicolumn{15}{@{}l}{\textbf{\textit{Traditional method:}}} \\
    % \textbf{\textit{Traditional method:}} & & & & & & & & & & & & & & \\
BRINet~\cite{hu2020bi} & {\tiny CVPR'20} & 36.86 & 20.72 & 35.53 & 14.26 & 19.93 & 9.87 & 10.66 & 2.98 & 2.84 & 1.14 & 61.59 & 58.22 & 38.73 & 31.51 \\
    LSCM~\cite{hui2020linguistic} & {\tiny ECCV'20} & 56.82 & 31.54 & 41.24 & 20.41 & 21.85 & 9.51 & 12.11 & 5.29 & 2.51 & 0.84 & 62.82 & 61.27 & 40.59 & 35.54 \\
    CMPC~\cite{huang2020referring} & {\tiny CVPR'20} & 46.09 & 32.36 & 26.45 & 14.14 & 12.76 & 6.55 & 7.42 & 1.76 & 1.39 & 0.22 & 63.55 & 55.39 & 42.08 & 40.63 \\
    CMSA~\cite{ye2019cross} & {\tiny CVPR'19} & 39.24 & 28.07 & 38.44 & 20.25 & 20.39 & 12.71 & 11.79 & 5.61 & 1.52 & 0.83 & 65.84 & 64.53 & 43.62 & 41.47 \\
    RRN~\cite{li2018referring} & {\tiny CVPR'18} & 55.43 & 30.26 & 42.98 & 23.01 & 23.11 & 14.87 & 13.72 & 7.17 & 2.64 & 0.98 & 69.24 & 65.06 & 50.81 & 41.88 \\
    EVF-SAM~\cite{zhang2024evf} & {\tiny Arxiv'24} & 57.77 & 35.17 & 37.59 & 22.34 & 16.24 & 9.36 & 4.87 & 2.86 & 1.86 & 0.39 & 59.61 & 55.51 & 46.98 & 36.64 \\
    CMPC+~\cite{liu2021cross} & {\tiny TPAMI'21} & 56.84 & 49.19 & 37.59 & 28.31 & 20.42 & 15.31 & 10.67 & 8.12 & 2.78 & 0.55 & 70.62 & 66.53 & 47.13 & 43.65 \\
    CARIS~\cite{liu2023caris} & {\tiny ACMMM'23} & 68.45 & 45.40 & 47.10 & 27.19 & 25.52 & 15.08 & 14.62 & 8.87 & 3.71 & 1.98 & 75.79 & 69.74 & 54.30 & 42.66 \\
    CRIS~\cite{wang2022cris} & {\tiny CVPR'22} & 53.13 & 35.77 & 36.19 & 24.11 & 24.36 & 14.36 & 11.83 & 6.38 & 2.55 & 1.21 & 72.14 & 65.87 & 53.74 & 43.26 \\
    LAVT~\cite{yang2022lavt} & {\tiny CVPR'22} & 80.97 & 51.84 & 58.70 & 30.27 & 31.09 & 17.34 & 15.55 & 9.52 & 4.64 & 2.09 & 78.50 & 71.86 & 61.53 & 47.40 \\
    RIS-DMMI~\cite{hu2023beyond} & {\tiny CVPR'23} & 86.17 & 63.89 & 74.71 & 44.30 & 38.05 & 19.81 & 18.10 & 6.49 & 3.25 & 1.00 & 74.02 & 68.58 & 65.72 & 52.15 \\
    LGCE~\cite{yuan2024rrsis} & {\tiny TGRS'24} & 90.72 & 73.75 & 86.31 & 61.14 & 71.93 & 39.46 & 32.95 & 16.02 & 10.21 & 5.45 & 83.56 & 76.81 & 72.51 & 59.96 \\
    RMSIN~\cite{liu2024rotated} & {\tiny CVPR'24} & 93.97 & 79.20 & 89.33 & 65.99 & 74.25 & 42.98 & 29.70 & 16.51 & 7.89 & 3.25 & 82.41 & 75.72 & 73.84 & 62.58 \\
    SegEarth-R1 & & \textbf{95.82} & \textbf{86.30} & \textbf{93.27} & \textbf{79.53} & \textbf{88.86} & \textbf{69.57} & \textbf{78.19} & \textbf{48.87} & \textbf{22.04} & \textbf{10.73} & \textbf{85.01} & \textbf{79.00} & \textbf{82.17} & \textbf{72.45} \\
    \bottomrule[1pt]
\end{tabular}}
\end{table}

\subsection{消融研究}

\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/res.pdf}
   \caption{\small SegEarth-R1 在 EarthReason 上的定性结果。更多结果可见附录~\ref{sec:appendix_examples}。}
   \label{fig:res}
   \vspace{-1em}
\end{figure}


\begin{minipage}{\textwidth}
\footnotesize
\begin{minipage}[t]{0.55\textwidth}
% 原有左侧表格代码保持不变
\makeatletter\def\@captype{table}
\caption{\small 在 EarthReason 上对 SegEarth-R1 组件的消融：查询描述嵌入（Query D.E.）、描述投影器（$D$-Projector）、标记压缩连接器（T.C. Connector）。}
\label{tab:ablation}
\centering
\scalebox{0.7}{
\begin{tabular}{@{}c|c|c|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Query D.E.} & \multirow{2}{*}{$D$-Projector} & \multirow{2}{*}{T.C. Connector} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & & & Val & Test & Val & Test \\
    \midrule[1pt]
\ding{55} & \ding{55} & \ding{55} & 62.03 & 64.61 & 66.61 & 68.30 \\
    \Checkmark & \ding{55} & \ding{55} & 63.34 & 66.19 & 67.42 & 69.15 \\
    \ding{55} & \Checkmark & \ding{55} & 63.32 & 66.31 & 67.22 & 69.21 \\
    \ding{55} & \ding{55} & \Checkmark & 63.47 & 65.41 & 68.31 & 69.20 \\
    \Checkmark & \Checkmark & \ding{55} & 64.12 & 66.71 & \textbf{68.61} & 69.61 \\
    \Checkmark & \Checkmark & \Checkmark & \textbf{64.13} & \textbf{68.25} & 68.60 & \textbf{70.75} \\
    \bottomrule[1pt]
\end{tabular}}
\end{minipage}
% \hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
% 右侧改为垂直排列的两个表格
\begin{minipage}[t][0.105\textheight][t]{\textwidth}
\makeatletter\def\@captype{table}
\caption{\small 在 RRSIS-D 上针对 LLM 类型的消融实验。}
\label{table:ablation_llm}
\centering
\scalebox{0.75}{
\begin{tabular}{@{}l|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{LLM Type} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & Val & Test & Val & Test \\
    \midrule[1pt]
phi-1.5 (1.3B) & 78.92 & 78.01 & 67.56 & 66.40 \\
    phi-2 (2B) & \textbf{78.98} & \textbf{78.35} & \textbf{67.91} & \textbf{66.67} \\
    Qwen2.5 (0.5B) & 78.53 & 77.87 & 67.70 & 66.49 \\
    \bottomrule[1pt]
\end{tabular}}
\end{minipage}
% \vspace{0.5cm} % 调整垂直间距
\begin{minipage}[t][0.01\textheight][t]{\textwidth} % 新增第三个表格
\makeatletter\def\@captype{table}
\caption{\small 在 EarthReason 验证集上对 $d$ 的消融研究。}
\label{table:ablation_d}
\centering
\scalebox{0.7}{
\begin{tabular}{@{}c|c|c||c|c|c@{}}
    \toprule[1pt]
    $d$ & \#Visual Token & gIoU & $d$ & \#Visual Token & gIoU \\
    \midrule[1pt]
0 & 1024 & 68.28 & \textbf{2} & \textbf{64} & \textbf{68.60} \\
    1 & 256 & 68.47 & 3 & 16 & 68.22 \\
    \bottomrule[1pt]
\end{tabular}}
\end{minipage}
\end{minipage}
\end{minipage}




\noindent
\textbf{Components.}
我们在 EarthReason 数据集上进行了消融实验，以评估 SegEarth-R1 中各新组件的有效性。如 Table~\ref{tab:ablation} 所示，第一行给出 PSALM 基线的结果。各个提出的组件均能带来性能提升，幅度在 0.85\% 至 0.9\% 之间。T.C. Connector 和 Query D.E. 不仅提高了性能，还能降低计算开销。此外，这些组件能够良好耦合，当它们全部启用，即完整的 SegEarth-R1 时，所有指标相较基线均有显著提升，验证了所提设计的有效性和兼容性。实际上，尽管这些组件最初针对遥感场景设计，其基本原理同样为通用图像理解提供了可借鉴的见解。

\noindent
\textbf{LLM Type.}
鉴于数据集规模有限，我们选取了一些小型的 LLM 进行对比，如 Table~\ref{table:ablation_llm} 所示。SegEarth-R1 在不同的 LLM 上均表现出持续且较高的性能，表明整个框架在鲁棒性和结构稳定性方面具有优势。值得注意的是，使用 Qwen2.5 (0.5B)~\cite{yang2024qwen2} 时仍能取得具有竞争力的结果，显示出其在边缘部署方面的潜力。

\noindent
\textbf{Layer Number of T.C. Connector.}
层数 $d$ 决定送入 LLM 的视觉令牌数量。如 Table~\ref{table:ablation_d} 所示，增加令牌数量并未带来性能提升。该观察与我们之前的分析一致，表明对视觉令牌进行适当压缩有助于对遥感影像的整体理解。在 SegEarth-R1 中，图像与指令之间的空间相关性主要由掩码生成器处理，而 LLM 则只负责相对的语义相关性。这种分工使得在不牺牲性能的前提下能够更高效地利用计算资源。

\section{结论}

在本文中，我们提出了地理空间像素推理（geospatial pixel reasoning）这一遥感新任务，该任务要求模型通过对空间上下文和领域知识进行推理，从隐含的自然语言查询中推断分割掩码。为推动该方向的研究，我们构建并发布了 EarthReason——首个强调复杂推理场景的大规模基准数据集。针对遥感领域的独特挑战，我们提出了 SegEarth-R1，一种语言引导的分割模型，集成了分层视觉编码器、用于指令解析与语义关联的 LLM，以及为空间相关性专门设计的掩码生成器。大量实验证明了 SegEarth-R1 的优越性，在地理空间像素推理和指称分割任务上均取得了 SOTA 的性能。该工作开创性地将自然语言推理与像素级地理空间分析相结合，对环境监测和灾害响应等应用具有潜在的变革性价值。


\bibliography{main}
\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\appendix

\section{数据}

\subsection{EarthReason 的标注}
\label{sec:appendix_ann}

EarthReason 基准的每个样本由一幅图像、对应的掩码以及六个推理查询及其各自答案组成。鉴于我们的元数据来自分类数据集，我们采用了 GPT-4o 和 GPT-3.5 来生成文本注释，并邀请了多位遥感与视觉领域的专家参与，以提供准确可靠的掩码标注。总体而言，我们的标注流程包括以下三个步骤：

\begin{itemize}
[leftmargin=1.5em]
    \item 步骤1：为充分利用 GPT-4o 强大的多模态能力和丰富的地理知识，我们精心设计了提示语（prompt），并将其与图像及对应的类别标签一并提供，用于生成推理问答对。该提示语见 Figure~\ref{fig:data_gen1}。 
    \item 步骤2：为避免在单一提示下产生同质化的问答格式，我们进一步利用 GPT-3.5 的文本能力，将每个生成的问题扩展为六种变体，每个答案扩展为三种备选。用于该扩展的提示语见 Figure~\ref{fig:data_gen2}。
    \item 步骤3：不同于以往依赖现成边界框或掩模进行半自动掩模标注的方法，我们邀请多位遥感视觉专家在生成问题的指导下进行准确且高效的掩模标注。为进一步提高标注效率，对于部分简单目标我们引入 SAM-H 作为辅助工具。随后对标注结果进行交叉核验，并对不符合质量标准的样本重新标注。如 Figure~\ref{fig:dataset_compare} 所示，(a)、(b) 与 (c)（源自 RRSIS-D 数据集）展示了基于边界框的半自动标注掩模，其中 (a) 和 (c) 存在明显的标注错误，而 (b) 中的查询与标注不一致；(d)、(e) 与 (f) 展示了我们高质量的手工标注。
\end{itemize}



%Our metadata is derived from a remote sensing image classification dataset, which provides category labels corresponding to each image. To obtain high-quality and diverse annotations, we carefully designed prompts to leverage the powerful multimodal capabilities of GPT-4o%

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/data_gen3.pdf}
   \caption{\small 用于生成地理像素推理问答对的提示构建流程示意图。}
   \label{fig:data_gen1}
   % \vspace{-1em}
\end{figure}

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/data_gen4.pdf}
   \caption{\small 用于扩展问答对的提示构建过程示意图。}
   \label{fig:data_gen2}
   % \vspace{-1em}
\end{figure}

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/quality.pdf}
   \caption{\small 注释质量比较。(a)、(b) 和 (c) 来自 RRSIS-D 数据集，(d)、(e) 和 (f) 来自我们的 EarthReason 数据集。}
   \label{fig:dataset_compare}
   % \vspace{-1em}
\end{figure}





\subsection{EarthReason 统计信息}
\label{sec:appendix_ann_stat}

EarthReason 基准包含 28 个类别，各类别的样本数量见 Figure~\ref{fig:category} (a)。可以观察到这 28 个类别的分布相对均衡。Figure~\ref{fig:category} (b)、(c) 和 (d) 分别展示了训练集、验证集和测试集的类别分布。为评估模型的泛化能力，我们专门将四个类别——``basketball court''、``island''、``lake'' 和 ``stadium''——从训练集中剔除。此外，我们引入了 119 个空目标样本，以减轻模型可能出现的幻觉。
% In addition, we also analyze the distribution of question lengths in the benchmark, as shown in Figure 9(b).

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/category.pdf}
   \caption{\small EarthReason 的类别分布。}
   \label{fig:category}
   % \vspace{-1em}
\end{figure}

\section{附加实现细节}
\label{sec:appendix_implementation}

\subsection{训练超参数详细信息}
Table~\ref{tab:hyperparam_training} 列出了训练本模型时使用的超参数设置。在对指称分割数据集进行训练时，我们仅采用 focal loss（焦点损失）和 dice loss（Dice 损失）来监督掩码生成。相较之下，在地理空间像素推理任务的训练中，我们还额外引入来自大规模语言模型的交叉熵损失以监督文本答案的生成。

\begin{table}[h]
\centering
\caption{模型训练的超参数。}
\label{tab:hyperparam_training}
\begin{tabular}{ll}
\toprule
\textbf{Parameters} & \textbf{Value} \\
\midrule
优化器 & AdamW \\
学习率 & $1 \times 10^{-4}$ \\
批量大小 & 16 \\
迭代次数 & 7,610 / 5,400 / 2,220 \\
学习率调度 & Cosine Decay \\
权重衰减 & 0.0 \\
预热比例 & 0.03 \\
$\beta_1$ & 0.9 \\
$\beta_2$ & 0.999 \\
图像尺寸 & 1024 $\times$ 1024 \\
图像处理 & \begin{tabular}[t]{@{}l@{}}将长边调整为 1024 \\ 并将短边填充至 1024。\end{tabular} \\
\bottomrule
\end{tabular}
\end{table}

\section{示例}
\label{sec:appendix_examples}

\subsection{在 EarthReason 上的更多定性结果}

Figure~\ref{fig:vis_reason} 展示了 SegEarth-R1 与其他模型在 EarthReason 数据集上的对比。可以看出，我们的模型对长推理指令具有更好的理解能力，并能生成更为准确的掩码。

\subsection{在 RRSIS-D 上的更多定性结果}
Figure~\ref{fig:vis_ref} 对比了 SegEarth-R1 与 PSALM 在 RRSIS-D 数据集上的表现。与 PSALM 相比，我们的模型在位置、颜色和大小等直接地理属性的理解上更为出色。这一改进归因于去除了使用 mask tokens 的间接掩码预测，使得语义信息（描述嵌入）能够与图像特征直接交互以生成掩码。

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/vis_reason.pdf}
   \caption{\small 在 EarthReason 上与其它模型的比较。}
   \label{fig:vis_reason}
   % \vspace{-1em}
\end{figure}


\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/vis_ref.pdf}
   \caption{\small 与 PSALM 在 RRSIS-D 上的比较}
   \label{fig:vis_ref}
   % \vspace{-1em}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
% \section*{NeurIPS 论文检查表}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
% limit. 

% Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% \begin{itemize}
%     \item 你应该回答 \answerYes{}, \answerNo{}, 或 \answerNA{}。
%     \item \answerNA{} 表示该问题对本论文不适用，或相关信息不可用。
%     \item 请在你的答案之后（即使为 NA）紧接着给出一段简短的说明（1–2 句）。 
%    % \item {\bf The papers not including the checklist will be desk rejected.}
\end{itemize}

% {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% IMPORTANT, please:
% \begin{itemize}
%     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist"},
%     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
\end{itemize} 
 

% %%% END INSTRUCTIONS %%%


% \begin{enumerate}
% \item {\bf Claims}
%     \item[] Question: 摘要和引言中提出的主要论断是否准确反映了论文的贡献和范围？
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示摘要和引言未包含论文中提出的论断。
%         \item 摘要和/或引言应清楚陈述所做的论断，包括论文的具体贡献及重要的假设与局限。若对此问题的回答为 No 或 NA，评审者通常会持负面看法。
%         \item 所提出的论断应与理论和实验结果相符，并反映这些结果在其他情形下的可推广程度。
%         \item 将远景性目标作为动机是可以接受的，但需明确这些目标并非由论文实现。
%     \end{itemize}

% \item {\bf Limitations}
%     \item[] Question: 论文是否讨论了作者所做工作的局限性？
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文没有局限性；答案 No 表示论文存在局限性但未在文中讨论。
%         \item 鼓励作者在论文中单独设立“Limitations”小节。
%         \item 论文应指出任何强假设并讨论结果对这些假设违背时的稳健性（例如独立性假设、无噪声设定、模型完全指定、渐近近似仅在局部成立等）。作者应反思这些假设在实际中可能如何被违反及其后果。
%         \item 作者应反思所提出论断的适用范围，例如方法是否仅在少数数据集或若干次实验中测试。通常实验结果依赖隐含假设，这些应当被阐明。
%         \item 作者应讨论影响方法性能的因素。例如，面部识别算法在图像分辨率低或光照不足时可能表现差；语音转文字系统在处理专业术语时可能无法可靠用于在线讲座的自动字幕。
%         \item 作者应讨论所提算法的计算效率及其随数据规模的扩展性。
%         \item 如适用，作者应讨论其方法在隐私与公平性方面的潜在局限。
%         \item 虽然作者可能担心对局限性的完全坦诚会被评审用作否决理由，但更糟的情况是评审发现未被承认的局限。作者应以最佳判断行事，认识到透明性对维护学术共同体诚信规范的重要性。评审者将被特别指示不要因对局限性诚实陈述而惩罚作者。
%     \end{itemize}

% \item {\bf Theory assumptions and proofs}
%     \item[] Question: 对于每个理论结果，论文是否给出了完整的假设集合以及一个完整（且正确）的证明？
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文不包含理论结果。
%         \item 论文中所有的定理、公式和证明应有编号并做好交叉引用。
%         \item 所有假设应在任何定理的陈述中被明确说明或引用。
%         \item 证明可以出现在正文章节或补充材料中，但若置于补充材料，建议作者提供简短的证明思路以便提供直观理解。
%         \item 相应地，任何出现在正文中的非形式证明，应由附录或补充材料中的形式证明补充。
%         \item 证明所依赖的定理和引理应被适当引用。
%     \end{itemize}

%     \item {\bf Experimental result reproducibility}
%     \item[] Question: 论文是否充分披露了重现主要实验结果所需的信息，且这些信息足以影响论文的主要论断和/或结论（无论是否提供代码和数据）？
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文不包含实验。
%         \item 若论文包含实验，回答 No 将被评审者负面看待：使论文可重现非常重要，无论是否提供代码与数据。
%         \item 若贡献是数据集和/或模型，作者应描述为使结果可重现或可验证所采取的步骤。
%         \item 根据贡献的类型，可通过不同方式实现可重现性。例如若贡献是新算法，论文应清楚说明如何重现该算法；若贡献是新模型架构，应完整描述架构；若贡献是具体模型及其经验评估，可能需要使他人能够用相同数据集复现模型或提供模型访问。一般而言，发布代码和数据通常是实现这一点的好方法，但也可以通过详尽的复现实验说明、托管模型的访问（例如大模型情形）、发布模型检查点或其他适当手段来实现可重现性。
%         \item 虽然 NeurIPS 并不强制要求公开代码，但会议要求所有投稿为可重现性提供合理途径，这取决于贡献的性质。例如
%         \begin{enumerate}
%             \item 若贡献主要是一种新算法，论文应说明如何重现该算法。
%             \item 若贡献主要是新模型架构，论文应清晰且完整地描述该架构。
%             \item 若贡献是一个新模型（例如大型语言模型），应提供可访问该模型以复现实验的途径，或提供重现该模型的方法（例如开放数据集或构建数据集的说明）。
%             \item 我们认识到在某些情况下可重现性可能具有挑战性，这时作者可以描述他们提供可重现性的具体方式。对于闭源模型，模型访问可能受到限制（例如仅对注册用户开放），但应确保其他研究者有某种途径来复现或验证结果。
%     \end{itemize}
\end{enumerate}
%     \end{itemize}


% \item {\bf Open access to data and code}
%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that paper does not include experiments requiring code.
%         \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
\end{itemize}


% \item {\bf Experimental setting/details}
%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文未包含实验。
%         \item 实验设置应在论文主体中以必要的细节程度呈现，以便读者理解并评估结果。
%         \item 全部细节可随代码提供，或列于附录，或作为补充材料。
\end{itemize}

% \item {\bf Experiment statistical significance}
%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文未包含实验。
%         \item 如果结果附带误差条、置信区间或统计显著性检验（至少针对支持论文主要结论的实验），作者应回答 "Yes"。
%         \item 应明确说明误差条所反映的变异因素（例如训练/测试划分、初始化、某些参数的随机抽样，或在给定实验条件下的整体运行）。
%         \item 应解释误差条的计算方法（解析公式、调用库函数、自助法（bootstrap）等）。
%         \item 应给出所作假设（例如误差服从正态分布）。
%         \item 应明确误差条表示的是标准差还是均值的标准误。
%         \item 报告 1-sigma 误差条是可以接受的，但应明确说明。如果误差正态性假设未经验证，作者应优先报告 2-sigma 误差条，而不是声称具有 96\% CI。
%         \item 对于非对称分布，作者应谨慎，避免在表格或图中展示对称的误差条从而导致超出取值范围的结果（例如出现负的错误率）。
%         \item 如果在表格或图中报告了误差条，作者应在正文中解释其计算方法，并在文中引用相应的图表。
\end{itemize}

% \item {\bf Experiments compute resources}
%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 回答 NA 表示论文不包含实验。
%         \item 论文应说明所使用计算节点的类型（CPU 或 GPU）、是内部集群还是云服务商，并给出相关的内存和存储信息。
%         \item 论文应提供每次独立实验运行所需的计算量，并估算总计算量。
%         \item 论文应披露整个研究项目是否比论文中报告的实验消耗更多计算资源（例如未包含在论文中的初步或失败实验）。
\end{itemize}
    
% \item {\bf Code of ethics}
%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示作者尚未审阅 NeurIPS 道德规范。
%         \item 如果作者回答 No，应解释要求偏离道德规范的特殊情况。
%         \item 作者应确保保持匿名（例如，如因其司法辖区的法律或法规存在特殊考虑）。
\end{itemize}


% \item {\bf Broader impacts}
%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
- 答案为 NA 表示所进行的工作没有社会影响。
- 如果作者回答 NA 或 No，应解释其工作为何没有社会影响，或为何论文未讨论社会影响。
- 负面社会影响的例子包括潜在的恶意或非预期用途（例如，散布虚假信息、生成虚假个人资料、监控）、公平性问题（例如，部署的技术可能会不公平地影响特定群体）、隐私问题和安全问题。
- 会议预计许多论文将属于基础研究，与特定应用乃至部署无关。然而，如果存在通往任何负面应用的直接途径，作者应指出。例如，指出生成模型质量的提升可能被用于生成用于散布虚假信息的深度伪造是合理的；另一方面，没有必要指出一个用于优化神经网络的通用算法可能使人们更快训练出用于生成深度伪造的模型。
- 作者应考虑技术在按预期使用且运行正常时可能带来的危害、在按预期使用但给出错误结果时可能带来的危害，以及（有意或无意）滥用该技术所导致的危害。
- 如果存在负面社会影响，作者还可以讨论可能的缓解策略（例如，受控发布模型、在提供攻击方法的同时提供防御措施、建立监测滥用的机制、监控系统随时间从反馈中学习的机制，以及提高机器学习的效率和可及性）。
\end{itemize}
    
% \item {\bf Safeguards}
%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 意味着论文不存在此类风险。
%         \item 对于具有高误用或双重用途风险的发布模型，应配备必要的安全防护以便受控使用，例如要求用户遵守使用指南或限制访问模型，或实施安全过滤器。
%         \item 从互联网上抓取的数据集可能带来安全风险。作者应说明如何避免发布不安全的图像。
%         \item 我们认识到提供有效的安全防护具有挑战性，且许多论文并不要求这样做，但我们鼓励作者将此纳入考虑并尽最大诚信努力。
\end{itemize}

% \item {\bf Licenses for existing assets}
%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文未使用现有资源。
%         \item 作者应当引用产生该代码包或数据集的原始论文。
%         \item 作者应说明所使用资源的版本，并在可能的情况下附上 URL。
%         \item 每项资源应包含其许可证名称（例如 CC-BY 4.0）。
%         \item 对于从特定来源（例如网站）抓取的数据，应提供该来源的版权信息和服务条款。
%         \item 如果发布了资源，应在包中提供许可证、版权信息和使用条款。对于流行数据集，\url{paperswithcode.com/datasets} 为一些数据集整理了许可信息。他们的许可指南可帮助确定数据集的许可证。
%         \item 对于被重新打包的已有数据集，应同时提供原始许可证以及派生资源的许可证（如果有更改）。
%         \item 如果这些信息在网上不可获得，鼓励作者联系资源的创建者。
\end{itemize}

% \item {\bf New assets}
%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 回答 NA 表示论文未发布新的资源。
%         \item 研究者应通过结构化模板在提交时提供数据集/代码/模型的详细信息，包括训练细节、许可、局限性等。
%         \item 论文应讨论是否以及如何从相关个人处获得了使用其资产的同意。
%         \item 在提交时，请记得对你的资源进行匿名化（如适用）。你可以创建匿名化的 URL，或附上匿名化的压缩文件。
\end{itemize}

% \item {\bf Crowdsourcing and research with human subjects}
%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文不涉及众包或与人类受试者相关的研究。
%         \item 将这些信息放在补充材料中是可以的，但如果论文的主要贡献涉及人类受试者，则应在正文中尽可能详尽地说明。
%         \item 根据 NeurIPS 伦理守则，参与数据收集、整理或其他劳动的工作人员应至少获得数据收集者所在国家的最低工资。
\end{itemize}

% \item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 答案 NA 表示论文既不涉及众包也不涉及人类受试者研究。
%         \item 根据研究进行的国家或地区，任何涉及人类受试者的研究可能需要 IRB 批准（或同等审批）。如果已取得 IRB 批准，应在论文中明确说明。
%         \item 我们认识到此类程序在不同机构和地区可能存在显著差异，期望作者遵守 NeurIPS 道德准则以及其所属机构的相关指南。
%         \item 在初次提交时，不要包含任何可能破坏匿名性的内容（如适用），例如负责审稿的机构名称。
\end{itemize}

% \item {\bf Declaration of LLM usage}
%     \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
%     %this research? 
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item 回答 NA 表示本研究的核心方法开发不涉及 LLM 作为任何重要、原创或非标准的组成部分。
%         \item 有关应当或不应当描述的内容，请参阅我们的 LLM 政策（\url{https://neurips.cc/Conferences/2025/LLM}）。
\end{itemize}

% \end{enumerate}


\end{document}