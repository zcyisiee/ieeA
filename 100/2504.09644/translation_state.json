{
  "completed": [
    "f00ba0f3-c406-4f73-94ee-2d44fe380acf",
    "40d8c52e-a3f3-4561-ab2c-3e13213f864e",
    "40ed0d44-9f82-4d3b-b6ef-c3bd69bb3f4c",
    "82c19b73-3744-4c4c-ba6d-d26b9495ecde",
    "4beb5871-efbe-414a-a768-6b7ee2ba6aca",
    "40a796f4-0f99-429c-ae99-23f5c8e53e4d",
    "0919dae2-29fe-4501-bbaa-16b745457295",
    "da2eabd2-115b-4116-a375-e13a52edf43a",
    "368db211-32d5-4631-875f-e051f61ba300",
    "d37bee76-3ba9-403f-ba0f-bcc2e34492f1",
    "31c37509-ddee-4a26-ae07-0037f090b6be",
    "d6aeba04-b803-43e8-a385-fe7bacab4461",
    "ac872593-a94e-4f06-b7a2-622882b98fb3",
    "7699e9f6-089c-42a1-96c9-60db11e02109",
    "a1ab49c4-8586-49ad-97d7-691d65e0e3f5",
    "ef3d654d-7852-4b8a-8baf-efc4895e4aed",
    "85d15fcb-6dcb-47a6-bacf-6b64c0ca2431",
    "cda7ee04-50cc-474a-87c0-98fe16853035",
    "b595e6dc-11c1-4044-a6d5-972acbd2f07b",
    "cdeaa510-7921-423f-8f18-0c18771193fa",
    "68ddb67c-ed24-444a-81d1-390d5897fd7c",
    "da4b0d7c-af97-43d0-9ff9-5a66d6238ed7",
    "67c21b73-c31e-48cd-b20c-4a0a68abb1ae",
    "b2185668-bf87-437d-8a20-8e6562acbf0e",
    "53e19068-22f1-4d8b-933a-f06d637ee781",
    "0ba9f6f8-8491-45f8-9524-dc01ee3f0328",
    "9f0fe516-9975-4f85-931c-c5b3d93187a5",
    "1befd8ae-3a2b-437f-a170-48d916987a1a",
    "ad8c87d3-2714-4010-9fb9-74869fca14d1",
    "0f5cc587-62bc-4f08-88d2-af4378394774",
    "c5d503d6-6329-431e-bd16-2721fd5578a3",
    "e3b89b97-30eb-4798-98ca-ec73803ec28f",
    "f34904c2-4087-46f0-92ff-7bf38838fc60",
    "7f6285d5-d4f7-4948-ae20-a845af96c8e0",
    "92609008-0b7c-4af1-ae92-f3ef80807b9d",
    "439a4785-1844-4da4-a025-b0c29fb4be5b",
    "e1fef000-53f3-4fb8-ad2d-9af96212d2e6",
    "b8ac025a-8131-40d0-b24a-c5f39200503f",
    "444a2fdd-ddd6-4ebd-980a-50dde9a058e6",
    "22b281de-6877-430b-b90c-28f19015e4da",
    "ca8a3e58-70a7-4329-8e28-a3ea292e2a2b",
    "e5f80a2a-67b1-497e-a951-8ba870b75b02",
    "fd6bb79b-d46a-4180-b611-e5f53ab074cd",
    "b6f3a347-f455-4168-939d-a608a0968bfd",
    "dcd0da8f-6c47-4370-889a-bf3cb2965931",
    "07a11818-06e0-4a9c-9092-d2d094b6ff1b",
    "95e3292e-9bd7-4ac2-9215-ede1059019fa",
    "f0936962-e16a-496b-80ec-a93c4ec410d5",
    "6bf8c035-bf89-4445-87c6-630c2151d985",
    "16d88966-5da0-4e0a-8e9a-5653311be1ac",
    "2c753d74-0332-444e-a358-63a61a1b47ba",
    "aded1100-48e5-4271-88c5-e0c72a078141",
    "b53ac536-721c-49cb-9369-47d84556bc40",
    "283a57f5-7209-4a3d-a788-d1fb55466027",
    "53bbada7-8bb3-4d08-8466-396347849a4b",
    "dc79b7e8-2579-451a-94ed-dc83396fa1b3",
    "d16f8f0f-26ee-432b-b84a-ea0016f7f129",
    "b5fe093c-2714-43f0-a198-7ebe6e195220",
    "53830f8a-ab02-4acf-bdce-4236b46a8d7d",
    "5fdd6ae7-7bcc-43cd-bb5e-82ff3492b5af",
    "a3c43b15-b6cb-4601-ba9f-51e7aa7eac15",
    "ac50ed18-40e9-4ce6-9759-8e0c5456c3c0",
    "9c3b1896-1cae-4d32-a079-937fffe97018",
    "538cf480-c07f-4cec-b3a1-fdb240c57be9",
    "b384c9e3-2eb8-4c99-9ab2-b9f9f709eaf4",
    "eb1e3b95-e15e-4872-b4f5-0f1780591b03",
    "8d57269d-d2e4-4954-98a4-f97bb857173a",
    "befd9910-62ee-4523-a098-27140aa5a3e6",
    "369d8762-f16f-4d68-8124-8abefbfba254",
    "6dfb3293-122e-4531-8a92-cc4dea3f0c8b",
    "87f96f37-86ac-41c8-9adf-3fc60aa64436",
    "1104868c-20bf-4079-be66-1526be766bd5",
    "8d4e99a4-00b3-4dba-9789-9acfdb61cc44",
    "ea563b09-1680-4c8e-82f4-9332dc34e24b",
    "e932073a-aebc-4499-9152-1c97eec04107",
    "55def7b8-b011-4f4d-8288-65f807eda501",
    "0806dc39-ff54-4613-b719-b6dc87d2c73c",
    "85fda28b-26df-4f88-ae81-51a7c88b18b0",
    "b52d60c9-35ac-442f-9bc5-2fb0f2fc3de7",
    "bdf4c825-0d0f-4225-ac96-0178045e5bb5",
    "4c1a3573-5c76-4140-921e-edd4f3f34dd6",
    "8d871354-e79e-49fc-8d15-0c188129ffdc",
    "61240f2e-dcb4-4e98-8165-ba8230187a6d",
    "1d336074-3d1e-436c-b835-486ded7d2d2d",
    "c0533510-5759-4560-b7cc-f8f5afb00a34",
    "8116a005-12f6-4ae0-8296-7c69b3a28baf",
    "96f0325f-2dd0-4fcf-aa52-84a17d803fe1",
    "65fd43e2-af3a-42dc-83d8-1e818105eb17",
    "8b0a2b0c-bc48-4231-ab97-dd30780e53ce",
    "405123e1-a529-4342-aa22-5a9d33dffdbc",
    "a459f311-a48d-4347-beff-0daf57e2be1d",
    "69f054ab-a24f-4a2a-9999-d4565fd222df",
    "4a4e9d3d-ce3d-49c7-84a5-5009956e0c06",
    "47783356-d5ff-438d-9055-99414d379630",
    "ca966102-c9b0-49b4-87d3-43fb45753232",
    "0f9f3a3f-0b6e-41e7-ae67-56e1c2036723",
    "f4f6e595-e347-401d-b472-82b647e9c507",
    "be62cd57-9a92-4b2c-8e06-87002c37f210",
    "ac708269-7765-4244-951c-0b5fca964ea4",
    "e040c617-78df-4877-8048-4c952c2ea762",
    "346eeb63-b154-4b64-8ffd-50d066a72e21",
    "3bcdd5d2-ab57-441a-8ee5-fd85396d209e",
    "5b3c9b47-3622-4fb2-ab01-d0f2064488eb",
    "370501c2-c938-4111-9ae7-a8bed0c21ae5",
    "fa0e9d8f-3b19-437b-8f7b-f1f1adcc76d9",
    "36e58d85-1d83-4ad6-91fa-6cde316f85ee",
    "e134c14a-e327-44e5-8b32-4ff0d783dd57",
    "2b0b3a93-5032-4ee6-bad6-97e256300204",
    "bd4ee9cf-60ce-4d77-86f4-130b05fea4da",
    "28fb537e-7f82-468d-ab13-0ebe2b89840f",
    "9ecbcf26-e848-4435-91d5-750f759c3493",
    "2702fc42-8da1-46dd-aa32-b1bf00eca26d",
    "5360f715-b47b-4eca-a5ea-a78b0190cada",
    "184912ee-6482-4feb-b573-713eeadb0d0d",
    "6dbf13ca-1178-4e20-a658-71b7ecc06a27",
    "f1ed3423-c391-4268-8b39-24837218994a",
    "a5b2831a-5e91-46ce-9d9c-8c8abc576b6f",
    "baedc3a8-5a75-4d16-a8e0-9bdeea292daf",
    "76b26573-cd68-40c4-8bfe-6f852cffaaa2",
    "b403029b-e79b-4a55-8d8f-7c0ef5b88827",
    "88f3f47d-c959-425b-9ae6-0d70b4011646",
    "95762f28-08aa-48d5-b69e-3de2173d657c",
    "e0554426-d6f9-48f4-826b-491a87040be8",
    "a551820d-248f-4306-8c46-456321b0e24a",
    "83785636-b362-4dfe-8aa5-b8257be10214",
    "457fd68e-2fc1-4ee5-a307-9f6d710cabcf"
  ],
  "results": [
    {
      "source": "SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model",
      "translation": "SegEarth-R1：基于大型语言模型的地理空间像素推理",
      "chunk_id": "f00ba0f3-c406-4f73-94ee-2d44fe380acf",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small xxx.\n%",
      "translation": "\\small xxx.",
      "chunk_id": "40d8c52e-a3f3-4561-ab2c-3e13213f864e",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small $D$-Projector.",
      "translation": "\\small $D$-投影器.",
      "chunk_id": "40ed0d44-9f82-4d3b-b6ef-c3bd69bb3f4c",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small Geospatial pixel reasoning results among SegEarth-R1 (ours) and previous related works.",
      "translation": "\\small SegEarth-R1（我们的方法）与先前相关工作的地理空间像素推理结果比较。",
      "chunk_id": "82c19b73-3744-4c4c-ba6d-d26b9495ecde",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small Ablation of LLM type on RRSIS-D.",
      "translation": "\\small 在 RRSIS-D 上针对 LLM 类型的消融实验。",
      "chunk_id": "4beb5871-efbe-414a-a768-6b7ee2ba6aca",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1,
        "batch_translated": true
      }
    },
    {
      "source": "\\small Ablation of $d$ on EarthReason Val set.",
      "translation": "\\small 在 EarthReason 验证集上对 $d$ 的消融研究。",
      "chunk_id": "40a796f4-0f99-429c-ae99-23f5c8e53e4d",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small The illustration of the prompt construction process for expand question-answer pairs.",
      "translation": "\\small 用于扩展问答对的提示构建过程示意图。",
      "chunk_id": "0919dae2-29fe-4501-bbaa-16b745457295",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small The category distribution of EarthReason.",
      "translation": "\\small EarthReason 的类别分布。",
      "chunk_id": "da2eabd2-115b-4116-a375-e13a52edf43a",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "The hyper-parameters for model training.",
      "translation": "模型训练的超参数。",
      "chunk_id": "368db211-32d5-4631-875f-e051f61ba300",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small Comparison with other models on EarthReason.",
      "translation": "\\small 在 EarthReason 上与其它模型的比较。",
      "chunk_id": "d37bee76-3ba9-403f-ba0f-bcc2e34492f1",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small Comparison with PSALM on RRSIS-D.",
      "translation": "\\small 与 PSALM 在 RRSIS-D 上的比较",
      "chunk_id": "31c37509-ddee-4a26-ae07-0037f090b6be",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Referring Segmentation",
      "translation": "指称分割",
      "chunk_id": "d6aeba04-b803-43e8-a385-fe7bacab4461",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Reasoning Segmentation",
      "translation": "推理分割",
      "chunk_id": "ac872593-a94e-4f06-b7a2-622882b98fb3",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "LLM-based Segmentation",
      "translation": "基于 LLM 的分割",
      "chunk_id": "7699e9f6-089c-42a1-96c9-60db11e02109",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1,
        "batch_translated": true
      }
    },
    {
      "source": "Comparison with Related Dataset",
      "translation": "与相关数据集的比较",
      "chunk_id": "a1ab49c4-8586-49ad-97d7-691d65e0e3f5",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Dataset Generation Pipeline",
      "translation": "数据集生成流程",
      "chunk_id": "ef3d654d-7852-4b8a-8baf-efc4895e4aed",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Dataset Statistics",
      "translation": "数据集统计",
      "chunk_id": "85d15fcb-6dcb-47a6-bacf-6b64c0ca2431",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Hierarchical Visual Encoder",
      "translation": "分层视觉编码器",
      "chunk_id": "cda7ee04-50cc-474a-87c0-98fe16853035",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Large Language Model and Input Schema",
      "translation": "大型语言模型与输入模式",
      "chunk_id": "b595e6dc-11c1-4044-a6d5-972acbd2f07b",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Mask Generation with Spatial Correlation",
      "translation": "考虑空间相关性的掩膜生成",
      "chunk_id": "cdeaa510-7921-423f-8f18-0c18771193fa",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Settings",
      "translation": "实验设置",
      "chunk_id": "68ddb67c-ed24-444a-81d1-390d5897fd7c",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Geospatial Pixel Reasoning Results",
      "translation": "地理空间像素推理结果",
      "chunk_id": "da4b0d7c-af97-43d0-9ff9-5a66d6238ed7",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Referring Segmentation Results",
      "translation": "指称分割结果",
      "chunk_id": "67c21b73-c31e-48cd-b20c-4a0a68abb1ae",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Ablation Study",
      "translation": "消融研究",
      "chunk_id": "b2185668-bf87-437d-8a20-8e6562acbf0e",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Annotation of EarthReason",
      "translation": "EarthReason 的标注",
      "chunk_id": "53e19068-22f1-4d8b-933a-f06d637ee781",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "EarthReason Statistics",
      "translation": "EarthReason 统计信息",
      "chunk_id": "0ba9f6f8-8491-45f8-9524-dc01ee3f0328",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Details of Training Hyper-parameters",
      "translation": "训练超参数详细信息",
      "chunk_id": "9f0fe516-9975-4f85-931c-c5b3d93187a5",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "More Qualitative Results on EarthReason",
      "translation": "在 EarthReason 上的更多定性结果",
      "chunk_id": "1befd8ae-3a2b-437f-a170-48d916987a1a",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "More Qualitative Results on RRSIS-D",
      "translation": "在 RRSIS-D 上的更多定性结果",
      "chunk_id": "ad8c87d3-2714-4010-9fb9-74869fca14d1",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Introduction",
      "translation": "引言",
      "chunk_id": "0f5cc587-62bc-4f08-88d2-af4378394774",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Related Work",
      "translation": "相关工作",
      "chunk_id": "c5d503d6-6329-431e-bd16-2721fd5578a3",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Benchmark Geospatial Pixel Reasoning Dataset---EarthReason",
      "translation": "基准地理空间像素推理数据集——EarthReason",
      "chunk_id": "e3b89b97-30eb-4798-98ca-ec73803ec28f",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Baseline Geospatial Pixel Reasoning Method---SegEarth-R1",
      "translation": "基线地理空间像素推理方法——SegEarth-R1",
      "chunk_id": "f34904c2-4087-46f0-92ff-7bf38838fc60",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Experiments",
      "translation": "实验",
      "chunk_id": "7f6285d5-d4f7-4948-ae20-a845af96c8e0",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Conclusion",
      "translation": "结论",
      "chunk_id": "92609008-0b7c-4af1-ae92-f3ef80807b9d",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Data",
      "translation": "数据",
      "chunk_id": "439a4785-1844-4da4-a025-b0c29fb4be5b",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Additional Implementation Details",
      "translation": "附加实现细节",
      "chunk_id": "e1fef000-53f3-4fb8-ad2d-9af96212d2e6",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Examples",
      "translation": "示例",
      "chunk_id": "b8ac025a-8131-40d0-b24a-c5f39200503f",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "NeurIPS Paper Checklist",
      "translation": "NeurIPS 论文检查表",
      "chunk_id": "444a2fdd-ddd6-4ebd-980a-50dde9a058e6",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Visual Token",
      "translation": "视觉标记",
      "chunk_id": "22b281de-6877-430b-b90c-28f19015e4da",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "Text Instruction",
      "translation": "文本指令",
      "chunk_id": "ca8a3e58-70a7-4329-8e28-a3ea292e2a2b",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "In summary, our contributions are as follows:",
      "translation": "综上所述，我们的贡献如下：",
      "chunk_id": "e5f80a2a-67b1-497e-a951-8ba870b75b02",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0,
        "batch_translated": true
      }
    },
    {
      "source": "\\small Comparison of semantic segmentation, referring segmentation and geospatial pixel inference. (left) Samples from the LoveDA~\\cite{wang2021loveda} and RRSIS-D~\\cite{liu2024rotated} datasets. (right) Samples from the EarthReason dataset. Previous tasks are limited by fixed taxonomies and explicit instructions, while geospatial pixel reasoning supports complex implicit instructions and requires the reasoning capability of the model.",
      "translation": "\\small 语义分割、指称分割与地理空间像素推理的比较。（左）来自 LoveDA~\\cite{wang2021loveda} 和 RRSIS-D~\\cite{liu2024rotated} 数据集的样例。（右）来自 EarthReason 数据集的样例。以往任务受限于固定的分类体系和显式指令，而地理空间像素推理则支持复杂的隐式指令，并要求模型具备更强的推理能力。",
      "chunk_id": "fd6bb79b-d46a-4180-b611-e5f53ab074cd",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Comparison between EarthReason and other related datasets. The {\\color{white!50!black}gray} rendering denotes the natural image dataset. ``Seg'', ``Det'', ``VG'', ``Cls'' denote segmentation, detection, visual grounding and classification datasets, respectively.",
      "translation": "EarthReason 与其他相关数据集的比较。  \n其中 {\\color{white!50!black}gray} 渲染表示自然图像数据集。  \n``Seg''、``Det''、``VG''、``Cls'' 分别表示分割、检测、视觉定位和分类数据集。",
      "chunk_id": "b6f3a347-f455-4168-939d-a608a0968bfd",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Comparison of referring segmentation and geospatial pixel inference. (left) Samples from the RRSIS-D~\\cite{liu2024rotated} and RefSegRS~\\cite{yuan2024rrsis} datasets. (right) Samples from the EarthReason dataset.",
      "translation": "\\small 指称分割与地理空间像素推理的比较。（左）来自 RRSIS-D~\\cite{liu2024rotated} 和 RefSegRS~\\cite{yuan2024rrsis} 数据集的样本。（右）来自 EarthReason 数据集的样本。",
      "chunk_id": "dcd0da8f-6c47-4370-889a-bf3cb2965931",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Overview of the proposed SegEarth-R1 architecture. Given an image $X_v$ and a text description $X_q$, a hierarchical visual encoder and a proposed connector are used to extract and compress visual tokens. Then, the visual tokens \\includegraphics[scale=0.004,valign=c]{figures/visual_token.png} and description embeddings \\includegraphics[scale=0.004,valign=c]{figures/text_token.png} are fed into an LLM for instruction interpretation and semantic correlation. Finally, description embeddings are directly mapped to the query vector and used for spatial correlation and segmentation mask generation.",
      "translation": "\\small 所提出的 SegEarth-R1 架构概览。对于输入图像 $X_v$ 和文本描述 $X_q$，采用分层视觉编码器和所提出的连接器来提取并压缩视觉 tokens。随后，将视觉 tokens \\includegraphics[scale=0.004,valign=c]{figures/visual_token.png} 和描述嵌入 \\includegraphics[scale=0.004,valign=c]{figures/text_token.png} 输入到 LLM 中，用于指令解析和语义关联。最后，将描述嵌入直接映射为查询向量，用于空间相关性计算及分割掩码生成。",
      "chunk_id": "07a11818-06e0-4a9c-9092-d2d094b6ff1b",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "\\small Redundancy analysis of remote sensing datasets and natural images, and the former exhibits higher redundancy.",
      "translation": "\\small 对遥感数据集与自然图像的冗余性分析表明，前者具有更高的冗余性。",
      "chunk_id": "95e3292e-9bd7-4ac2-9215-ede1059019fa",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Referring segmentation results among SegEarth-R1 and previous related works on RRSIS-D dataset.",
      "translation": "\\small 在 RRSIS-D 数据集上，SegEarth-R1 与以往相关工作在指代分割任务上的结果比较。",
      "chunk_id": "f0936962-e16a-496b-80ec-a93c4ec410d5",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Referring segmentation results among SegEarth-R1 and previous related works on RefSegRS dataset.",
      "translation": "\\small SegEarth-R1 与先前相关工作在 RefSegRS 数据集上的指称分割结果。",
      "chunk_id": "6bf8c035-bf89-4445-87c6-630c2151d985",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Qualitative Results of SegEarth-R1 on EarthReason. More results can be found in  Appendix~\\ref{sec:appendix_examples}.",
      "translation": "\\small SegEarth-R1 在 EarthReason 上的定性结果。更多结果可见附录~\\ref{sec:appendix_examples}。",
      "chunk_id": "16d88966-5da0-4e0a-8e9a-5653311be1ac",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Ablation of SegEarth-R1 components on EarthReason: query description embedding (Query D.E.), description projector ($D$-Projector), token compression connector (T.C. Connector).",
      "translation": "\\small 在 EarthReason 上对 SegEarth-R1 组件的消融：查询描述嵌入（Query D.E.）、描述投影器（$D$-Projector）、标记压缩连接器（T.C. Connector）。",
      "chunk_id": "2c753d74-0332-444e-a358-63a61a1b47ba",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small The illustration of the prompt construction process for generating question-answer pairs for geospatial pixel reasoning.",
      "translation": "\\small 用于生成地理像素推理问答对的提示构建流程示意图。",
      "chunk_id": "aded1100-48e5-4271-88c5-e0c72a078141",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\small Comparison of annotation quality. (a), (b) and (c) are from RRSIS-D dataset, (d), (e) and (f) are from our EarthReason dataset.",
      "translation": "\\small 注释质量比较。(a)、(b) 和 (c) 来自 RRSIS-D 数据集，(d)、(e) 和 (f) 来自我们的 EarthReason 数据集。",
      "chunk_id": "b53ac536-721c-49cb-9369-47d84556bc40",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over spatial context, domain knowledge, and implicit user intent. Motivated by this, we introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To advance this task, we construct and release the first large-scale benchmark dataset called EarthReason, which comprises 5,434 manually annotated image masks with over 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a simple yet effective language-guided segmentation baseline that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator for spatial correlation. The design of SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution remote sensing images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional and LLM-based segmentation methods. Our data and code will be released at [[URL_229]].\n\n\n% Recent advancements in Multimodal Large Language Models (MLLMs) have shown promise in natural image domains, but their direct application to remote sensing is challenging due to the unique characteristics of overhead imagery, such as extreme scale variation, oblique object orientations, and densely packed small-scale features. To address these challenges, we introduce SegEarth-R1, a novel framework for geospatial pixel reasoning that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator designed for spatial correlation. SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. \n\n% We also present EarthReason, the first large-scale benchmark for geospatial pixel reasoning, comprising 5,434 manually annotated remote sensing images with over 30,000 implicit question-answer pairs. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional segmentation methods and off-the-shelf MLLMs. Our contributions include the introduction of the remote sensing reasoning segmentation task, the release of the EarthReason dataset, and the design of SegEarth-R1, which advances the capabilities of LLM-based segmentation in remote sensing.",
      "translation": "遥感在理解环境动态、城市规划与灾害管理等方面已变得至关重要。然而，传统的遥感工作流通常依赖显式的分割或检测方法，这些方法在应对需要基于空间语境、领域知识与隐含用户意图进行推理的复杂隐式查询时往往力不从心。基于此动机，我们提出了一个新任务，即 \\ie geospatial pixel reasoning，该任务支持隐式查询与推理，并输出目标区域的掩码。为推动该任务的发展，我们构建并发布了首个大规模基准数据集 EarthReason，包含 5,434 幅人工标注的图像掩码以及超过 30,000 对隐式问答对。 \n\n此外，我们提出了 SegEarth-R1，一种简单却高效的语言引导分割基线，集成了分层视觉编码器、用于指令解析的大型语言模型 (LLM) 以及为空间关联定制的掩码生成器。SegEarth-R1 的设计包含若干领域专用的改进，包括为处理超高分辨率遥感影像而采用的激进视觉令牌压缩、用于融合语言与多尺度特征的描述投影模块，以及可直接查询描述嵌入的精简掩码预测流程。大量实验表明，SegEarth-R1 在推理与指称分割任务上均取得了最先进的性能，显著优于传统和基于 LLM 的分割方法。我们的数据与代码将发布于 [[URL_229]]。\n\n% 最近多模态大型语言模型 (MLLM) 在自然图像领域展现出潜力，但由于航拍影像具有极端尺度变化、倾斜的目标朝向以及密集的小尺度特征等独特特性，将其直接应用于遥感存在挑战。为应对这些挑战，我们引入了 SegEarth-R1——一个用于 geospatial pixel reasoning 的新框架，集成了分层视觉编码器、用于指令解析的大型语言模型 (LLM) 以及为空间相关性设计的定制掩码生成器。SegEarth-R1 融入了领域特化的改进，包括为处理超高分辨率图像的激进视觉令牌压缩、用于融合语言与多尺度特征的描述投影模块，以及直接查询描述嵌入的简化掩码预测流水线。\n\n% 我们还提出了 EarthReason，这是首个面向 geospatial pixel reasoning 的大规模基准，包含 5,434 幅人工标注的遥感影像和超过 30,000 个隐式问答对。大量实验表明，SegEarth-R1 在推理与指称分割任务上取得了最先进的表现，显著优于传统分割方法和现成的 MLLM。我们的贡献包括提出遥感推理分割任务、发布 EarthReason 数据集，以及设计 SegEarth-R1，从而推进了基于 LLM 的分割在遥感领域的能力。",
      "chunk_id": "283a57f5-7209-4a3d-a788-d1fb55466027",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 6
      }
    },
    {
      "source": "% \\item {\\bf Claims}\n%     \\item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item The answer NA means that the abstract and introduction do not include the claims made in the paper.\n%         \\item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \n%         \\item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \n%         \\item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. \n%     \\end{itemize}\n\n% \\item {\\bf Limitations}\n%     \\item[] Question: Does the paper discuss the limitations of the work performed by the authors?\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \n%         \\item The authors are encouraged to create a separate \"Limitations\" section in their paper.\n%         \\item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n%         \\item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n%         \\item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n%         \\item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n%         \\item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n%         \\item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n%     \\end{itemize}\n\n% \\item {\\bf Theory assumptions and proofs}\n%     \\item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item The answer NA means that the paper does not include theoretical results. \n%         \\item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n%         \\item All assumptions should be clearly stated or referenced in the statement of any theorems.\n%         \\item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \n%         \\item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n%         \\item Theorems and Lemmas that the proof relies upon should be properly referenced. \n%     \\end{itemize}\n\n%     \\item {\\bf Experimental result reproducibility}\n%     \\item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item The answer NA means that the paper does not include experiments.\n%         \\item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n%         \\item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \n%         \\item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n%         \\item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n%         \\begin{enumerate}\n%             \\item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n%             \\item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n%             \\item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n%             \\item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n%",
      "translation": "% \\item {\\bf Claims}\n%     \\item[] Question: 摘要和引言中提出的主要论断是否准确反映了论文的贡献和范围？\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item 答案 NA 表示摘要和引言未包含论文中提出的论断。\n%         \\item 摘要和/或引言应清楚陈述所做的论断，包括论文的具体贡献及重要的假设与局限。若对此问题的回答为 No 或 NA，评审者通常会持负面看法。\n%         \\item 所提出的论断应与理论和实验结果相符，并反映这些结果在其他情形下的可推广程度。\n%         \\item 将远景性目标作为动机是可以接受的，但需明确这些目标并非由论文实现。\n%     \\end{itemize}\n\n% \\item {\\bf Limitations}\n%     \\item[] Question: 论文是否讨论了作者所做工作的局限性？\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item 答案 NA 表示论文没有局限性；答案 No 表示论文存在局限性但未在文中讨论。\n%         \\item 鼓励作者在论文中单独设立“Limitations”小节。\n%         \\item 论文应指出任何强假设并讨论结果对这些假设违背时的稳健性（例如独立性假设、无噪声设定、模型完全指定、渐近近似仅在局部成立等）。作者应反思这些假设在实际中可能如何被违反及其后果。\n%         \\item 作者应反思所提出论断的适用范围，例如方法是否仅在少数数据集或若干次实验中测试。通常实验结果依赖隐含假设，这些应当被阐明。\n%         \\item 作者应讨论影响方法性能的因素。例如，面部识别算法在图像分辨率低或光照不足时可能表现差；语音转文字系统在处理专业术语时可能无法可靠用于在线讲座的自动字幕。\n%         \\item 作者应讨论所提算法的计算效率及其随数据规模的扩展性。\n%         \\item 如适用，作者应讨论其方法在隐私与公平性方面的潜在局限。\n%         \\item 虽然作者可能担心对局限性的完全坦诚会被评审用作否决理由，但更糟的情况是评审发现未被承认的局限。作者应以最佳判断行事，认识到透明性对维护学术共同体诚信规范的重要性。评审者将被特别指示不要因对局限性诚实陈述而惩罚作者。\n%     \\end{itemize}\n\n% \\item {\\bf Theory assumptions and proofs}\n%     \\item[] Question: 对于每个理论结果，论文是否给出了完整的假设集合以及一个完整（且正确）的证明？\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item 答案 NA 表示论文不包含理论结果。\n%         \\item 论文中所有的定理、公式和证明应有编号并做好交叉引用。\n%         \\item 所有假设应在任何定理的陈述中被明确说明或引用。\n%         \\item 证明可以出现在正文章节或补充材料中，但若置于补充材料，建议作者提供简短的证明思路以便提供直观理解。\n%         \\item 相应地，任何出现在正文中的非形式证明，应由附录或补充材料中的形式证明补充。\n%         \\item 证明所依赖的定理和引理应被适当引用。\n%     \\end{itemize}\n\n%     \\item {\\bf Experimental result reproducibility}\n%     \\item[] Question: 论文是否充分披露了重现主要实验结果所需的信息，且这些信息足以影响论文的主要论断和/或结论（无论是否提供代码和数据）？\n%     \\item[] Answer: \\answerTODO{} % Replace by \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item[] Justification: \\justificationTODO{}\n%     \\item[] Guidelines:\n%     \\begin{itemize}\n%         \\item 答案 NA 表示论文不包含实验。\n%         \\item 若论文包含实验，回答 No 将被评审者负面看待：使论文可重现非常重要，无论是否提供代码与数据。\n%         \\item 若贡献是数据集和/或模型，作者应描述为使结果可重现或可验证所采取的步骤。\n%         \\item 根据贡献的类型，可通过不同方式实现可重现性。例如若贡献是新算法，论文应清楚说明如何重现该算法；若贡献是新模型架构，应完整描述架构；若贡献是具体模型及其经验评估，可能需要使他人能够用相同数据集复现模型或提供模型访问。一般而言，发布代码和数据通常是实现这一点的好方法，但也可以通过详尽的复现实验说明、托管模型的访问（例如大模型情形）、发布模型检查点或其他适当手段来实现可重现性。\n%         \\item 虽然 NeurIPS 并不强制要求公开代码，但会议要求所有投稿为可重现性提供合理途径，这取决于贡献的性质。例如\n%         \\begin{enumerate}\n%             \\item 若贡献主要是一种新算法，论文应说明如何重现该算法。\n%             \\item 若贡献主要是新模型架构，论文应清晰且完整地描述该架构。\n%             \\item 若贡献是一个新模型（例如大型语言模型），应提供可访问该模型以复现实验的途径，或提供重现该模型的方法（例如开放数据集或构建数据集的说明）。\n%             \\item 我们认识到在某些情况下可重现性可能具有挑战性，这时作者可以描述他们提供可重现性的具体方式。对于闭源模型，模型访问可能受到限制（例如仅对注册用户开放），但应确保其他研究者有某种途径来复现或验证结果。\n%     \\end{itemize}",
      "chunk_id": "53bbada7-8bb3-4d08-8466-396347849a4b",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "[leftmargin=8pt]\n\\item We introduce the geospatial pixel reasoning task, which requires models to infer segmentation masks from implicit natural language queries by reasoning over spatial context and domain knowledge.\n\\item We build and release the first large-scale benchmark with 5,434 image-mask pairs, 28 categories, and over 30,000 implicit question-answer pairs, fostering research in geospatial pixel reasoning.\n\\item We propose an LLM-based segmentation model, SegEarth-R1, which incorporates new segmentation capabilities in remote sensing, containing several domain-specific designs.\n\\item Extensive experiments show that SegEarth-R1 achieves state-of-the-art performance on reasoning and referring segmentation tasks, compared to traditional methods and other LLM-based methods.",
      "translation": "[leftmargin=8pt]\n\\item 我们提出了地理空间像素推理任务，该任务要求模型通过对空间上下文和领域知识的推理，从隐含的自然语言查询中推断出分割掩码。\n\\item 我们构建并发布了首个大规模基准数据集，包含 5,434 对图像-掩码、28 个类别，以及超过 30,000 个隐式问答对，以促进地理空间像素推理的研究。\n\\item 我们提出了一种基于 LLM 的分割模型 SegEarth-R1，为遥感分割引入了新的能力，并包含若干领域特定的设计。\n\\item 大量实验表明，Compared to traditional methods and other LLM-based methods, SegEarth-R1 在推理与指称分割任务上达到了最先进的性能。",
      "chunk_id": "dc79b7e8-2579-451a-94ed-dc83396fa1b3",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "[leftmargin=8pt]\n\\item According to information theory~[[CITE_117]], entropy measures the average uncertainty or information content of an image, while the maximum entropy corresponds to the idealized scenario where pixel values are uniformly distributed (\\ie, no redundancy). Thus, from the entropy perspective, the image redundancy can be defined as~[[CITE_118]]:\n[[MATHENV_1]]\nwhere [[MATH_13]] denotes the number of distinct intensity levels (\\eg, [[MATH_14]] for an 8-bit grayscale image), and [[MATH_15]] denotes the probability mass function of the pixel intensity value [[MATH_16]].",
      "translation": "[leftmargin=8pt]\\item 根据信息论~[[CITE_117]]，熵衡量图像的平均不确定性或信息量，而最大熵对应像素值均匀分布的理想情形（\\ie，无冗余）。因此，从熵的角度，图像的冗余可以定义为~[[CITE_118]]：\n[[MATHENV_1]]\n其中 [[MATH_13]] 表示不同强度级别的数量（\\eg，对于 8 位灰度图像为 [[MATH_14]]），而 [[MATH_15]] 表示像素强度值 [[MATH_16]] 的概率质量函数。",
      "chunk_id": "d16f8f0f-26ee-432b-b84a-ea0016f7f129",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "[leftmargin=8pt]\n\\item Beyond pixel-level statistical redundancy, structural self-similarity reflects spatial redundancy caused by repetitive patterns (\\eg, textures, geometric features). To quantify this, we leverage the Structural Similarity Index Matrix (SSIM)~[[CITE_119]] to measure inter-patch similarity. For an image partitioned into [[MATH_17]] patches, the SSIM matrix [[MATH_18]] is defined as:\n[[MATHENV_2]]\nwhere [[MATH_19]], [[MATH_20]] denote the mean and variance of the [[MATH_21]]-th patch, [[MATH_22]] is the covariance between patches [[MATH_23]] and [[MATH_24]], and [[MATH_25]], [[MATH_26]] are stability constants. Then, the structural self-similarity redundancy [[MATH_27]] is derived by averaging off-diagonal elements of [[MATH_28]]:\n[[MATHENV_3]]",
      "translation": "[leftmargin=8pt]\\item 除了像素级的统计冗余之外，结构自相似性反映了由重复模式（\\eg 纹理、几何特征）引起的空间冗余。为量化这一点，我们采用结构相似性指数矩阵（SSIM）~[[CITE_119]]来衡量补丁之间的相似性。对于被划分为 [[MATH_17]] 个补丁的图像，SSIM 矩阵 [[MATH_18]] 定义为：\n[[MATHENV_2]]\n其中 [[MATH_19]]、[[MATH_20]] 分别表示第 [[MATH_21]] 个补丁的均值和方差，[[MATH_22]] 表示第 [[MATH_23]] 与第 [[MATH_24]] 个补丁之间的协方差，[[MATH_25]]、[[MATH_26]] 为稳定性常数。然后，结构自相似冗余 [[MATH_27]] 通过对 [[MATH_28]] 的非对角元素求平均得到：\n[[MATHENV_3]]",
      "chunk_id": "b5fe093c-2714-43f0-a198-7ebe6e195220",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "[leftmargin=1.5em]\n    \\item Step-1: To fully leverage the powerful multimodal capabilities and extensive geographic knowledge of GPT-4o, we carefully design the prompt, which is then provided alongside images and their corresponding category labels to generate a reasoning question–answer pair. The prompt is illustrated in Figure~[[REF_192]]. \n    \\item Step-2: To avoid homogeneous question–answer formats under a single prompt, we further employ the textual capabilities of GPT-3.5 to expand each generated question into six variations and each answer into three alternatives. The prompt used for this expansion is shown in Figure~[[REF_193]].\n    \\item Step-3: Unlike previous methods that rely on semi-automatic mask annotation based on off-the-shelf bounding boxes or masks, we invite multiple remote sensing vision experts to perform accurate and efficient mask annotation guided by the generated questions. To further improve annotation efficiency, we incorporate SAM-H as an auxiliary tool for some simple targets. Subsequently, we perform cross-validation of the annotation results and re-annotate the samples that do not meet the quality standards. As shown in Figure~[[REF_194]], (a), (b), and (c), derived from the RRSIS-D dataset, illustrate the masks of semi-automatic annotation based on bounding boxes. (a) and (c) exhibit noticeable annotation errors, while in (b), the query does not align with the annotation. (d), (e), and (f) illustrate our high-quality manual annotations.",
      "translation": "[leftmargin=1.5em]\n    \\item 步骤1：为充分利用 GPT-4o 强大的多模态能力和丰富的地理知识，我们精心设计了提示语（prompt），并将其与图像及对应的类别标签一并提供，用于生成推理问答对。该提示语见 Figure~[[REF_192]]。 \n    \\item 步骤2：为避免在单一提示下产生同质化的问答格式，我们进一步利用 GPT-3.5 的文本能力，将每个生成的问题扩展为六种变体，每个答案扩展为三种备选。用于该扩展的提示语见 Figure~[[REF_193]]。\n    \\item 步骤3：不同于以往依赖现成边界框或掩模进行半自动掩模标注的方法，我们邀请多位遥感视觉专家在生成问题的指导下进行准确且高效的掩模标注。为进一步提高标注效率，对于部分简单目标我们引入 SAM-H 作为辅助工具。随后对标注结果进行交叉核验，并对不符合质量标准的样本重新标注。如 Figure~[[REF_194]] 所示，(a)、(b) 与 (c)（源自 RRSIS-D 数据集）展示了基于边界框的半自动标注掩模，其中 (a) 和 (c) 存在明显的标注错误，而 (b) 中的查询与标注不一致；(d)、(e) 与 (f) 展示了我们高质量的手工标注。",
      "chunk_id": "53830f8a-ab02-4acf-bdce-4236b46a8d7d",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "%     \\item You should answer \\answerYes{}, \\answerNo{}, or \\answerNA{}.\n%     \\item \\answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.\n%     \\item Please provide a short (1–2 sentence) justification right after your answer (even for NA). \n%    % \\item {\\bf The papers not including the checklist will be desk rejected.}\n%",
      "translation": "%     \\item 你应该回答 \\answerYes{}, \\answerNo{}, 或 \\answerNA{}。\n%     \\item \\answerNA{} 表示该问题对本论文不适用，或相关信息不可用。\n%     \\item 请在你的答案之后（即使为 NA）紧接着给出一段简短的说明（1–2 句）。 \n%    % \\item {\\bf The papers not including the checklist will be desk rejected.}",
      "chunk_id": "5fdd6ae7-7bcc-43cd-bb5e-82ff3492b5af",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%     \\item {\\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist\"},\n%     \\item  {\\bf Keep the checklist subsection headings, questions/answers and guidelines below.}\n%     \\item {\\bf Do not modify the questions and only use the provided macros for your answers}.\n%",
      "translation": "%     \\item {\\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist\"},\n%     \\item  {\\bf Keep the checklist subsection headings, questions/answers and guidelines below.}\n%     \\item {\\bf Do not modify the questions and only use the provided macros for your answers}.",
      "chunk_id": "a3c43b15-b6cb-4601-ba9f-51e7aa7eac15",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that paper does not include experiments requiring code.\n%         \\item Please see the NeurIPS code and data submission guidelines ([[URL_231]]) for more details.\n%         \\item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n%         \\item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([[URL_232]]) for more details.\n%         \\item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n%         \\item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n%         \\item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n%         \\item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n%",
      "translation": "%         \\item The answer NA means that paper does not include experiments requiring code.\n%         \\item Please see the NeurIPS code and data submission guidelines ([[URL_231]]) for more details.\n%         \\item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n%         \\item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([[URL_232]]) for more details.\n%         \\item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n%         \\item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n%         \\item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n%         \\item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.",
      "chunk_id": "ac50ed18-40e9-4ce6-9759-8e0c5456c3c0",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not include experiments.\n%         \\item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n%         \\item The full details can be provided either with the code, in appendix, or as supplemental material.\n%",
      "translation": "%         \\item 答案 NA 表示论文未包含实验。\n%         \\item 实验设置应在论文主体中以必要的细节程度呈现，以便读者理解并评估结果。\n%         \\item 全部细节可随代码提供，或列于附录，或作为补充材料。",
      "chunk_id": "9c3b1896-1cae-4d32-a079-937fffe97018",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not include experiments.\n%         \\item The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\n%         \\item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\n%         \\item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\n%         \\item The assumptions made should be given (e.g., Normally distributed errors).\n%         \\item It should be clear whether the error bar is the standard deviation or the standard error of the mean.\n%         \\item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\\% CI, if the hypothesis of Normality of errors is not verified.\n%         \\item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\n%         \\item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\n%",
      "translation": "%         \\item 答案 NA 表示论文未包含实验。\n%         \\item 如果结果附带误差条、置信区间或统计显著性检验（至少针对支持论文主要结论的实验），作者应回答 \"Yes\"。\n%         \\item 应明确说明误差条所反映的变异因素（例如训练/测试划分、初始化、某些参数的随机抽样，或在给定实验条件下的整体运行）。\n%         \\item 应解释误差条的计算方法（解析公式、调用库函数、自助法（bootstrap）等）。\n%         \\item 应给出所作假设（例如误差服从正态分布）。\n%         \\item 应明确误差条表示的是标准差还是均值的标准误。\n%         \\item 报告 1-sigma 误差条是可以接受的，但应明确说明。如果误差正态性假设未经验证，作者应优先报告 2-sigma 误差条，而不是声称具有 96\\% CI。\n%         \\item 对于非对称分布，作者应谨慎，避免在表格或图中展示对称的误差条从而导致超出取值范围的结果（例如出现负的错误率）。\n%         \\item 如果在表格或图中报告了误差条，作者应在正文中解释其计算方法，并在文中引用相应的图表。",
      "chunk_id": "538cf480-c07f-4cec-b3a1-fdb240c57be9",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not include experiments.\n%         \\item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n%         \\item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \n%         \\item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). \n%",
      "translation": "%         \\item 回答 NA 表示论文不包含实验。\n%         \\item 论文应说明所使用计算节点的类型（CPU 或 GPU）、是内部集群还是云服务商，并给出相关的内存和存储信息。\n%         \\item 论文应提供每次独立实验运行所需的计算量，并估算总计算量。\n%         \\item 论文应披露整个研究项目是否比论文中报告的实验消耗更多计算资源（例如未包含在论文中的初步或失败实验）。",
      "chunk_id": "b384c9e3-2eb8-4c99-9ab2-b9f9f709eaf4",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n%         \\item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n%         \\item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n%",
      "translation": "%         \\item 答案 NA 表示作者尚未审阅 NeurIPS 道德规范。\n%         \\item 如果作者回答 No，应解释要求偏离道德规范的特殊情况。\n%         \\item 作者应确保保持匿名（例如，如因其司法辖区的法律或法规存在特殊考虑）。",
      "chunk_id": "eb1e3b95-e15e-4872-b4f5-0f1780591b03",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that there is no societal impact of the work performed.\n%         \\item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\n%         \\item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\n%         \\item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\n%         \\item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\n%         \\item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\n%",
      "translation": "- 答案为 NA 表示所进行的工作没有社会影响。\n- 如果作者回答 NA 或 No，应解释其工作为何没有社会影响，或为何论文未讨论社会影响。\n- 负面社会影响的例子包括潜在的恶意或非预期用途（例如，散布虚假信息、生成虚假个人资料、监控）、公平性问题（例如，部署的技术可能会不公平地影响特定群体）、隐私问题和安全问题。\n- 会议预计许多论文将属于基础研究，与特定应用乃至部署无关。然而，如果存在通往任何负面应用的直接途径，作者应指出。例如，指出生成模型质量的提升可能被用于生成用于散布虚假信息的深度伪造是合理的；另一方面，没有必要指出一个用于优化神经网络的通用算法可能使人们更快训练出用于生成深度伪造的模型。\n- 作者应考虑技术在按预期使用且运行正常时可能带来的危害、在按预期使用但给出错误结果时可能带来的危害，以及（有意或无意）滥用该技术所导致的危害。\n- 如果存在负面社会影响，作者还可以讨论可能的缓解策略（例如，受控发布模型、在提供攻击方法的同时提供防御措施、建立监测滥用的机制、监控系统随时间从反馈中学习的机制，以及提高机器学习的效率和可及性）。",
      "chunk_id": "8d57269d-d2e4-4954-98a4-f97bb857173a",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper poses no such risks.\n%         \\item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \n%         \\item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n%         \\item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n%",
      "translation": "%         \\item 答案 NA 意味着论文不存在此类风险。\n%         \\item 对于具有高误用或双重用途风险的发布模型，应配备必要的安全防护以便受控使用，例如要求用户遵守使用指南或限制访问模型，或实施安全过滤器。\n%         \\item 从互联网上抓取的数据集可能带来安全风险。作者应说明如何避免发布不安全的图像。\n%         \\item 我们认识到提供有效的安全防护具有挑战性，且许多论文并不要求这样做，但我们鼓励作者将此纳入考虑并尽最大诚信努力。",
      "chunk_id": "befd9910-62ee-4523-a098-27140aa5a3e6",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not use existing assets.\n%         \\item The authors should cite the original paper that produced the code package or dataset.\n%         \\item The authors should state which version of the asset is used and, if possible, include a URL.\n%         \\item The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n%         \\item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\n%         \\item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, [[URL_234]] has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\n%         \\item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\n%         \\item If this information is not available online, the authors are encouraged to reach out to the asset's creators.\n%",
      "translation": "%         \\item 答案 NA 表示论文未使用现有资源。\n%         \\item 作者应当引用产生该代码包或数据集的原始论文。\n%         \\item 作者应说明所使用资源的版本，并在可能的情况下附上 URL。\n%         \\item 每项资源应包含其许可证名称（例如 CC-BY 4.0）。\n%         \\item 对于从特定来源（例如网站）抓取的数据，应提供该来源的版权信息和服务条款。\n%         \\item 如果发布了资源，应在包中提供许可证、版权信息和使用条款。对于流行数据集，[[URL_234]] 为一些数据集整理了许可信息。他们的许可指南可帮助确定数据集的许可证。\n%         \\item 对于被重新打包的已有数据集，应同时提供原始许可证以及派生资源的许可证（如果有更改）。\n%         \\item 如果这些信息在网上不可获得，鼓励作者联系资源的创建者。",
      "chunk_id": "369d8762-f16f-4d68-8124-8abefbfba254",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not release new assets.\n%         \\item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \n%         \\item The paper should discuss whether and how consent was obtained from people whose asset is used.\n%         \\item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n%",
      "translation": "%         \\item 回答 NA 表示论文未发布新的资源。\n%         \\item 研究者应通过结构化模板在提交时提供数据集/代码/模型的详细信息，包括训练细节、许可、局限性等。\n%         \\item 论文应讨论是否以及如何从相关个人处获得了使用其资产的同意。\n%         \\item 在提交时，请记得对你的资源进行匿名化（如适用）。你可以创建匿名化的 URL，或附上匿名化的压缩文件。",
      "chunk_id": "6dfb3293-122e-4531-8a92-cc4dea3f0c8b",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n%         \\item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \n%         \\item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. \n%",
      "translation": "%         \\item 答案 NA 表示论文不涉及众包或与人类受试者相关的研究。\n%         \\item 将这些信息放在补充材料中是可以的，但如果论文的主要贡献涉及人类受试者，则应在正文中尽可能详尽地说明。\n%         \\item 根据 NeurIPS 伦理守则，参与数据收集、整理或其他劳动的工作人员应至少获得数据收集者所在国家的最低工资。",
      "chunk_id": "87f96f37-86ac-41c8-9adf-3fc60aa64436",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n%         \\item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \n%         \\item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \n%         \\item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\n%",
      "translation": "%         \\item 答案 NA 表示论文既不涉及众包也不涉及人类受试者研究。\n%         \\item 根据研究进行的国家或地区，任何涉及人类受试者的研究可能需要 IRB 批准（或同等审批）。如果已取得 IRB 批准，应在论文中明确说明。\n%         \\item 我们认识到此类程序在不同机构和地区可能存在显著差异，期望作者遵守 NeurIPS 道德准则以及其所属机构的相关指南。\n%         \\item 在初次提交时，不要包含任何可能破坏匿名性的内容（如适用），例如负责审稿的机构名称。",
      "chunk_id": "1104868c-20bf-4079-be66-1526be766bd5",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "%         \\item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.\n%         \\item Please refer to our LLM policy ([[URL_235]]) for what should or should not be described.\n%",
      "translation": "%         \\item 回答 NA 表示本研究的核心方法开发不涉及 LLM 作为任何重要、原创或非标准的组成部分。\n%         \\item 有关应当或不应当描述的内容，请参阅我们的 LLM 政策（[[URL_235]]）。",
      "chunk_id": "8d4e99a4-00b3-4dba-9789-9acfdb61cc44",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "Earth observation through remote sensing has emerged as a cornerstone of modern geospatial analysis, enabling unprecedented insights into environmental dynamics, urban planning, and disaster management~[[CITE_48]]. Satellite and aerial images provide a unique vantage point for monitoring planetary-scale phenomena, ranging from deforestation patterns to coastal erosion. However, converting this raw pixel data into actionable insights requires more than traditional computer vision techniques; it demands models capable of reasoning about spatial context, domain knowledge, and implicit user intent. Conventional remote sensing workflows predominantly rely on explicit tasks, \\eg, semantic segmentation and referring segmentation~[[CITE_49]], which operate within fixed taxonomies and require precise user instructions. While effective for well-defined scenarios, these approaches struggle to accommodate complex, implicit queries—for example, identifying regions at elevated risk of landslides based on slope, vegetation cover, and proximity to infrastructure. Such tasks limit implicit reasoning over heterogeneous spatial patterns, object relationships, and environmental metadata, exceeding the capabilities of standard segmentation or detection pipelines.",
      "translation": "通过遥感进行的地球观测已成为现代地理空间分析的基石，为环境动态、城市规划和灾害管理提供了前所未有的洞见~[[CITE_48]]。卫星与航拍影像为监测从森林砍伐模式到海岸侵蚀等行星尺度现象提供了独特视角。然而，要把这些原始像素数据转化为可操作的结论，单靠传统计算机视觉技术并不够；需要能够对空间语境、领域知识和隐含用户意图进行推理的模型。传统的遥感工作流程主要依赖显式任务，\\eg, 语义分割和指代分割~[[CITE_49]]，它们在固定的分类体系内运行并且依赖精确的用户指令。尽管在定义明确的场景中这些方法效果良好，但在处理复杂的隐含查询时却显得力不从心——例如，基于坡度、植被覆盖和与基础设施的接近程度来识别高滑坡风险区域。这类任务要求对异质空间模式、对象关系及环境元数据进行隐式推理，已超出标准分割或检测流程的能力范围。",
      "chunk_id": "ea563b09-1680-4c8e-82f4-9332dc34e24b",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Motivated by this, we introduce a new task, \\ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To enable research in this task, we build and release the first large-scale benchmark dataset, called EarthReason, which contains 5,434 manually annotated remote sensing image-mask pairs drawn from diverse classification sources, covering 28 scene categories at spatial resolutions ranging from 0.5m to 153m. Each image is paired with multiple implicit reasoning questions that require the model to infer target masks based on contextual and domain-specific knowledge, rather than explicit object names. In addition, by incorporating empty target cases and varying spatial scales, EarthReason pushes models to generalize across complex real-world remote sensing scenarios.",
      "translation": "受此启发，我们提出了一个新任务，\\ie，geospatial pixel reasoning，允许进行隐式查询与推理并生成目标区域的掩码。为推动该任务的研究，我们构建并发布了第一个大规模基准数据集 EarthReason，包含 5,434 对人工标注的遥感图像-掩码对，这些样本来自多种分类来源，覆盖 28 个场景类别，空间分辨率范围为 0.5m 到 153m。每幅图像都配有多个隐式推理问题，要求模型基于上下文信息与领域知识推断目标掩码，而非依赖明确的目标名称。此外，EarthReason 通过引入空目标样例和不同的空间尺度，推动模型在复杂的真实遥感场景中实现更强的泛化能力。",
      "chunk_id": "e932073a-aebc-4499-9152-1c97eec04107",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Recent progress in multimodal large language models (MLLMs) has demonstrated impressive performance in natural image domains, where models like LISA~[[CITE_50]] and PixelLM~[[CITE_51]] leverage large language models (LLMs)~[[CITE_52]] to interpret rich textual prompts and generate pixel-level outputs. These frameworks excel at tasks such as reasoning segmentation~[[CITE_53]], where the target mask is not directly specified but must be inferred from nuanced language cues. Unfortunately, directly transferring these methods to geospatial pixel reasoning is non-trivial since remote sensing images present extreme scale variation, densely packed small-scale objects and ultra-high resolution that violate assumptions of natural images. Moreover, different from natural images, remote sensing queries often require spatial correlations. For instance, identifying ``informal settlements'' relies on detecting roof material irregularities, road network fragmentation, and spatial adjacency to legal land-use zones.",
      "translation": "近期在多模态大语言模型 MLLM 方面的进展在自然图像领域表现出显著效果，例如 LISA~[[CITE_50]] 和 PixelLM~[[CITE_51]] 等模型借助 large language models (LLM)~[[CITE_52]] 来解析丰富的文本提示并生成像素级输出。这类框架在诸如推理分割~[[CITE_53]] 的任务上尤为擅长——此类任务中目标掩模并未被直接指定，而需从细微的语言线索中推断出来。遗憾的是，将这些方法直接迁移到地理空间像素推理并非易事，因为遥感影像存在极端的尺度变化、密集的小尺度目标以及超高分辨率，这些特性违背了自然图像的常见假设。此外，区别于自然图像，遥感查询通常还需要考虑显著的空间关联性。例如，识别“informal settlements”通常需要同时检测屋顶材料的不规则性、道路网络的碎片化程度以及与合法土地使用区的空间邻接关系。",
      "chunk_id": "55def7b8-b011-4f4d-8288-65f807eda501",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "To address these challenges, we present SegEarth-R1, a simple yet effective language-guided segmentation model that integrates a hierarchical visual encoder, an LLM for instruction parsing, and a tailored mask generator designed for spatial correlation. Further, some components are also designed to adapt to the characteristics of remote sensing images. Specifically, we propose the aggressive visual token compression to handle ultra-high-resolution images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Despite its architectural simplicity, SegEarth-R1 achieves advanced performance on EarthReason and referring segmentation datasets, significantly outperforming both traditional and LLM-based segmentation methods.",
      "translation": "为了解决这些挑战，我们提出了 SegEarth-R1，一种简单而有效的语言引导分割模型。该模型融合了分层视觉编码器、用于指令解析的 LLM 以及为空间相关性定制的掩码生成器。此外，若干组件还针对遥感影像的特性进行了适配。具体而言，我们提出了用于处理超高分辨率图像的激进视觉令牌压缩策略、用于将语言与多尺度特征融合的描述投影模块，以及直接查询描述嵌入的精简掩码预测流程。尽管架构较为简洁，SegEarth-R1 在 EarthReason 和指称分割数据集上均取得了领先表现，显著优于传统以及基于 LLM 的分割方法。",
      "chunk_id": "0806dc39-ff54-4613-b719-b6dc87d2c73c",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "Referring segmentation aims to segment targets in an image based on natural language descriptions, requiring precise alignment between linguistic expressions and visual content. Early approaches adopted CNN-RNN/LSTM frameworks~[[CITE_54]] to extract visual features and encode textual queries, respectively. However, these methods struggled with complex expressions due to limited local receptive fields and insufficient cross-modal interaction~[[CITE_55]]. To address these limitations, attention mechanisms~[[CITE_56]] emerged as a pivotal technique~[[CITE_57]]. VLT~[[CITE_58]] dynamically generates adaptive query vectors based on image-text interactions, enabling precise localization through cross-modal attention. LAVT~[[CITE_59]] further advances this paradigm by integrating hierarchical visual-linguistic fusion within a Swin Transformer~[[CITE_60]] backbone, where pixel-word attention refines multiscale features to achieve fine-grained semantic alignment. In remote sensing, specifying segmentation for certain instances can improve interpretation efficiency and user interactivity. Recently, Yuan \\textit{et al.}~[[CITE_61]] introduced referring segmentation into satellite images for the first time. Subsequently, following the LAVT~[[CITE_62]] architecture, RMSIN~[[CITE_63]] also incorporated adaptive rotated convolutions to address scale and orientation variations. FIANet~[[CITE_64]] and CroBIM~[[CITE_65]] introduced elaborate cross-modal interactions for feature alignment. RSSep~[[CITE_66]] reformulated referring segmentation as a sequence-to-sequence task, predicting polygonal boundaries to handle scale variations and blurred edges~[[CITE_67]]. However, existing methods effectively follow explicit instructions for target segmentation but lack implicit intent reasoning. In this paper, the proposed geospatial pixel reasoning task advances beyond referring segmentation by employing LLMs' reasoning capabilities to interpret subtle instructions and accurately segment desired targets.",
      "translation": "指代分割旨在根据自然语言描述对图像中的目标进行分割，要求语言表达与视觉内容之间精确对齐。早期方法采用了 CNN-RNN/LSTM 框架~[[CITE_54]] 来提取视觉特征并编码文本查询，但由于局部感受野有限且跨模态交互不足，这些方法难以处理复杂表达~[[CITE_55]]。为了解决这些局限，注意力机制~[[CITE_56]] 成为关键技术~[[CITE_57]]。VLT~[[CITE_58]] 基于图文交互动态生成自适应查询向量，通过跨模态注意力实现精确定位。LAVT~[[CITE_59]] 更进一步，在 Swin Transformer~[[CITE_60]] 主干中融入分层的视觉-语言融合，利用像素-词语注意力精炼多尺度特征以达到细粒度语义对齐。在遥感领域，为特定实例指定分割有助于提升解读效率和用户交互性。最近，Yuan \\textit{et al.}~[[CITE_61]] 首次将指代分割引入卫星影像。随后，基于 LAVT 架构的 RMSIN~[[CITE_63]] 也通过加入自适应旋转卷积来应对尺度和方向的变化~[[CITE_62]]。FIANet~[[CITE_64]] 与 CroBIM~[[CITE_65]] 引入了更为精细的跨模态交互以对齐特征。RSSep~[[CITE_66]] 则将指代分割重新表述为序列到序列任务，预测多边形边界以处理尺度变化和边缘模糊~[[CITE_67]]。然而，现有方法虽能有效执行明确指令下的目标分割，却缺乏对隐含意图的推理能力。本文提出的地理空间像素推理任务超越了指代分割，依托 LLM 的推理能力来解读细微指令并精确分割目标。",
      "chunk_id": "85fda28b-26df-4f88-ae81-51a7c88b18b0",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 4
      }
    },
    {
      "source": "Recent advances in LLMs have significantly expanded their capabilities to integrate pixel-level segmentation with language reasoning~[[CITE_68]]. For instance, Florence-2~[[CITE_69]] unified text, detection, and segmentation through a sequence-to-sequence framework with task instructions. To address the complexity of real-world segmentation scenarios, some works focus on architectural specialization and instruction-aware adaptation. LISA~[[CITE_70]] established the paradigm by introducing a \\texttt{[SEG]} token to connect LLMs with segmentation decoders like SAM~[[CITE_71]], enabling language-guided mask prediction. Subsequent studies enhanced this paradigm: GSVA~[[CITE_72]] introduced shared-weight \\texttt{[SEG]} tokens and \\texttt{[REJ]} tokens for multi-target and empty-target handling~[[CITE_73]], while GLaMM~[[CITE_74]] achieved pixel-grounded conversational capabilities through holistic segmentation~[[CITE_75]]. Parallel efforts focused on architectural unification - PSALM~[[CITE_76]] established a flexible input schema for multi-task segmentation, and OMG-LLaVA~[[CITE_77]] combined universal segmentation backbones with LLMs for pixel-level reasoning. Video understanding extensions emerged through VISA~[[CITE_78]] and InstructSeg~[[CITE_79]], which integrated temporal reasoning. Notably, Text4Seg~[[CITE_80]] redefined segmentation as a text generation problem using semantic descriptors, eliminating the need for an additional decoder. In remote sensing, benefiting from the above paradigms~[[CITE_81]], some unified models such as RSUniVLM~[[CITE_82]], GeoGround~[[CITE_83]] and GeoPix~[[CITE_84]] are equipped with segmentation capabilities. Although based on LLM, these models focus only on explicit text-guided segmentation. Further, GeoPixel~[[CITE_85]] introduced grounded conversation generation~[[CITE_86]] to remote sensing, but it still does not provide reasoning capability. Our SegEarth-R1 also follows the LLM-based segmentation paradigm, but is different from previous methods. Specifically, SegEarth-R1 is the first work to support reasoning about the target region from implicit queries, and its components are specifically designed for the challenges in remote sensing.",
      "translation": "最近在 LLM 方面的进展显著扩展了其将像素级分割与语言推理相结合的能力~[[CITE_68]]。例如，Florence-2~[[CITE_69]] 通过带有任务指令的序列到序列框架，将文本、检测与分割统一起来。为应对真实世界分割场景的复杂性，部分研究侧重于架构专门化与指令感知的适配。LISA~[[CITE_70]] 通过引入 \\texttt{[SEG]} token 将 LLM 与诸如 SAM~[[CITE_71]] 的分割解码器连接起来，奠定了语言引导掩膜预测的范式。后续工作对该范式进行了增强：GSVA~[[CITE_72]] 提出共享权重的 \\texttt{[SEG]} token 与 \\texttt{[REJ]} token 以处理多目标与空目标场景~[[CITE_73]]，而 GLaMM~[[CITE_74]] 则通过整体分割实现了像素级的对话能力~[[CITE_75]]。与此同时，也有工作致力于架构统一——PSALM~[[CITE_76]] 为多任务分割建立了灵活的输入方案，OMG-LLaVA~[[CITE_77]] 将通用分割主干与 LLM 结合以实现像素级推理。面向视频理解的拓展由 VISA~[[CITE_78]] 与 InstructSeg~[[CITE_79]] 提出，二者引入了时序推理。值得注意的是，Text4Seg~[[CITE_80]] 将分割重新定义为使用语义描述符的文本生成问题，从而无需额外的解码器。在遥感领域，受上述范式的启发~[[CITE_81]]，一些统一模型如 RSUniVLM~[[CITE_82]]、GeoGround~[[CITE_83]] 与 GeoPix~[[CITE_84]] 被赋予了分割能力。尽管这些模型基于 LLM，但它们仅聚焦于显式文本引导的分割。此外，GeoPixel~[[CITE_85]] 将 grounded conversation generation~[[CITE_86]] 引入遥感，但仍未提供推理能力。我们的 SegEarth-R1 同样遵循基于 LLM 的分割范式，但与以往方法不同：SegEarth-R1 首次支持从隐式查询中对目标区域进行推理，其各组件也针对遥感场景的挑战进行了专项设计。",
      "chunk_id": "b52d60c9-35ac-442f-9bc5-2fb0f2fc3de7",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 5
      }
    },
    {
      "source": "Dataset & Mask Label & Reasoning Query & Spatial resolution & Image Size & Image Num & Image Source & Class Num \\\\",
      "translation": "数据集 & 掩码标签 & 推理查询 & 空间分辨率 & 图像大小 & 图像数量 & 图像来源 & 类别数 \\\\",
      "chunk_id": "bdf4c825-0d0f-4225-ac96-0178045e5bb5",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "{\\color{white!50!black}ReasonSeg} [[CITE_87]] & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}-} & {\\color{white!50!black}-} & {\\color{white!50!black}1,218} & \\makecell[c]{{\\color{white!50!black}OpenImages (Seg) \\&} \\\\ {\\color{white!50!black}ScanNetv2 (Seg)}} & {\\color{white!50!black}-} \\\\\n    {\\color{white!50!black}LLM-Seg40K} [[CITE_88]] & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}-} & {\\color{white!50!black}-} & {\\color{white!50!black}14,000} & \\makecell[c]{{\\color{white!50!black}LVIS (Seg) \\&} \\\\ {\\color{white!50!black}EgoObjects (Seg)}} & {\\color{white!50!black}-} \\\\",
      "translation": "{\\color{white!50!black}ReasonSeg} [[CITE_87]] & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}-} & {\\color{white!50!black}-} & {\\color{white!50!black}1,218} & \\makecell[c]{{\\color{white!50!black}OpenImages (Seg) \\&} \\\\ {\\color{white!50!black}ScanNetv2 (Seg)}} & {\\color{white!50!black}-} \\\\\n    {\\color{white!50!black}LLM-Seg40K} [[CITE_88]] & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}\\checkmark} & {\\color{white!50!black}-} & {\\color{white!50!black}-} & {\\color{white!50!black}14,000} & \\makecell[c]{{\\color{white!50!black}LVIS (Seg) \\&} \\\\ {\\color{white!50!black}EgoObjects (Seg)}} & {\\color{white!50!black}-} \\\\",
      "chunk_id": "4c1a3573-5c76-4140-921e-edd4f3f34dd6",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "EarthVQA~[[CITE_89]] & \\ding{55} & \\checkmark & 0.3m & [[MATH_5]] & 6,000 & LoveDA (Seg) & 14 \\\\\n    RegSegRS~[[CITE_90]] & \\checkmark & \\ding{55} & 0.5m-30m & [[MATH_6]] & 4,420 & SkyScapes (Seg) & 14 \\\\\n    RRSIS-D~[[CITE_91]] & \\checkmark & \\ding{55} & 0.13m & [[MATH_7]] & 17,402 & RSVGD (VG) \\& DIOR (OD) & 20 \\\\",
      "translation": "EarthVQA~[[CITE_89]] & \\ding{55} & \\checkmark & 0.3m & [[MATH_5]] & 6,000 & LoveDA (Seg) & 14 \\\\\n    RegSegRS~[[CITE_90]] & \\checkmark & \\ding{55} & 0.5m-30m & [[MATH_6]] & 4,420 & SkyScapes (Seg) & 14 \\\\\n    RRSIS-D~[[CITE_91]] & \\checkmark & \\ding{55} & 0.13m & [[MATH_7]] & 17,402 & RSVGD (VG) \\& DIOR (OD) & 20 \\\\",
      "chunk_id": "8d871354-e79e-49fc-8d15-0c188129ffdc",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "RISBench~[[CITE_93]] & \\checkmark & \\ding{55} & 0.1m-30m & [[MATH_8]] & 52,472 & DOTAv2(OD) \\& DIOR (OD) & 26 \\\\",
      "translation": "RISBench~[[CITE_93]] & \\checkmark & \\ding{55} & 0.1m-30m & [[MATH_8]] & 52,472 & DOTAv2(OD) \\& DIOR (OD) & 26 \\\\",
      "chunk_id": "61240f2e-dcb4-4e98-8165-ba8230187a6d",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "EarthReason & \\checkmark & \\checkmark & 0.5m-153m & [[MATH_9]]-[[MATH_10]] & 5,434 & AID (Cls) \\& fMoW (Cls) & 28  \\\\",
      "translation": "EarthReason & \\checkmark & \\checkmark & 0.5m-153m & [[MATH_9]]-[[MATH_10]] & 5,434 & AID (Cls) \\& fMoW (Cls) & 28  \\\\",
      "chunk_id": "1d336074-3d1e-436c-b835-486ded7d2d2d",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "We analyze three types of tasks and datasets related to geospatial pixel reasoning, \\ie, natural image reasoning segmentation, remote sensing visual question answering (VQA), and remote sensing referring segmentation, as shown in Table~[[REF_177]].\nRefSegRS~[[CITE_94]] and RRSIS-D~[[CITE_95]] provide early benchmarks with image-text-mask triplets. RISBench~[[CITE_96]], the largest RRSIS dataset to date, introduced 52,472 triplets with oriented bounding boxes and pixel-level masks generated via a semi-automatic pipeline. These datasets address the limitations of earlier text-focused datasets (\\eg, RSICD~[[CITE_97]], EarthVQA~[[CITE_98]], \\etc) and enable comprehensive evaluation of multimodal models. Compared to the previous referring segmentation datasets, our EarthReason datasets has the following features: \\textbf{(1)} The mask labels in EarthReason are not explicitly specified by the query, but require further reasoning to determine the target, which challenges the model's reasoning ability. \\textbf{(2)} EarthReason uses a more raw data source. The previous related datasets directly transform existing segmentation datasets~[[CITE_99]] or SAM-processed detection datasets~[[CITE_100]], while our EarthReason uses images from classification datasets~[[CITE_101]] and we manually annotate them. This allows EarthReason to provide more data gain when it comes to co-training of unified segmentation tasks. \\textbf{(3)} EarthReason has more diverse spatial resolutions and image sizes, which are conducive to solving the object scale spanning problem inherent in remote sensing images~[[CITE_102]]. Compared to the first natural image reasoning segmentation dataset, ReasonSeg, EarthReason contains [[MATH_11]] more data than it. Therefore, we believe that EarthReason, as the first geospatial pixel reasoning dataset in the remote sensing area, is capable of performing initial explorations of this task.",
      "translation": "我们分析了三类与地理空间像素推理相关的任务和数据集，\\ie, 自然图像推理分割、遥感视觉问答（VQA）和遥感指代分割，如 Table~[[REF_177]] 所示。RefSegRS~[[CITE_94]] 和 RRSIS-D~[[CITE_95]] 提供了早期包含图像—文本—掩码三元组的基准。RISBench~[[CITE_96]]，迄今为止最大的 RRSIS 数据集，通过半自动流程引入了 52,472 个带有定向边界框和像素级掩码的三元组。这些数据集弥补了早期以文本为主的数据集（\\eg, RSICD~[[CITE_97]], EarthVQA~[[CITE_98]], \\etc）的局限，使多模态模型得以进行更全面的评估。与此前的指代分割数据集相比，EarthReason 具有以下特点： \\textbf{(1)} EarthReason 中的掩码标签并非由查询显式指定，而是需要进一步推理以确定目标，从而对模型的推理能力提出了更高要求。 \\textbf{(2)} EarthReason 使用更原始的数据来源。此前相关数据集通常直接转换自现有的分割数据集~[[CITE_99]]或基于 SAM 处理的检测数据集~[[CITE_100]]，而 EarthReason 则采用来自分类数据集~[[CITE_101]] 的图像并由人工标注，这在统一分割任务的联合训练中能带来更多的数据增益。 \\textbf{(3)} EarthReason 拥有更多样的空间分辨率和图像尺寸，有利于解决遥感图像中固有的对象尺度跨越问题~[[CITE_102]]。与第一个自然图像推理分割数据集 ReasonSeg 相比，EarthReason 的数据量多出 [[MATH_11]]。因此，我们认为 EarthReason 作为遥感领域首个地理空间像素推理数据集，能够对该任务开展初步探索。",
      "chunk_id": "c0533510-5759-4560-b7cc-f8f5afb00a34",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Our benchmark dataset EarthReason is generated according to the following three steps, i.e., image collection, question-answer pair generation, and object mask labeling.",
      "translation": "我们的基准数据集 EarthReason 按照以下三个步骤生成：图像收集、问答对生成和对象掩码标注。",
      "chunk_id": "8116a005-12f6-4ae0-8296-7c69b3a28baf",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\noindent\n\\textbf{Image Collection.} As mentioned above, to avoid potential data leakage in the future construction of unified segmentation models for remote sensing, we collect images from existing classification data. Although this increases the annotation cost, it also motivates more diverse scenes. Specifically, we first select the 28 categories that are more suitable for reasoning in the Million-AID~[[CITE_103]] dataset, and sample about 200 images for each category. Then, we find that the actual geographic range contained in Million-AID's images is limited. Thus, we also collect 800 images in the fMoW~[[CITE_104]] dataset to enhance the model's reasoning ability in complex scenes. Further, to alleviate the factitious illusion issue~[[CITE_105]], we add an extra 200 empty target images (\\ie, the implied target is not in the image). Finally, some low-quality images are eliminated, and we obtain a total of 5,434 images.",
      "translation": "\\noindent\n\\textbf{Image Collection.} 如上所述，为了在未来构建用于遥感的统一分割模型时避免潜在的数据泄露，我们从现有的分类数据中收集图像。虽然这增加了标注成本，但也促使场景更为多样。具体而言，我们首先从 Million-AID~[[CITE_103]] 数据集中筛选出更适合推理的 28 个类别，并为每个类别采样约 200 张图像。随后我们发现 Million-AID 的图像所涵盖的实际地理范围有限。因此，我们还从 fMoW~[[CITE_104]] 数据集中收集了 800 张图像，以增强模型在复杂场景下的推理能力。进一步地，为缓解虚构幻觉问题~[[CITE_105]]，我们额外加入了 200 张空目标图像（\\ie，隐含目标不在图像中）。最后，剔除了一些低质量图像，最终得到共计 5,434 张图像。",
      "chunk_id": "96f0325f-2dd0-4fcf-aa52-84a17d803fe1",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "\\noindent\n\\textbf{Question-Answer Pair Generation.} We use GPT-4o[[FOOTNOTE_236]] to construct question-answer pairs, and given its excellent visual comprehension, we take the remote sensing image and the corresponding scene category (provided by Million-AID and fMoW) as part of the prompt to generate questions and answers that are closely related to the image. An example of such a prompt is illustrated in Appendix~[[REF_178]]. In addition, following~[[CITE_106]], to make the questions and answers diverse, we adapt GPT-3.5 to rephrase the instructional questions and answers, as shown in Appendix Figure~[[REF_179]].",
      "translation": "\\noindent\n\\textbf{Question-Answer Pair Generation.} 我们使用 GPT-4o[[FOOTNOTE_236]] 来构建问答对。鉴于其出色的视觉理解能力，我们将遥感影像及相应的场景类别（由 Million-AID 和 fMoW 提供）作为提示的一部分，以生成与影像密切相关的问题与答案。此类提示的示例见附录~[[REF_178]]。此外，遵循~[[CITE_106]] 的做法，为了提高问题与答案的多样性，我们采用 GPT-3.5 对指令性问题和答案进行改写，参见附录图~[[REF_179]]。",
      "chunk_id": "65fd43e2-af3a-42dc-83d8-1e818105eb17",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 3
      }
    },
    {
      "source": "\\noindent\n\\textbf{Object Mask Labeling.} Different from previous referring and reasoning segmentation datasets (which use off-the-shelf masks or bounding boxes), we annotate images from scratch. Specifically, we employ multiple experts in remote sensing and vision, assign each expert a few hundred images to annotate, and cross-validate the annotations after they are completed. For simple targets (\\eg, lake), SAM-H~[[CITE_107]] is used to assist in annotation; for complex targets (\\eg, wind turbine), each point of the polygon is finely marked. A description of mask quality is provided in Appendix~[[REF_180]].",
      "translation": "\\noindent\n\\textbf{Object Mask Labeling.} 不同于先前的指称和推理分割数据集（这些数据集使用现成的掩码或边界框），我们从头对图像进行标注。具体而言，我们聘请了多名遥感与视觉领域的专家，每位专家分配数百张图像进行标注，并在标注完成后对标注结果进行交叉验证。对于简单目标（\\eg, lake），使用 SAM-H~[[CITE_107]] 辅助标注；对于复杂目标（\\eg, wind turbine），对多边形的每个点进行精细标注。关于掩码质量的说明见附录~[[REF_180]]。",
      "chunk_id": "8b0a2b0c-bc48-4231-ab97-dd30780e53ce",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\noindent\n\\textbf{Dataset Statistics.}\nThe EarthReason dataset is partitioned into training, validation, and testing sets, comprising 2,371, 1,135, and 1,928 images, respectively. In the training set, each image is annotated with an average of six questions and three corresponding answers. The average question length is 20.86 words, while the average answer length is 26.76 words. To assess the model’s generalization capability, several semantic categories are deliberately reserved for the validation and test sets, ensuring they remain unseen during training. Additional dataset details are provided in the Appendix~[[REF_181]].",
      "translation": "\\noindent\n\\textbf{Dataset Statistics.}\nEarthReason 数据集被划分为训练集、验证集和测试集，分别包含 2,371、1,135 和 1,928 张图像。训练集中每张图像平均标注了 6 个问题和 3 个对应答案。问题的平均长度为 20.86 个词，答案的平均长度为 26.76 个词。为评估模型的泛化能力，故意将若干语义类别保留在验证集和测试集中，确保这些类别在训练阶段未被见到。更多数据集细节见附录~[[REF_181]]。",
      "chunk_id": "405123e1-a529-4342-aa22-5a9d33dffdbc",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Compared with natural images, remote sensing images exhibit distinctive characteristics that demand specialized architectural designs for pixel-wise geospatial reasoning. In this work, we propose SegEarth-R1, a simple yet powerful baseline for geospatial pixel reasoning that effectively harnesses LLM capabilities while incorporating domain-specific adaptations. As illustrated in Figure~[[REF_182]], our architecture comprises three core parts: A visual encoder for image feature extraction, an LLM for instruction interpretation and semantic correlation, and a mask generator for spatial correlation and mask prediction. Each part incorporates critical design considerations to address the unique challenges of remote sensing images.",
      "translation": "与自然图像相比，遥感影像具有独特特性，这要求在像素级地理空间推理任务中采用专门的架构设计。在本工作中，我们提出了 SegEarth-R1——一个简单但功能强大的像素级地理空间推理基线，它在融入领域特定适配的同时，有效发挥了 LLM 的能力。如图 Figure~[[REF_182]] 所示，我们的架构由三大核心模块组成：用于图像特征提取的视觉编码器、用于指令解析与语义关联的 LLM，以及用于空间相关建模与掩膜预测的掩膜生成器。每一模块均包含针对遥感影像独特挑战的关键设计考量。",
      "chunk_id": "a459f311-a48d-4347-beff-0daf57e2be1d",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "Satellite and aerial targets present two critical challenges: (1) extreme scale variations ranging from sub-meter objects to kilometer-scale geographical formations~[[CITE_108]], and (2) densely distributed small objects requiring high-resolution analysis~[[CITE_109]]. Conventional ViT-based encoders adopted in MLLMs~[[CITE_110]] (\\eg, image encoder in CLIP~[[CITE_111]] and SAM~[[CITE_112]]) prove suboptimal due to their fixed-scale feature extraction and information compression through aggressive patch merging. To alleviate these limitations, following~[[CITE_113]], SegEarth-R1 employs a Swin Transformer~[[CITE_114]] backbone enhanced with progressive feature hierarchy construction. This architecture generates multi-scale feature maps [[MATH_12]] at {1/4, 1/8, 1/16, 1/32} of the original resolution through controlled downsampling operations, preserving high-resolution details for small objects while capturing contextual semantics at deeper layers.",
      "translation": "卫星与航拍目标面临两大关键挑战：(1) 尺度极端变化，从亚米级目标到公里级地形构造~[[CITE_108]]；(2) 小目标密集分布，需进行高分辨率分析~[[CITE_109]]。由于固定尺度的特征提取以及通过激进的patch合并导致的信息压缩，MLLM~[[CITE_110]]中常用的基于ViT的编码器（\\eg，CLIP~[[CITE_111]]和SAM~[[CITE_112]]中的图像编码器）在此类场景下表现欠佳。为缓解这些局限，依据~[[CITE_113]]，SegEarth-R1采用了带有渐进式特征层次构建的Swin Transformer~[[CITE_114]]主干。该架构通过受控下采样生成多尺度特征图 [[MATH_12]]，其分辨率为原图的{1/4, 1/8, 1/16, 1/32}，既保留了对小目标有利的高分辨率细节，又在更深层次捕捉了上下文语义。",
      "chunk_id": "69f054ab-a24f-4a2a-9999-d4565fd222df",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "SegEarth-R1 adopts the MLLM paradigm~[[CITE_115]] by jointly embedding visual tokens and textual instructions into a unified LLM input space for multimodal reasoning. Unlike natural images, remote sensing data exhibits ultra-high-resolution coverage~[[CITE_116]], posing computational challenges when processed through billion-level LLMs. Therefore, we expect to compress the visual token to alleviate the computational cost and make only simple semantic correlations in LLM.",
      "translation": "SegEarth-R1 采用 MLLM 范式~[[CITE_115]]，通过将视觉标记和文本指令联合嵌入到统一的 LLM 输入空间来实现多模态推理。与自然图像不同，遥感数据具有超高分辨率覆盖~[[CITE_116]]，在通过十亿级别的 LLM 处理时带来了计算上的挑战。因此，我们希望压缩视觉标记以缓解计算开销，并仅在 LLM 中进行简单的语义关联。",
      "chunk_id": "4a4e9d3d-ce3d-49c7-84a5-5009956e0c06",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 4
      }
    },
    {
      "source": "\\textbf{Redundancy Analysis.}\nImage redundancy quantifies the proportion of compressible, non-informative data within an image. To investigate the feasibility of aggressive visual token compression for remote sensing images, we conduct a redundancy analysis from dual perspectives: pixel-level statistical redundancy and spatial structural redundancy.",
      "translation": "\\textbf{Redundancy Analysis.} 图像冗余度量了图像中可压缩且非信息性数据的比例。为了评估对遥感影像实施激进视觉令牌压缩的可行性，我们从像素级统计冗余和空间结构冗余两个层面对冗余性进行了分析。",
      "chunk_id": "47783356-d5ff-438d-9055-99414d379630",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "We evaluate six benchmark datasets spanning natural images (COCO~[[CITE_120]], ADE20K~[[CITE_121]], PASCAL~[[CITE_122]]) and remote sensing images (LoveDA~[[CITE_123]], DeepGlobe~[[CITE_124]], xBD~[[CITE_125]]) for redundancy analysis. As shown in Figure~[[REF_183]], our analysis reveals two critical findings: 1) Remote sensing images demonstrate 1.9[[MATH_29]]3.3[[MATH_30]] higher entropic redundancy than natural images, indicating greater pixel-level compressibility. 2) The average self-similarity for remote sensing data exceeds natural images by 42.6\\%, confirming the higher prevalence of repetitive textures and geometric patterns. This insight justifies aggressive token compression for semantic-level comprehension in remote sensing images.",
      "translation": "我们为冗余性分析评估了六个基准数据集，涵盖自然影像（COCO~[[CITE_120]]、ADE20K~[[CITE_121]]、PASCAL~[[CITE_122]]）和遥感影像（LoveDA~[[CITE_123]]、DeepGlobe~[[CITE_124]]、xBD~[[CITE_125]]）。如图~[[REF_183]]所示，我们的分析揭示了两个关键发现：1) 遥感影像在熵冗余方面比自然影像高出 1.9[[MATH_29]]3.3[[MATH_30]]，表明其在像素层面具有更强的可压缩性；2) 遥感数据的平均自相似性比自然影像高出 42.6\\%，进一步证实了重复纹理和几何模式更为普遍。该洞见为在遥感影像的语义层面采用激进的 token 压缩提供了充分依据。",
      "chunk_id": "ca966102-c9b0-49b4-87d3-43fb45753232",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\textbf{Token Compression Connector.}\nIn modern MLLM, connectors such as Q-Former~[[CITE_126]] and MLP~[[CITE_127]] are designed to transform visual tokens into a multi-modal space. However, some works~[[CITE_128]] point out that Q-Former may lead to loss of vision information and is difficult to train. Therefore, in SegEarth-R1, we follow the MLP connector fashion in LLaVA~[[CITE_129]] and use a simple but effective connector, \\ie, stacked convolutional blocks and Layer Normalization (LN). Here, convolutional blocks are used for spatial down-sampling to compress the size of the feature map, and LN is used to stabilize cross-modal training. Specifically, our connector can be formulated as:",
      "translation": "\\textbf{Token Compression Connector.}\n在现代 MLLM 中，诸如 Q-Former~[[CITE_126]] 和 MLP~[[CITE_127]] 的连接器被设计用于将视觉 tokens 转换到多模态空间。然而，一些工作~[[CITE_128]] 指出 Q-Former 可能导致视觉信息丢失且难以训练。因此，在 SegEarth-R1 中，我们遵循 LLaVA~[[CITE_129]] 中的 MLP 连接器风格，采用一种简单但有效的连接器，\\ie，堆叠的卷积块与 Layer Normalization (LN)。其中，卷积块用于空间下采样以压缩特征图尺寸，LN 则用于稳定跨模态训练。具体地，我们的连接器可以表述为：",
      "chunk_id": "0f9f3a3f-0b6e-41e7-ae67-56e1c2036723",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "where [[MATH_31]] denotes the function composition operator, and [[MATH_32]] denotes the number of stacked layers.",
      "translation": "其中 [[MATH_31]] 表示函数复合算子，[[MATH_32]] 表示堆叠层数。",
      "chunk_id": "f4f6e595-e347-401d-b472-82b647e9c507",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Although the instructions involved in geospatial pixel reasoning are implicit and contain more words than referring segmentation, they still maintain the same data format. Therefore, it is easy to convert them into question-answer pairs using a template like ``\\textbf{USER}: This is an image <IMAGE>, please doing geospatial pixel reasoning according to the following instruction: <DESCRIPTION>. \\textbf{ASSISTANT}: <ANSWER>''. For referring segmentation task, the task name in the instruction is changed to ``referring segmentation''.",
      "translation": "尽管地理像素推理中的指令是隐含的、且比指称分割包含更多文字，但它们仍然保持相同的数据格式。因此，可以很容易地使用如下模板将其转换为问答对：``\\textbf{USER}: 这是图像 <IMAGE>，请根据以下指令进行地理像素推理：<DESCRIPTION>。 \\textbf{ASSISTANT}: <ANSWER>''。对于 referring segmentation 任务，指令中的任务名称改为``referring segmentation''。",
      "chunk_id": "be62cd57-9a92-4b2c-8e06-87002c37f210",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "Some recent LLM-based segmentation models~[[CITE_130]] use Mask2Former~[[CITE_131]] paradigm as mask generator. They use [[MATH_33]] learnable mask tokens (typically [[MATH_34]]=100) as queries in the transformer decoder to generate [[MATH_35]] candidate masks with corresponding scores, which are then assigned to the description embeddings[[FOOTNOTE_237]] by bipartite matching. Unlike the reasoning segmentation of natural images, which is more inclined to make inferences based on the attributes of the object itself. In geospatial pixel reasoning, the model must understand and extrapolate more based on the spatial layout and inter-object correlations within the image (\\eg, identifying earthquake evacuation zones in Figure~[[REF_184]], which requires analyzing topological relationships between roads and buildings). In addition, we posit that the mask query mechanism~[[CITE_132]] is inflexible and redundant in language-guided segmentation, and we only need to generate variable numbers of masks based on the instruction. Motivated by the above, we propose to directly use description embedding as the query for the mask generator and explicitly associate it with the image spatial features.",
      "translation": "一些最近基于 LLM 的分割模型~[[CITE_130]] 采用 Mask2Former~[[CITE_131]] 范式作为掩码生成器。它们在 transformer 解码器中使用 [[MATH_33]] 个可学习的掩码 token（通常 [[MATH_34]]=100）作为查询，生成带有对应分数的 [[MATH_35]] 个候选掩码，然后通过二分匹配将这些掩码分配给描述嵌入[[FOOTNOTE_237]]。与更倾向于基于目标自身属性进行推理的自然图像推理分割不同，在地理空间像素推理中，模型必须更多地基于图像内的空间布局和对象间的关联进行理解和推断（\\eg，识别 Figure~[[REF_184]] 中的地震撤离区就需要分析道路与建筑之间的拓扑关系）。此外，我们认为掩码查询机制~[[CITE_132]] 在语言引导的分割任务中显得不够灵活且存在冗余；实际上只需根据指令生成可变数量的掩码即可。基于上述动机，我们提出直接将描述嵌入作为掩码生成器的查询，并将其与图像的空间特征显式关联。",
      "chunk_id": "ac708269-7765-4244-951c-0b5fca964ea4",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "The length of description embeddings varies according to the user's instruction, whereas in our geospatial pixel reasoning or referring segmentation setting, the segmentation result can be represented by a single binary mask. Therefore, we introduce a description projection module ([[MATH_36]]-Projector), which converts the whole description into a single vector as shown in Figure~[[REF_185]]. Specifically, the description embeddings are averaged into a global vector, which is then interacted with the flattened multi-scale visual features via a cross-attention operation, and a skip-connection and linear layer maps it into the query vector. Following this, the query vector is fed into the Transformer decoder of Mask2Former, which is composed of stacked masked attention, self-attention and FFN. Notably, since the mask query mechanism has been removed, the number of generated masks is the same as the number of queries, and therefore, score prediction and bipartite matching are also not required. Finally, the predicted mask is supervised with a linear combination of focal loss~[[CITE_133]] and dice loss~[[CITE_134]].",
      "translation": "描述嵌入的长度随用户指令而变化，而在我们的地理空间像素推理或指称分割设置中，分割结果可以由单个二值掩码表示。因此，我们引入了描述投影模块（[[MATH_36]]-Projector），用于将整个描述转换为单一向量，如图~[[REF_185]]所示。具体地，先将描述嵌入求平均得到全局向量，然后通过交叉注意力与展平的多尺度视觉特征进行交互，随后通过跳跃连接和线性层将其映射为查询向量。接着，该查询向量被送入 Mask2Former 的 Transformer 解码器，后者由堆叠的掩蔽注意力、自注意力和 FFN 组成。值得注意的是，由于已移除 mask query 机制，生成的掩码数与查询数相同，因此不再需要得分预测和二分匹配。最后，用 focal loss~[[CITE_133]] 与 dice loss~[[CITE_134]] 的线性组合作为对预测掩码的监督。",
      "chunk_id": "e040c617-78df-4877-8048-4c952c2ea762",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "\\noindent\n\\textbf{Datasets and Tasks.} In addition to the geospatial pixel reasoning on EarthReason, we assess the basic geolocation capability with the explicit short query of our model. We employed two benchmark referring image segmentation datasets: RefSegRS~[[CITE_135]] and RRSIS-D~[[CITE_136]]. These datasets contain 14 and 20 semantic categories, respectively, with textual descriptions primarily focusing on direct visual attributes such as orientation, color, and size. All models are trained on their own training set and evaluated on their validation and testing sets.",
      "translation": "\\noindent\n\\textbf{Datasets and Tasks.} 除了 EarthReason 上的地理空间像素推理外，我们还通过对模型进行明确的短查询来评估其基本地理定位能力。我们采用了两个基准的指称图像分割数据集：RefSegRS~[[CITE_135]] 和 RRSIS-D~[[CITE_136]]。这些数据集分别包含 14 和 20 个语义类别，文本描述主要侧重于方向、颜色和大小等直接视觉属性。所有模型均在各自的训练集上训练，并在对应的验证集和测试集上进行评估。",
      "chunk_id": "346eeb63-b154-4b64-8ffd-50d066a72e21",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\noindent\n\\textbf{Evaluation metrics.} Following~[[CITE_137]], we adopt two evaluation metrics: \\textbf{gIoU} (per-image IoU average) and cIoU (cumulative intersection over union). The latter is preferred because of its stability.",
      "translation": "\\noindent\n\\textbf{Evaluation metrics.} 参照~[[CITE_137]]，我们采用两项评估指标：\\textbf{gIoU}（每幅图像的 IoU 平均）和 cIoU（累积交并比）。后者因其稳定性而被优先采用。",
      "chunk_id": "3bcdd5d2-ab57-441a-8ee5-fd85396d209e",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\noindent\n\\textbf{Network Architecture.} Unless otherwise specified, SegEarth-R1 use phi-1.5 (1.3B)~[[CITE_138]] as the LLM, and adopt the Swin-B as the visual encoder. The token compression connector is configured with a layer number [[MATH_37]]. The mask generator follows the Mask2Former architecture, but removes mask tokens as mentioned above.",
      "translation": "\\noindent\n\\textbf{Network Architecture.} 除非另有说明，SegEarth-R1 使用 phi-1.5 (1.3B)~[[CITE_138]] 作为 LLM，并采用 Swin-B 作为视觉编码器。令牌压缩连接器的层数配置为 [[MATH_37]]。掩码生成器遵循 Mask2Former 架构，但如上所述移除了掩码令牌。",
      "chunk_id": "5b3c9b47-3622-4fb2-ab01-d0f2064488eb",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "\\noindent\n\\textbf{Implementation details.} During training, we use bf16 precision and freeze the visual encoder. The LLM is initialized from Phi-1.5, while both the Swin-B encoder and the mask generator are initialized with pretrained weights from Mask2Former. All images are resized to [[MATH_38]], maintaining the original aspect ratio by padding the shorter side. We adopt the AdamW optimizer with an initial learning rate of [[MATH_39]], cosine learning rate schedule, and no weight decay. A uniform batch size of 16 is used across datasets, with training steps set to 7,610 (RRSIS-D), 5,400 (RefSegRS), and 2,220 (EarthReason). All experiments are conducted on two NVIDIA A100 80GB GPUs.",
      "translation": "\\noindent\n\\textbf{Implementation details.} 在训练过程中，我们使用 bf16 精度并冻结视觉编码器。LLM 从 Phi-1.5 初始化，而 Swin-B 编码器和掩码生成器均采用 Mask2Former 的预训练权重初始化。所有图像被调整为 [[MATH_38]]，通过对短边填充以保持原始宽高比。我们采用 AdamW 优化器，初始学习率为 [[MATH_39]]，使用余弦学习率调度且不使用权重衰减。在各数据集上统一使用批量大小 16，训练步数分别设为 7,610（RRSIS-D）、5,400（RefSegRS）和 2,220（EarthReason）。所有实验均在两块 NVIDIA A100 80GB GPU 上进行。",
      "chunk_id": "370501c2-c938-4111-9ae7-a8bed0c21ae5",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "LISA~[[CITE_139]] & CLIP-L & Vicuna-7B~[[CITE_140]] & 57.39 & 59.10 & 61.04 & 60.88 \\\\\n    PixelLM~[[CITE_141]] & CLIP-L & Vicuna-7B~[[CITE_142]] & 57.79 & 59.22 & 57.94 & 60.01 \\\\",
      "translation": "LISA~[[CITE_139]] & CLIP-L & Vicuna-7B~[[CITE_140]] & 57.39 & 59.10 & 61.04 & 60.88 \\\\\n    PixelLM~[[CITE_141]] & CLIP-L & Vicuna-7B~[[CITE_142]] & 57.79 & 59.22 & 57.94 & 60.01 \\\\",
      "chunk_id": "fa0e9d8f-3b19-437b-8f7b-f1f1adcc76d9",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "PSALM~[[CITE_144]] & Swin-B & phi-1.5 (1.3B)~[[CITE_145]] & 62.03 & 64.61 & 66.61 & 68.30 \\\\\n    \\textit{SegEarth-R1} & Swin-B & phi-1.5 (1.3B)~[[CITE_146]] & \\textbf{64.13} & \\textbf{68.25} & \\textbf{68.60} & \\textbf{70.75} \\\\",
      "translation": "PSALM~[[CITE_144]]，骨干为 Swin-B，使用 phi-1.5 (1.3B)~[[CITE_145]]，各项得分分别为 62.03、64.61、66.61 和 68.30。\\\\\n\\textit{SegEarth-R1}，骨干为 Swin-B，使用 phi-1.5 (1.3B)~[[CITE_146]]，各项得分分别为 \\textbf{64.13}、\\textbf{68.25}、\\textbf{68.60} 和 \\textbf{70.75}。\\\\",
      "chunk_id": "36e58d85-1d83-4ad6-91fa-6cde316f85ee",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "We conduct a comparative evaluation of SOTA LLM-based methods and SegEarth-R1 on the EarthReason dataset. As shown in Table~[[REF_186]], all models are trained solely on the training split of EarthReason to ensure a fair comparison. LISA and PixelLM demonstrate comparable performance; however, despite leveraging larger LLM or MLLM, the quality of their predicted segmentation masks remains suboptimal. This can be primarily attributed to their reliance on CLIP as the visual encoder, which tends to diminish the representation of small-scale geospatial targets. As one of the baselines of SegEarth-R1, PSALM achieves notable improvements over LISA and PixelLM. Nevertheless, PSALM does not adequately incorporate LLM-based segmentation and the Mask2Former paradigm, and lacks considerations for overhead images. SegEarth-R1 achieves the best results on both metrics surpassing PSALM by 3.64\\% and 2.45\\% on the test set. Importantly, SegEarth-R1 uses fewer visual tokens in LLM and reduces the number of queries in the mask generator, thus providing a lower inference cost.",
      "translation": "我们在 EarthReason 数据集上对 SOTA LLM-based 方法与 SegEarth-R1 进行了比较评估。如 Table~[[REF_186]] 所示，所有模型均仅在 EarthReason 的训练集上训练，以确保比较公平。LISA 与 PixelLM 的表现相当；然而，尽管它们利用了更大的 LLM 或 MLLM，其预测的分割掩码质量仍然不理想。这主要归因于它们依赖 CLIP 作为视觉编码器，CLIP 往往削弱了对小尺度地理目标的表征。作为 SegEarth-R1 的基线之一，PSALM 相较于 LISA 和 PixelLM 取得了显著提升。尽管如此，PSALM 并未充分结合基于 LLM 的分割与 Mask2Former 范式，也缺乏对航拍图像（overhead images）的专门考量。SegEarth-R1 在两项指标上均取得了最优结果，在测试集上分别比 PSALM 高出 3.64\\% 和 2.45\\%。值得注意的是，SegEarth-R1 在 LLM 中使用更少的视觉 tokens，并减少了掩码生成器中的 queries 数量，从而降低了推理成本。",
      "chunk_id": "e134c14a-e327-44e5-8b32-4ff0d783dd57",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 6
      }
    },
    {
      "source": "\\multicolumn{8}{@{}l}{\\textbf{\\textit{Traditional method:}}} \\\\\n    RRN~[[CITE_147]] & {\\tiny CVPR'18} & 51.09 & 51.07 & 66.53 & 66.43 & 46.06 & 45.64 \\\\\n    CSMC~[[CITE_148]] & {\\tiny CVPR'19} & 55.68 & 55.32 & 69.39 & 69.39 & 48.85 & 48.54 \\\\\n    LSCM~[[CITE_149]] & {\\tiny ECCV'20} & 57.12 & 56.02 & 69.05 & 69.28 & 50.36 & 49.92 \\\\\n    CMPC~[[CITE_150]] & {\\tiny CVPR'20} & 57.93 & 55.83 & 69.22 & 69.39 & 50.41 & 49.24 \\\\\n    BRINet~[[CITE_151]] & {\\tiny CVPR'20} & 58.79 & 56.90 & 70.73 & 69.88 & 51.14 & 49.65 \\\\\n    CMPC+~[[CITE_152]] & {\\tiny TPAMI'20} & 59.19 & 57.65 & 70.14 & 68.64 & 51.41 & 50.24 \\\\\n    LGCE~[[CITE_153]] & {\\tiny TGRS'24} & 68.10 & 67.65 & 76.68 & 76.34 & 60.16 & 59.37 \\\\\n    RIS-DMMI~[[CITE_154]] & {\\tiny CVPR'23} & 70.40 & 68.74 & 77.01 & 76.20 & 60.72 & 60.12 \\\\\n    LAVT~[[CITE_155]] & {\\tiny CVPR'22} & 69.54 & 69.52 & 77.59 & 77.19 & 61.46 & 61.04 \\\\\n    RMSIN~[[CITE_156]] & {\\tiny CVPR'24} & 74.66 & 74.26 & 78.27 & 77.79 & 65.10 & 64.20 \\\\",
      "translation": "\\multicolumn{8}{@{}l}{\\textbf{\\textit{Traditional method:}}} \\\\\n    RRN~[[CITE_147]] & {\\tiny CVPR'18} & 51.09 & 51.07 & 66.53 & 66.43 & 46.06 & 45.64 \\\\\n    CSMC~[[CITE_148]] & {\\tiny CVPR'19} & 55.68 & 55.32 & 69.39 & 69.39 & 48.85 & 48.54 \\\\\n    LSCM~[[CITE_149]] & {\\tiny ECCV'20} & 57.12 & 56.02 & 69.05 & 69.28 & 50.36 & 49.92 \\\\\n    CMPC~[[CITE_150]] & {\\tiny CVPR'20} & 57.93 & 55.83 & 69.22 & 69.39 & 50.41 & 49.24 \\\\\n    BRINet~[[CITE_151]] & {\\tiny CVPR'20} & 58.79 & 56.90 & 70.73 & 69.88 & 51.14 & 49.65 \\\\\n    CMPC+~[[CITE_152]] & {\\tiny TPAMI'20} & 59.19 & 57.65 & 70.14 & 68.64 & 51.41 & 50.24 \\\\\n    LGCE~[[CITE_153]] & {\\tiny TGRS'24} & 68.10 & 67.65 & 76.68 & 76.34 & 60.16 & 59.37 \\\\\n    RIS-DMMI~[[CITE_154]] & {\\tiny CVPR'23} & 70.40 & 68.74 & 77.01 & 76.20 & 60.72 & 60.12 \\\\\n    LAVT~[[CITE_155]] & {\\tiny CVPR'22} & 69.54 & 69.52 & 77.59 & 77.19 & 61.46 & 61.04 \\\\\n    RMSIN~[[CITE_156]] & {\\tiny CVPR'24} & 74.66 & 74.26 & 78.27 & 77.79 & 65.10 & 64.20 \\\\",
      "chunk_id": "2b0b3a93-5032-4ee6-bad6-97e256300204",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\multicolumn{8}{@{}l}{\\textbf{\\textit{LLM-based method:}}} \\\\\n    LISA~[[CITE_157]] & {\\tiny CVPR'24} & 27.07 & 24.51 & - & - & 27.84 & 26.78 \\\\\n    PixelLM~[[CITE_158]] & {\\tiny CVPR'24} & 33.46 & 28.81 & - & - & 33.89 & 31.65 \\\\\n    NEXT-Chat~[[CITE_159]] & {\\tiny arXiv'23} & 28.97 & 26.37 & - & - & 26.98 & 24.98 \\\\\n    GeoGround~[[CITE_160]] & {\\tiny arXiv'25} & 68.69 & 67.50 & - & - & 61.10 & 60.50 \\\\",
      "translation": "\\multicolumn{8}{@{}l}{\\textbf{\\textit{LLM-based method:}}} \\\\\n    LISA~[[CITE_157]] & {\\tiny CVPR'24} & 27.07 & 24.51 & - & - & 27.84 & 26.78 \\\\\n    PixelLM~[[CITE_158]] & {\\tiny CVPR'24} & 33.46 & 28.81 & - & - & 33.89 & 31.65 \\\\\n    NEXT-Chat~[[CITE_159]] & {\\tiny arXiv'23} & 28.97 & 26.37 & - & - & 26.98 & 24.98 \\\\\n    GeoGround~[[CITE_160]] & {\\tiny arXiv'25} & 68.69 & 67.50 & - & - & 61.10 & 60.50 \\\\",
      "chunk_id": "bd4ee9cf-60ce-4d77-86f4-130b05fea4da",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "SegEarth-R1 also supports basic explicit language-guided segmentation. As shown in Table~[[REF_187]], we compare its performance with existing SOTA traditional methods (not based on LLM) as well as recent LLM-based methods. Notably, prior to SegEarth-R1, LLM-based methods consistently underperformed in comparison to traditional methods on the referring segmentation task. For instance, the advanced GeoGround~[[CITE_161]] lags behind RMSIN~[[CITE_162]] by 3.7\\% in terms of gIoU on the RRSIS-D dataset. In contrast, SegEarth-R1, as a universal LLM-based language-guided segmentation method, surpasses traditional methods on the referring segmentation task for the first time with a 2.2\\% improvement. This result highlights the enhanced generalization capability and practical potential of SegEarth-R1. On the RefSegRS dataset, the improvement of SegEarth-R1 is more significant than the previous method, with an 8.33\\% and 9.87\\% improvement over RMSIN on the validation and testing sets, respectively, as listed in Table~[[REF_188]].",
      "translation": "SegEarth-R1 还支持基础的显式语言引导分割。如 Table~[[REF_187]] 所示，我们将其性能与现有的 SOTA 传统方法（不基于 LLM）以及近期基于 LLM 的方法进行了比较。值得注意的是，在 SegEarth-R1 之前，基于 LLM 的方法在指称分割任务上一直落后于传统方法。例如，先进的 GeoGround~[[CITE_161]] 在 RRSIS-D 数据集上的 gIoU 比 RMSIN~[[CITE_162]] 低 3.7\\%。相比之下，作为一种通用的基于 LLM 的语言引导分割方法，SegEarth-R1 首次在指称分割任务上超越了传统方法，提升了 2.2\\%，该结果凸显了 SegEarth-R1 更强的泛化能力与实际应用潜力。在 RefSegRS 数据集上，SegEarth-R1 的提升更加显著：在验证集和测试集上分别较 RMSIN 提升了 8.33\\% 和 9.87\\%，详见 Table~[[REF_188]]。",
      "chunk_id": "28fb537e-7f82-468d-ab13-0ebe2b89840f",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 5
      }
    },
    {
      "source": "\\multirow{2}{*}{Method} & & \\multicolumn{2}{c|}{P@0.5} & \\multicolumn{2}{c|}{P@0.6} & \\multicolumn{2}{c|}{P@0.7} & \\multicolumn{2}{c|}{P@0.8} & \\multicolumn{2}{c|}{P@0.9} & \\multicolumn{2}{c|}{cloU} & \\multicolumn{2}{c}{gloU} \\\\",
      "translation": "\\multirow{2}{*}{Method} & & \\multicolumn{2}{c|}{P@0.5} & \\multicolumn{2}{c|}{P@0.6} & \\multicolumn{2}{c|}{P@0.7} & \\multicolumn{2}{c|}{P@0.8} & \\multicolumn{2}{c|}{P@0.9} & \\multicolumn{2}{c|}{cloU} & \\multicolumn{2}{c}{gloU} \\\\",
      "chunk_id": "9ecbcf26-e848-4435-91d5-750f759c3493",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "BRINet~[[CITE_163]] & {\\tiny CVPR'20} & 36.86 & 20.72 & 35.53 & 14.26 & 19.93 & 9.87 & 10.66 & 2.98 & 2.84 & 1.14 & 61.59 & 58.22 & 38.73 & 31.51 \\\\\n    LSCM~[[CITE_164]] & {\\tiny ECCV'20} & 56.82 & 31.54 & 41.24 & 20.41 & 21.85 & 9.51 & 12.11 & 5.29 & 2.51 & 0.84 & 62.82 & 61.27 & 40.59 & 35.54 \\\\\n    CMPC~[[CITE_165]] & {\\tiny CVPR'20} & 46.09 & 32.36 & 26.45 & 14.14 & 12.76 & 6.55 & 7.42 & 1.76 & 1.39 & 0.22 & 63.55 & 55.39 & 42.08 & 40.63 \\\\\n    CMSA~[[CITE_166]] & {\\tiny CVPR'19} & 39.24 & 28.07 & 38.44 & 20.25 & 20.39 & 12.71 & 11.79 & 5.61 & 1.52 & 0.83 & 65.84 & 64.53 & 43.62 & 41.47 \\\\\n    RRN~[[CITE_167]] & {\\tiny CVPR'18} & 55.43 & 30.26 & 42.98 & 23.01 & 23.11 & 14.87 & 13.72 & 7.17 & 2.64 & 0.98 & 69.24 & 65.06 & 50.81 & 41.88 \\\\\n    EVF-SAM~[[CITE_168]] & {\\tiny Arxiv'24} & 57.77 & 35.17 & 37.59 & 22.34 & 16.24 & 9.36 & 4.87 & 2.86 & 1.86 & 0.39 & 59.61 & 55.51 & 46.98 & 36.64 \\\\\n    CMPC+~[[CITE_169]] & {\\tiny TPAMI'21} & 56.84 & 49.19 & 37.59 & 28.31 & 20.42 & 15.31 & 10.67 & 8.12 & 2.78 & 0.55 & 70.62 & 66.53 & 47.13 & 43.65 \\\\\n    CARIS~[[CITE_170]] & {\\tiny ACMMM'23} & 68.45 & 45.40 & 47.10 & 27.19 & 25.52 & 15.08 & 14.62 & 8.87 & 3.71 & 1.98 & 75.79 & 69.74 & 54.30 & 42.66 \\\\\n    CRIS~[[CITE_171]] & {\\tiny CVPR'22} & 53.13 & 35.77 & 36.19 & 24.11 & 24.36 & 14.36 & 11.83 & 6.38 & 2.55 & 1.21 & 72.14 & 65.87 & 53.74 & 43.26 \\\\\n    LAVT~[[CITE_172]] & {\\tiny CVPR'22} & 80.97 & 51.84 & 58.70 & 30.27 & 31.09 & 17.34 & 15.55 & 9.52 & 4.64 & 2.09 & 78.50 & 71.86 & 61.53 & 47.40 \\\\\n    RIS-DMMI~[[CITE_173]] & {\\tiny CVPR'23} & 86.17 & 63.89 & 74.71 & 44.30 & 38.05 & 19.81 & 18.10 & 6.49 & 3.25 & 1.00 & 74.02 & 68.58 & 65.72 & 52.15 \\\\\n    LGCE~[[CITE_174]] & {\\tiny TGRS'24} & 90.72 & 73.75 & 86.31 & 61.14 & 71.93 & 39.46 & 32.95 & 16.02 & 10.21 & 5.45 & 83.56 & 76.81 & 72.51 & 59.96 \\\\\n    RMSIN~[[CITE_175]] & {\\tiny CVPR'24} & 93.97 & 79.20 & 89.33 & 65.99 & 74.25 & 42.98 & 29.70 & 16.51 & 7.89 & 3.25 & 82.41 & 75.72 & 73.84 & 62.58 \\\\\n    SegEarth-R1 & & \\textbf{95.82} & \\textbf{86.30} & \\textbf{93.27} & \\textbf{79.53} & \\textbf{88.86} & \\textbf{69.57} & \\textbf{78.19} & \\textbf{48.87} & \\textbf{22.04} & \\textbf{10.73} & \\textbf{85.01} & \\textbf{79.00} & \\textbf{82.17} & \\textbf{72.45} \\\\",
      "translation": "BRINet~[[CITE_163]] & {\\tiny CVPR'20} & 36.86 & 20.72 & 35.53 & 14.26 & 19.93 & 9.87 & 10.66 & 2.98 & 2.84 & 1.14 & 61.59 & 58.22 & 38.73 & 31.51 \\\\\n    LSCM~[[CITE_164]] & {\\tiny ECCV'20} & 56.82 & 31.54 & 41.24 & 20.41 & 21.85 & 9.51 & 12.11 & 5.29 & 2.51 & 0.84 & 62.82 & 61.27 & 40.59 & 35.54 \\\\\n    CMPC~[[CITE_165]] & {\\tiny CVPR'20} & 46.09 & 32.36 & 26.45 & 14.14 & 12.76 & 6.55 & 7.42 & 1.76 & 1.39 & 0.22 & 63.55 & 55.39 & 42.08 & 40.63 \\\\\n    CMSA~[[CITE_166]] & {\\tiny CVPR'19} & 39.24 & 28.07 & 38.44 & 20.25 & 20.39 & 12.71 & 11.79 & 5.61 & 1.52 & 0.83 & 65.84 & 64.53 & 43.62 & 41.47 \\\\\n    RRN~[[CITE_167]] & {\\tiny CVPR'18} & 55.43 & 30.26 & 42.98 & 23.01 & 23.11 & 14.87 & 13.72 & 7.17 & 2.64 & 0.98 & 69.24 & 65.06 & 50.81 & 41.88 \\\\\n    EVF-SAM~[[CITE_168]] & {\\tiny Arxiv'24} & 57.77 & 35.17 & 37.59 & 22.34 & 16.24 & 9.36 & 4.87 & 2.86 & 1.86 & 0.39 & 59.61 & 55.51 & 46.98 & 36.64 \\\\\n    CMPC+~[[CITE_169]] & {\\tiny TPAMI'21} & 56.84 & 49.19 & 37.59 & 28.31 & 20.42 & 15.31 & 10.67 & 8.12 & 2.78 & 0.55 & 70.62 & 66.53 & 47.13 & 43.65 \\\\\n    CARIS~[[CITE_170]] & {\\tiny ACMMM'23} & 68.45 & 45.40 & 47.10 & 27.19 & 25.52 & 15.08 & 14.62 & 8.87 & 3.71 & 1.98 & 75.79 & 69.74 & 54.30 & 42.66 \\\\\n    CRIS~[[CITE_171]] & {\\tiny CVPR'22} & 53.13 & 35.77 & 36.19 & 24.11 & 24.36 & 14.36 & 11.83 & 6.38 & 2.55 & 1.21 & 72.14 & 65.87 & 53.74 & 43.26 \\\\\n    LAVT~[[CITE_172]] & {\\tiny CVPR'22} & 80.97 & 51.84 & 58.70 & 30.27 & 31.09 & 17.34 & 15.55 & 9.52 & 4.64 & 2.09 & 78.50 & 71.86 & 61.53 & 47.40 \\\\\n    RIS-DMMI~[[CITE_173]] & {\\tiny CVPR'23} & 86.17 & 63.89 & 74.71 & 44.30 & 38.05 & 19.81 & 18.10 & 6.49 & 3.25 & 1.00 & 74.02 & 68.58 & 65.72 & 52.15 \\\\\n    LGCE~[[CITE_174]] & {\\tiny TGRS'24} & 90.72 & 73.75 & 86.31 & 61.14 & 71.93 & 39.46 & 32.95 & 16.02 & 10.21 & 5.45 & 83.56 & 76.81 & 72.51 & 59.96 \\\\\n    RMSIN~[[CITE_175]] & {\\tiny CVPR'24} & 93.97 & 79.20 & 89.33 & 65.99 & 74.25 & 42.98 & 29.70 & 16.51 & 7.89 & 3.25 & 82.41 & 75.72 & 73.84 & 62.58 \\\\\n    SegEarth-R1 & & \\textbf{95.82} & \\textbf{86.30} & \\textbf{93.27} & \\textbf{79.53} & \\textbf{88.86} & \\textbf{69.57} & \\textbf{78.19} & \\textbf{48.87} & \\textbf{22.04} & \\textbf{10.73} & \\textbf{85.01} & \\textbf{79.00} & \\textbf{82.17} & \\textbf{72.45} \\\\",
      "chunk_id": "2702fc42-8da1-46dd-aa32-b1bf00eca26d",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "\\ding{55} & \\ding{55} & \\ding{55} & 62.03 & 64.61 & 66.61 & 68.30 \\\\\n    \\Checkmark & \\ding{55} & \\ding{55} & 63.34 & 66.19 & 67.42 & 69.15 \\\\\n    \\ding{55} & \\Checkmark & \\ding{55} & 63.32 & 66.31 & 67.22 & 69.21 \\\\\n    \\ding{55} & \\ding{55} & \\Checkmark & 63.47 & 65.41 & 68.31 & 69.20 \\\\\n    \\Checkmark & \\Checkmark & \\ding{55} & 64.12 & 66.71 & \\textbf{68.61} & 69.61 \\\\\n    \\Checkmark & \\Checkmark & \\Checkmark & \\textbf{64.13} & \\textbf{68.25} & 68.60 & \\textbf{70.75} \\\\",
      "translation": "\\ding{55} & \\ding{55} & \\ding{55} & 62.03 & 64.61 & 66.61 & 68.30 \\\\\n    \\Checkmark & \\ding{55} & \\ding{55} & 63.34 & 66.19 & 67.42 & 69.15 \\\\\n    \\ding{55} & \\Checkmark & \\ding{55} & 63.32 & 66.31 & 67.22 & 69.21 \\\\\n    \\ding{55} & \\ding{55} & \\Checkmark & 63.47 & 65.41 & 68.31 & 69.20 \\\\\n    \\Checkmark & \\Checkmark & \\ding{55} & 64.12 & 66.71 & \\textbf{68.61} & 69.61 \\\\\n    \\Checkmark & \\Checkmark & \\Checkmark & \\textbf{64.13} & \\textbf{68.25} & 68.60 & \\textbf{70.75} \\\\",
      "chunk_id": "5360f715-b47b-4eca-a5ea-a78b0190cada",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "phi-1.5 (1.3B) & 78.92 & 78.01 & 67.56 & 66.40 \\\\\n    phi-2 (2B) & \\textbf{78.98} & \\textbf{78.35} & \\textbf{67.91} & \\textbf{66.67} \\\\\n    Qwen2.5 (0.5B) & 78.53 & 77.87 & 67.70 & 66.49 \\\\",
      "translation": "phi-1.5 (1.3B) & 78.92 & 78.01 & 67.56 & 66.40 \\\\\n    phi-2 (2B) & \\textbf{78.98} & \\textbf{78.35} & \\textbf{67.91} & \\textbf{66.67} \\\\\n    Qwen2.5 (0.5B) & 78.53 & 77.87 & 67.70 & 66.49 \\\\",
      "chunk_id": "184912ee-6482-4feb-b573-713eeadb0d0d",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 1
      }
    },
    {
      "source": "0 & 1024 & 68.28 & \\textbf{2} & \\textbf{64} & \\textbf{68.60} \\\\\n    1 & 256 & 68.47 & 3 & 16 & 68.22 \\\\",
      "translation": "0 & 1024 & 68.28 & \\textbf{2} & \\textbf{64} & \\textbf{68.60} \\\\\n    1 & 256 & 68.47 & 3 & 16 & 68.22 \\\\",
      "chunk_id": "6dbf13ca-1178-4e20-a658-71b7ecc06a27",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\noindent\n\\textbf{Components.}\nWe conduct ablation studies on the EarthReason dataset to evaluate the effectiveness of the novel components involved in SegEarth-R1. As listed in Table~[[REF_189]], the first row shows the results of the PSALM baseline. Each proposed component contributes to performance enhancement, yielding improvements ranging from 0.85\\% to 0.9\\%. The T.C. Connector and Query D.E. not only enhances performance but also reduces computational overhead. Further, the proposed components can be well coupled, and when they are all activated, \\ie, complete SegEarth-R1, all metrics exhibit substantial gains over the baseline, confirming the effectiveness and compatibility of the proposed design. In fact, although these components are initially designed with remote sensing scenarios in mind, their underlying principles offer transferable insights applicable to general image understanding.",
      "translation": "\\noindent\n\\textbf{Components.}\n我们在 EarthReason 数据集上进行了消融实验，以评估 SegEarth-R1 中各新组件的有效性。如 Table~[[REF_189]] 所示，第一行给出 PSALM 基线的结果。各个提出的组件均能带来性能提升，幅度在 0.85\\% 至 0.9\\% 之间。T.C. Connector 和 Query D.E. 不仅提高了性能，还能降低计算开销。此外，这些组件能够良好耦合，当它们全部启用，即完整的 SegEarth-R1 时，所有指标相较基线均有显著提升，验证了所提设计的有效性和兼容性。实际上，尽管这些组件最初针对遥感场景设计，其基本原理同样为通用图像理解提供了可借鉴的见解。",
      "chunk_id": "f1ed3423-c391-4268-8b39-24837218994a",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "\\noindent\n\\textbf{LLM Type.}\nGiven the limited scale of the dataset, we select some small LLM for comparation, as presented in Table~[[REF_190]]. SegEarth-R1 demonstrates consistently high performance across different LLM, indicating the robustness and architectural stability of the overall framework. Notably, with Qwen2.5 (0.5B)~[[CITE_176]], it still achieves competitive results, indicating its potential for edge deployment.",
      "translation": "\\noindent\n\\textbf{LLM Type.}\n鉴于数据集规模有限，我们选取了一些小型的 LLM 进行对比，如 Table~[[REF_190]] 所示。SegEarth-R1 在不同的 LLM 上均表现出持续且较高的性能，表明整个框架在鲁棒性和结构稳定性方面具有优势。值得注意的是，使用 Qwen2.5 (0.5B)~[[CITE_176]] 时仍能取得具有竞争力的结果，显示出其在边缘部署方面的潜力。",
      "chunk_id": "a5b2831a-5e91-46ce-9d9c-8c8abc576b6f",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 4
      }
    },
    {
      "source": "\\noindent\n\\textbf{Layer Number of T.C. Connector.}\nThe layer number [[MATH_43]] controls the number of visual tokens fed into the LLM. As shown in Table~[[REF_191]], increasing token quantity does not improve performance. This observation aligns with our earlier analysis, suggesting that appropriate compression of visual tokens is beneficial for the global understanding of a remote sensing image. In SegEarth-R1, spatial correlations between the image and the instruction are primarily handled by the mask generator, while the LLM is only responsible for relatively semantic correlations. This division of labor allows for more efficient use of computational resources without compromising performance.",
      "translation": "\\noindent\n\\textbf{Layer Number of T.C. Connector.}\n层数 [[MATH_43]] 决定送入 LLM 的视觉令牌数量。如 Table~[[REF_191]] 所示，增加令牌数量并未带来性能提升。该观察与我们之前的分析一致，表明对视觉令牌进行适当压缩有助于对遥感影像的整体理解。在 SegEarth-R1 中，图像与指令之间的空间相关性主要由掩码生成器处理，而 LLM 则只负责相对的语义相关性。这种分工使得在不牺牲性能的前提下能够更高效地利用计算资源。",
      "chunk_id": "baedc3a8-5a75-4d16-a8e0-9bdeea292daf",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "In this paper, we introduce geospatial pixel reasoning, a new task in remote sensing that requires models to infer segmentation masks from implicit natural language queries by reasoning over spatial context and domain knowledge. To enable research in this direction, we present EarthReason, the first large-scale benchmark dataset that emphasises complex reasoning scenarios. To address the distinct challenges inherent in remote sensing, we propose SegEarth-R1, a language-guided segmentation model that integrates a hierarchical visual encoder, an LLM for instruction parsing and semantic correlation, and a tailored mask generator designed for spatial correlation. Extensive experiments validate SegEarth-R1’s superiority, achieving SOTA performance on both geospatial pixel reasoning and referring segmentation tasks. This work pioneers the fusion of natural language reasoning with pixel-level geospatial analysis, offering transformative potential for applications like environmental monitoring and disaster response.",
      "translation": "在本文中，我们提出了地理空间像素推理（geospatial pixel reasoning）这一遥感新任务，该任务要求模型通过对空间上下文和领域知识进行推理，从隐含的自然语言查询中推断分割掩码。为推动该方向的研究，我们构建并发布了 EarthReason——首个强调复杂推理场景的大规模基准数据集。针对遥感领域的独特挑战，我们提出了 SegEarth-R1，一种语言引导的分割模型，集成了分层视觉编码器、用于指令解析与语义关联的 LLM，以及为空间相关性专门设计的掩码生成器。大量实验证明了 SegEarth-R1 的优越性，在地理空间像素推理和指称分割任务上均取得了 SOTA 的性能。该工作开创性地将自然语言推理与像素级地理空间分析相结合，对环境监测和灾害响应等应用具有潜在的变革性价值。",
      "chunk_id": "76b26573-cd68-40c4-8bfe-6f852cffaaa2",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "Each sample of the EarthReason benchmark consists of an image, a corresponding mask, and six reasoning queries along with their respective answers. Given that our metadata is derived from classification datasets, we employed GPT-4o and GPT-3.5 to generate textual annotations, and invited multiple remote sensing and vision experts to provide accurate and reliable mask annotations. Overall, our annotation process consists of the following three steps:",
      "translation": "EarthReason 基准的每个样本由一幅图像、对应的掩码以及六个推理查询及其各自答案组成。鉴于我们的元数据来自分类数据集，我们采用了 GPT-4o 和 GPT-3.5 来生成文本注释，并邀请了多位遥感与视觉领域的专家参与，以提供准确可靠的掩码标注。总体而言，我们的标注流程包括以下三个步骤：",
      "chunk_id": "b403029b-e79b-4a55-8d8f-7c0ef5b88827",
      "metadata": {
        "had_glossary_terms": true,
        "glossary_terms_count": 2
      }
    },
    {
      "source": "The EarthReason benchmark comprises 28 categories, and the number of samples in each category is shown in Figure~[[REF_195]] (a). It can be observed that the distribution of the 28 categories is relatively balanced. Figure~[[REF_196]] (b), (c), and (d) illustrate the category distributions in the training, validation, and test sets, respectively. To evaluate the model's generalization capability, we specifically excluded four categories—``basketball court'', ``island'', ``lake'', and ``stadium''—from the training set. Moreover, we introduced 119 empty target samples to mitigate potential hallucinations of the model.",
      "translation": "EarthReason 基准包含 28 个类别，各类别的样本数量见 Figure~[[REF_195]] (a)。可以观察到这 28 个类别的分布相对均衡。Figure~[[REF_196]] (b)、(c) 和 (d) 分别展示了训练集、验证集和测试集的类别分布。为评估模型的泛化能力，我们专门将四个类别——``basketball court''、``island''、``lake'' 和 ``stadium''——从训练集中剔除。此外，我们引入了 119 个空目标样本，以减轻模型可能出现的幻觉。",
      "chunk_id": "88f3f47d-c959-425b-9ae6-0d70b4011646",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Table~[[REF_197]] presents the hyper-parameter settings used during the training of our model. For training on the referring segmentation datasets, we employ only focal loss and dice loss to supervise mask generation. In contrast, for training on geospatial pixel reasoning task, we additionally incorporate the cross-entropy loss from the large language model to supervise text answer generation.",
      "translation": "Table~[[REF_197]] 列出了训练本模型时使用的超参数设置。在对指称分割数据集进行训练时，我们仅采用 focal loss（焦点损失）和 dice loss（Dice 损失）来监督掩码生成。相较之下，在地理空间像素推理任务的训练中，我们还额外引入来自大规模语言模型的交叉熵损失以监督文本答案的生成。",
      "chunk_id": "95762f28-08aa-48d5-b69e-3de2173d657c",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Optimizer & AdamW \\\\\nLearning Rate & [[MATH_44]] \\\\\nBatch Size & 16 \\\\\nNumber of Iteration & 7,610 / 5,400 / 2,220 \\\\\nLearning Rate Schedule & Cosine Decay \\\\\nWeight Decay & 0.0 \\\\\nWarmup Ratio & 0.03 \\\\",
      "translation": "优化器 & AdamW \\\\\n学习率 & [[MATH_44]] \\\\\n批量大小 & 16 \\\\\n迭代次数 & 7,610 / 5,400 / 2,220 \\\\\n学习率调度 & Cosine Decay \\\\\n权重衰减 & 0.0 \\\\\n预热比例 & 0.03 \\\\",
      "chunk_id": "e0554426-d6f9-48f4-826b-491a87040be8",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Image Size & 1024 [[MATH_47]] 1024 \\\\\nImage Processing & \\begin{tabular}[t]{@{}l@{}}Resize long edge to 1024 \\\\ and padding short edge to 1024.\\end{tabular} \\\\",
      "translation": "图像尺寸 & 1024 [[MATH_47]] 1024 \\\\\n图像处理 & \\begin{tabular}[t]{@{}l@{}}将长边调整为 1024 \\\\ 并将短边填充至 1024。\\end{tabular} \\\\",
      "chunk_id": "a551820d-248f-4306-8c46-456321b0e24a",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Figure~[[REF_198]] presents a comparison between SegEarth-R1 and other models on the EarthReason dataset. It can be observed that our model demonstrates a better understanding of long reasoning instructions and produces more accurate mask generation.",
      "translation": "Figure~[[REF_198]] 展示了 SegEarth-R1 与其他模型在 EarthReason 数据集上的对比。可以看出，我们的模型对长推理指令具有更好的理解能力，并能生成更为准确的掩码。",
      "chunk_id": "83785636-b362-4dfe-8aa5-b8257be10214",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    },
    {
      "source": "Figure~[[REF_199]] presents a comparison between SegEarth-R1 and PSALM on the RRSIS-D dataset. Our model demonstrates a better understanding of direct geographical attributes such as location, color, and size compared to PSALM. This improvement is attributed to the removal of indirect mask prediction using mask tokens, allowing semantic information (description embeddings) to directly interact with image features to generate masks.",
      "translation": "Figure~[[REF_199]] 对比了 SegEarth-R1 与 PSALM 在 RRSIS-D 数据集上的表现。与 PSALM 相比，我们的模型在位置、颜色和大小等直接地理属性的理解上更为出色。这一改进归因于去除了使用 mask tokens 的间接掩码预测，使得语义信息（描述嵌入）能够与图像特征直接交互以生成掩码。",
      "chunk_id": "457fd68e-2fc1-4ee5-a307-9f6d710cabcf",
      "metadata": {
        "had_glossary_terms": false,
        "glossary_terms_count": 0
      }
    }
  ]
}