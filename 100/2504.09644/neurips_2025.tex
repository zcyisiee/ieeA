\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025
\PassOptionsToPackage{numbers, compress}{natbib}

% ready for submission
% \usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{colortbl}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow,hhline}
\usepackage{caption}

\usepackage{multirow}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{subcaption}
% \usepackage{subfig}

\usepackage{enumitem}

% \usepackage{hyperref}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{graphics}
\usepackage[export]{adjustbox}
\usepackage{bbding}

\usepackage{enumitem}

\newcommand{\tian}[1]{\textcolor{black}{#1}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\zhan}[1]{\textcolor{blue}{#1}}
\newcommand{\model}{\textit{SegEarth-R1}}
\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\etc}{\textit{etc.}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}


\title{SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model}

\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother

\author{
  \vspace{-25pt}\\
  \textbf{Kaiyu Li$^{1,*}$,
  \quad Zepeng Xin$^{1,*}$,
  \quad Li Pang$^1$,
  \quad Chao Pang$^{2}$,
  \quad Yupeng Deng$^{3}$} \\
  \textbf{ Jing Yao$^{3}$,
  \quad Guisong Xia$^{2}$,
  \quad Deyu Meng$^1$,
  \quad Zhi Wang$^1$,
  \quad Xiangyong Cao$^{1,\dag}$}\\
  % \vspace{3pt}
  $^1$Xi'an Jiaotong University ~~\quad $^2$Wuhan University ~~\quad $^3$Chinese Academy of Sciences\\
  \vspace{-25pt}
}
  
\definecolor{mygray}{gray}{.91}

\begin{document}

\maketitle

% \begin{figure}[ht]
% \vspace{-10pt}
% \begin{center}
% 	\includegraphics[width=1.0\linewidth]{figures/task.pdf}
% \end{center}
% \vspace{-7pt}
% \caption{\small xxx.
% }
% \vspace{-2pt}
% \label{fig:abs}
% \end{figure}

\renewcommand{\thefootnote}{*}
\footnotetext[1]{Equal contribution}
\renewcommand{\thefootnote}{\dag}
\footnotetext[1]{Corresponding author: caoxiangyong@mail.xjtu.edu.cn}
\renewcommand{\thefootnote}{\arabic{footnote}}

% 语言引导的分割任务

\begin{abstract}
Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over spatial context, domain knowledge, and implicit user intent. Motivated by this, we introduce a new task, \ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To advance this task, we construct and release the first large-scale benchmark dataset called EarthReason, which comprises 5,434 manually annotated image masks with over 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a simple yet effective language-guided segmentation baseline that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator for spatial correlation. The design of SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution remote sensing images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional and LLM-based segmentation methods. Our data and code will be released at \url{https://github.com/earth-insights/SegEarth-R1}.


% Recent advancements in Multimodal Large Language Models (MLLMs) have shown promise in natural image domains, but their direct application to remote sensing is challenging due to the unique characteristics of overhead imagery, such as extreme scale variation, oblique object orientations, and densely packed small-scale features. To address these challenges, we introduce SegEarth-R1, a novel framework for geospatial pixel reasoning that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator designed for spatial correlation. SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. 

% We also present EarthReason, the first large-scale benchmark for geospatial pixel reasoning, comprising 5,434 manually annotated remote sensing images with over 30,000 implicit question-answer pairs. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional segmentation methods and off-the-shelf MLLMs. Our contributions include the introduction of the remote sensing reasoning segmentation task, the release of the EarthReason dataset, and the design of SegEarth-R1, which advances the capabilities of LLM-based segmentation in remote sensing.

\end{abstract}

\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/title.pdf}
   \caption{\small Comparison of semantic segmentation, referring segmentation and geospatial pixel inference. (left) Samples from the LoveDA~\cite{wang2021loveda} and RRSIS-D~\cite{liu2024rotated} datasets. (right) Samples from the EarthReason dataset. Previous tasks are limited by fixed taxonomies and explicit instructions, while geospatial pixel reasoning supports complex implicit instructions and requires the reasoning capability of the model.}
   \label{fig:data_sample}
   \vspace{-1em}
\end{figure}


\section{Introduction}
\label{sec:intro}

Earth observation through remote sensing has emerged as a cornerstone of modern geospatial analysis, enabling unprecedented insights into environmental dynamics, urban planning, and disaster management~\cite{rolf2024mission, lu2025vision}. Satellite and aerial images provide a unique vantage point for monitoring planetary-scale phenomena, ranging from deforestation patterns to coastal erosion. However, converting this raw pixel data into actionable insights requires more than traditional computer vision techniques; it demands models capable of reasoning about spatial context, domain knowledge, and implicit user intent. Conventional remote sensing workflows predominantly rely on explicit tasks, \eg, semantic segmentation and referring segmentation~\cite{long2021creating, christie2018functional, yuan2024rrsis}, which operate within fixed taxonomies and require precise user instructions. While effective for well-defined scenarios, these approaches struggle to accommodate complex, implicit queries—for example, identifying regions at elevated risk of landslides based on slope, vegetation cover, and proximity to infrastructure. Such tasks limit implicit reasoning over heterogeneous spatial patterns, object relationships, and environmental metadata, exceeding the capabilities of standard segmentation or detection pipelines.

Motivated by this, we introduce a new task, \ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To enable research in this task, we build and release the first large-scale benchmark dataset, called EarthReason, which contains 5,434 manually annotated remote sensing image-mask pairs drawn from diverse classification sources, covering 28 scene categories at spatial resolutions ranging from 0.5m to 153m. Each image is paired with multiple implicit reasoning questions that require the model to infer target masks based on contextual and domain-specific knowledge, rather than explicit object names. In addition, by incorporating empty target cases and varying spatial scales, EarthReason pushes models to generalize across complex real-world remote sensing scenarios.

% To address these challenges, we introduce the geospatial pixel reasoning task and curate the first large-scale benchmark, EarthReason, for geospatial pixel reasoning. EarthReason comprises 5,434 manually annotated remote sensing image-mask pairs drawn from diverse classification sources, covering 28 scene categories at spatial resolutions ranging from 0.5m to 153m. Each image is paired with multiple implicit reasoning questions that demand the model to infer target masks based on contextual and domain-specific knowledge, rather than explicit object names. Further, by incorporating empty-target cases and varying spatial scales, EarthReason pushes models to generalize across complex, real-world remote sensing scenarios.

Recent progress in multimodal large language models (MLLMs) has demonstrated impressive performance in natural image domains, where models like LISA~\cite{lai2024lisa} and PixelLM~\cite{ren2024pixellm} leverage large language models (LLMs)~\cite{touvron2023llama, chiang2023vicuna, yang2024qwen2} to interpret rich textual prompts and generate pixel-level outputs. These frameworks excel at tasks such as reasoning segmentation~\cite{lai2024lisa}, where the target mask is not directly specified but must be inferred from nuanced language cues. Unfortunately, directly transferring these methods to geospatial pixel reasoning is non-trivial since remote sensing images present extreme scale variation, densely packed small-scale objects and ultra-high resolution that violate assumptions of natural images. Moreover, different from natural images, remote sensing queries often require spatial correlations. For instance, identifying ``informal settlements'' relies on detecting roof material irregularities, road network fragmentation, and spatial adjacency to legal land-use zones.

To address these challenges, we present SegEarth-R1, a simple yet effective language-guided segmentation model that integrates a hierarchical visual encoder, an LLM for instruction parsing, and a tailored mask generator designed for spatial correlation. Further, some components are also designed to adapt to the characteristics of remote sensing images. Specifically, we propose the aggressive visual token compression to handle ultra-high-resolution images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Despite its architectural simplicity, SegEarth-R1 achieves advanced performance on EarthReason and referring segmentation datasets, significantly outperforming both traditional and LLM-based segmentation methods.

In summary, our contributions are as follows:
\begin{itemize}[leftmargin=8pt]
\item We introduce the geospatial pixel reasoning task, which requires models to infer segmentation masks from implicit natural language queries by reasoning over spatial context and domain knowledge.
\item We build and release the first large-scale benchmark with 5,434 image-mask pairs, 28 categories, and over 30,000 implicit question-answer pairs, fostering research in geospatial pixel reasoning.
\item We propose an LLM-based segmentation model, SegEarth-R1, which incorporates new segmentation capabilities in remote sensing, containing several domain-specific designs.
\item Extensive experiments show that SegEarth-R1 achieves state-of-the-art performance on reasoning and referring segmentation tasks, compared to traditional methods and other LLM-based methods.


\end{itemize}



\section{Related Work}
\label{sec:related_work}

% 1. 参考分割
% 2. 推理分割
% 3. 基于LLM/MLLM的分割方法

% \subsection{}


\subsection{Referring Segmentation}
% 自然图像->遥感图像

Referring segmentation aims to segment targets in an image based on natural language descriptions, requiring precise alignment between linguistic expressions and visual content. Early approaches adopted CNN-RNN/LSTM frameworks~\cite{hu2016segmentation, liu2017recurrent, li2018referring, margffoy2018dynamic, shi2018key, huang2020referring} to extract visual features and encode textual queries, respectively. However, these methods struggled with complex expressions due to limited local receptive fields and insufficient cross-modal interaction~\cite{ji2024survey}. To address these limitations, attention mechanisms~\cite{vaswani2017attention} emerged as a pivotal technique~\cite{ding2021vision, yang2022lavt, wu2022language, hu2023beyond, xu2023bridging, nag2024safari, wu2024towards, shang2024prompt}. VLT~\cite{ding2021vision} dynamically generates adaptive query vectors based on image-text interactions, enabling precise localization through cross-modal attention. LAVT~\cite{yang2022lavt} further advances this paradigm by integrating hierarchical visual-linguistic fusion within a Swin Transformer~\cite{liu2021swin} backbone, where pixel-word attention refines multiscale features to achieve fine-grained semantic alignment. In remote sensing, specifying segmentation for certain instances can improve interpretation efficiency and user interactivity. Recently, Yuan \textit{et al.}~\cite{yuan2024rrsis} introduced referring segmentation into satellite images for the first time. Subsequently, following the LAVT~\cite{yang2022lavt} architecture, RMSIN~\cite{liu2024rotated} also incorporated adaptive rotated convolutions to address scale and orientation variations. FIANet~\cite{lei2024exploring} and CroBIM~\cite{dong2024cross} introduced elaborate cross-modal interactions for feature alignment. RSSep~\cite{ho2024rssep} reformulated referring segmentation as a sequence-to-sequence task, predicting polygonal boundaries to handle scale variations and blurred edges~\cite{liu2023polyformer}. However, existing methods effectively follow explicit instructions for target segmentation but lack implicit intent reasoning. In this paper, the proposed geospatial pixel reasoning task advances beyond referring segmentation by employing LLMs' reasoning capabilities to interpret subtle instructions and accurately segment desired targets.


% \subsection{Reasoning Segmentation}
\subsection{LLM-based Segmentation}

Recent advances in LLMs have significantly expanded their capabilities to integrate pixel-level segmentation with language reasoning~\cite{xiao2023florence, wang2023visionllm, wu2025visionllm, beyer2024paligemma, steiner2024paligemma, zhang2024next, yuan2025sa2va, he2024multi}. For instance, Florence-2~\cite{xiao2023florence} unified text, detection, and segmentation through a sequence-to-sequence framework with task instructions. To address the complexity of real-world segmentation scenarios, some works focus on architectural specialization and instruction-aware adaptation. LISA~\cite{lai2024lisa, yang2023lisa++} established the paradigm by introducing a \texttt{[SEG]} token to connect LLMs with segmentation decoders like SAM~\cite{kirillov2023segment}, enabling language-guided mask prediction. Subsequent studies enhanced this paradigm: GSVA~\cite{xia2024gsva} introduced shared-weight \texttt{[SEG]} tokens and \texttt{[REJ]} tokens for multi-target and empty-target handling~\cite{liu2023gres, ren2024pixellm, zhang2024groundhog}, while GLaMM~\cite{rasheed2024glamm} achieved pixel-grounded conversational capabilities through holistic segmentation~\cite{zhou2024instruction}. Parallel efforts focused on architectural unification - PSALM~\cite{zhang2024psalm} established a flexible input schema for multi-task segmentation, and OMG-LLaVA~\cite{zhang2025omg} combined universal segmentation backbones with LLMs for pixel-level reasoning. Video understanding extensions emerged through VISA~\cite{yan2024visa} and InstructSeg~\cite{wei2024instructseg}, which integrated temporal reasoning. Notably, Text4Seg~\cite{lan2024text4seg} redefined segmentation as a text generation problem using semantic descriptors, eliminating the need for an additional decoder. In remote sensing, benefiting from the above paradigms~\cite{lai2024lisa, lan2024text4seg}, some unified models such as RSUniVLM~\cite{liu2024rsunivlm}, GeoGround~\cite{zhou2024geoground} and GeoPix~\cite{ou2025geopix} are equipped with segmentation capabilities. Although based on LLM, these models focus only on explicit text-guided segmentation. Further, GeoPixel~\cite{shabbir2025geopixel} introduced grounded conversation generation~\cite{rasheed2024glamm} to remote sensing, but it still does not provide reasoning capability. Our SegEarth-R1 also follows the LLM-based segmentation paradigm, but is different from previous methods. Specifically, SegEarth-R1 is the first work to support reasoning about the target region from implicit queries, and its components are specifically designed for the challenges in remote sensing.

% the components in SegEarth-R1 are well-designed for the challenges in remote sensing.


\section{Benchmark Geospatial Pixel Reasoning Dataset---EarthReason}

\begin{table*}
  \caption{Comparison between EarthReason and other related datasets. The {\color{white!50!black}gray} rendering denotes the natural image dataset. ``Seg'', ``Det'', ``VG'', ``Cls'' denote segmentation, detection, visual grounding and classification datasets, respectively.}
  \label{table_data}
  \centering
  \scalebox{0.63}{
  \begin{tabular}{@{}lcccccccccc@{}}
    \toprule[1pt]
    Dataset & Mask Label & Reasoning Query & Spatial resolution & Image Size & Image Num & Image Source & Class Num \\
    \midrule[1pt]
    {\color{white!50!black}ReasonSeg} \cite{lai2024lisa} & {\color{white!50!black}\checkmark} & {\color{white!50!black}\checkmark} & {\color{white!50!black}-} & {\color{white!50!black}-} & {\color{white!50!black}1,218} & \makecell[c]{{\color{white!50!black}OpenImages (Seg) \&} \\ {\color{white!50!black}ScanNetv2 (Seg)}} & {\color{white!50!black}-} \\
    {\color{white!50!black}LLM-Seg40K} \cite{wang2024llm} & {\color{white!50!black}\checkmark} & {\color{white!50!black}\checkmark} & {\color{white!50!black}-} & {\color{white!50!black}-} & {\color{white!50!black}14,000} & \makecell[c]{{\color{white!50!black}LVIS (Seg) \&} \\ {\color{white!50!black}EgoObjects (Seg)}} & {\color{white!50!black}-} \\
    \midrule[1pt]
    EarthVQA~\cite{wang2024earthvqa} & \ding{55} & \checkmark & 0.3m & $1024^2$ & 6,000 & LoveDA (Seg) & 14 \\
    RegSegRS~\cite{yuan2024rrsis} & \checkmark & \ding{55} & 0.5m-30m & $800^2$ & 4,420 & SkyScapes (Seg) & 14 \\
    RRSIS-D~\cite{liu2024rotated} & \checkmark & \ding{55} & 0.13m & $512^2$ & 17,402 & RSVGD (VG) \& DIOR (OD) & 20 \\
    % VRSBench~\cite{li2024vrsbench} & \checkmark & \ding{55} & 
    RISBench~\cite{dong2024cross} & \checkmark & \ding{55} & 0.1m-30m & $512^2$ & 52,472 & DOTAv2(OD) \& DIOR (OD) & 26 \\
    \midrule
    EarthReason & \checkmark & \checkmark & 0.5m-153m & $123^2$-$7617^2$ & 5,434 & AID (Cls) \& fMoW (Cls) & 28  \\
    \bottomrule[1pt]
  \end{tabular}}
\end{table*}

% Million-AID

\subsection{Comparison with Related Dataset}


We analyze three types of tasks and datasets related to geospatial pixel reasoning, \ie, natural image reasoning segmentation, remote sensing visual question answering (VQA), and remote sensing referring segmentation, as shown in Table~\ref{table_data}.
RefSegRS~\cite{yuan2024rrsis} and RRSIS-D~\cite{liu2024rotated} provide early benchmarks with image-text-mask triplets. RISBench~\cite{dong2024cross}, the largest RRSIS dataset to date, introduced 52,472 triplets with oriented bounding boxes and pixel-level masks generated via a semi-automatic pipeline. These datasets address the limitations of earlier text-focused datasets (\eg, RSICD~\cite{lu2017exploring}, EarthVQA~\cite{wang2024earthvqa}, \etc) and enable comprehensive evaluation of multimodal models. Compared to the previous referring segmentation datasets, our EarthReason datasets has the following features: \textbf{(1)} The mask labels in EarthReason are not explicitly specified by the query, but require further reasoning to determine the target, which challenges the model's reasoning ability. \textbf{(2)} EarthReason uses a more raw data source. The previous related datasets directly transform existing segmentation datasets~\cite{azimi2019skyscapes, wang2021loveda} or SAM-processed detection datasets~\cite{zhan2023rsvg, li2020object, ding2021object}, while our EarthReason uses images from classification datasets~\cite{long2021creating, christie2018functional} and we manually annotate them. This allows EarthReason to provide more data gain when it comes to co-training of unified segmentation tasks. \textbf{(3)} EarthReason has more diverse spatial resolutions and image sizes, which are conducive to solving the object scale spanning problem inherent in remote sensing images~\cite{rolf2024mission}. Compared to the first natural image reasoning segmentation dataset, ReasonSeg, EarthReason contains $4.46\times$ more data than it. Therefore, we believe that EarthReason, as the first geospatial pixel reasoning dataset in the remote sensing area, is capable of performing initial explorations of this task.

\subsection{Dataset Generation Pipeline}
Our benchmark dataset EarthReason is generated according to the following three steps, i.e., image collection, question-answer pair generation, and object mask labeling.

\noindent
\textbf{Image Collection.} As mentioned above, to avoid potential data leakage in the future construction of unified segmentation models for remote sensing, we collect images from existing classification data. Although this increases the annotation cost, it also motivates more diverse scenes. Specifically, we first select the 28 categories that are more suitable for reasoning in the Million-AID~\cite{long2021creating} dataset, and sample about 200 images for each category. Then, we find that the actual geographic range contained in Million-AID's images is limited. Thus, we also collect 800 images in the fMoW~\cite{christie2018functional} dataset to enhance the model's reasoning ability in complex scenes. Further, to alleviate the factitious illusion issue~\cite{pang2024vhm}, we add an extra 200 empty target images (\ie, the implied target is not in the image). Finally, some low-quality images are eliminated, and we obtain a total of 5,434 images.


\noindent
\textbf{Question-Answer Pair Generation.} We use GPT-4o\footnote{\url{https://platform.openai.com/docs/models/gpt-4o}} to construct question-answer pairs, and given its excellent visual comprehension, we take the remote sensing image and the corresponding scene category (provided by Million-AID and fMoW) as part of the prompt to generate questions and answers that are closely related to the image. An example of such a prompt is illustrated in Appendix~\ref{sec:appendix_ann}. In addition, following~\cite{lai2024lisa}, to make the questions and answers diverse, we adapt GPT-3.5 to rephrase the instructional questions and answers, as shown in Appendix Figure~\ref{fig:data_gen2}.

\noindent
\textbf{Object Mask Labeling.} Different from previous referring and reasoning segmentation datasets (which use off-the-shelf masks or bounding boxes), we annotate images from scratch. Specifically, we employ multiple experts in remote sensing and vision, assign each expert a few hundred images to annotate, and cross-validate the annotations after they are completed. For simple targets (\eg, lake), SAM-H~\cite{kirillov2023segment} is used to assist in annotation; for complex targets (\eg, wind turbine), each point of the polygon is finely marked. A description of mask quality is provided in Appendix~\ref{sec:appendix_ann}.

% 参考分割和地理空间像素推理的比较。(a) RRSIS-D和RefSegRS数据集中的样本。(b) EarthReason数据集中的样本。
% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1.0\linewidth]{figures/task.pdf}
%    \caption{\small Comparison of referring segmentation and geospatial pixel inference. (left) Samples from the RRSIS-D~\cite{liu2024rotated} and RefSegRS~\cite{yuan2024rrsis} datasets. (right) Samples from the EarthReason dataset.}
%    \label{fig:data_sample}
%    \vspace{-1em}
% \end{figure}

% \subsection{Dataset Statistics}
\noindent
\textbf{Dataset Statistics.}
The EarthReason dataset is partitioned into training, validation, and testing sets, comprising 2,371, 1,135, and 1,928 images, respectively. In the training set, each image is annotated with an average of six questions and three corresponding answers. The average question length is 20.86 words, while the average answer length is 26.76 words. To assess the model’s generalization capability, several semantic categories are deliberately reserved for the validation and test sets, ensuring they remain unseen during training. Additional dataset details are provided in the Appendix~\ref{sec:appendix_ann_stat}.


\section{Baseline Geospatial Pixel Reasoning Method---SegEarth-R1}

Compared with natural images, remote sensing images exhibit distinctive characteristics that demand specialized architectural designs for pixel-wise geospatial reasoning. In this work, we propose SegEarth-R1, a simple yet powerful baseline for geospatial pixel reasoning that effectively harnesses LLM capabilities while incorporating domain-specific adaptations. As illustrated in Figure~\ref{fig:method}, our architecture comprises three core parts: A visual encoder for image feature extraction, an LLM for instruction interpretation and semantic correlation, and a mask generator for spatial correlation and mask prediction. Each part incorporates critical design considerations to address the unique challenges of remote sensing images.

\subsection{Hierarchical Visual Encoder}

\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/method2.pdf}
   \caption{\small Overview of the proposed SegEarth-R1 architecture. Given an image $X_v$ and a text description $X_q$, a hierarchical visual encoder and a proposed connector are used to extract and compress visual tokens. Then, the visual tokens \includegraphics[scale=0.004,valign=c]{figures/visual_token.png} and description embeddings \includegraphics[scale=0.004,valign=c]{figures/text_token.png} are fed into an LLM for instruction interpretation and semantic correlation. Finally, description embeddings are directly mapped to the query vector and used for spatial correlation and segmentation mask generation.}
   \label{fig:method}
   \vspace{-1em}
\end{figure}


Satellite and aerial targets present two critical challenges: (1) extreme scale variations ranging from sub-meter objects to kilometer-scale geographical formations~\cite{rolf2024mission}, and (2) densely distributed small objects requiring high-resolution analysis~\cite{li2024segearth}. Conventional ViT-based encoders adopted in MLLMs~\cite{lai2024lisa, yang2023lisa++, kirillov2023segment, xia2024gsva} (\eg, image encoder in CLIP~\cite{radford2021learning} and SAM~\cite{kirillov2023segment, ravi2024sam}) prove suboptimal due to their fixed-scale feature extraction and information compression through aggressive patch merging. To alleviate these limitations, following~\cite{zhang2024psalm}, SegEarth-R1 employs a Swin Transformer~\cite{liu2021swin} backbone enhanced with progressive feature hierarchy construction. This architecture generates multi-scale feature maps $v_h, h\in [1,4]$ at {1/4, 1/8, 1/16, 1/32} of the original resolution through controlled downsampling operations, preserving high-resolution details for small objects while capturing contextual semantics at deeper layers.

% The shifted window mechanism further enables cross-window connectivity without sacrificing computational efficiency, making it particularly suitable for analyzing large-scale geospatial images.

\subsection{Large Language Model and Input Schema}

SegEarth-R1 adopts the MLLM paradigm~\cite{liu2023visual, li2023blip} by jointly embedding visual tokens and textual instructions into a unified LLM input space for multimodal reasoning. Unlike natural images, remote sensing data exhibits ultra-high-resolution coverage~\cite{ji2023ultra, wang2025xlrs}, posing computational challenges when processed through billion-level LLMs. Therefore, we expect to compress the visual token to alleviate the computational cost and make only simple semantic correlations in LLM.

\subsubsection{Visual Token} 


\textbf{Redundancy Analysis.}
Image redundancy quantifies the proportion of compressible, non-informative data within an image. To investigate the feasibility of aggressive visual token compression for remote sensing images, we conduct a redundancy analysis from dual perspectives: pixel-level statistical redundancy and spatial structural redundancy.

% To confirm the feasibility of over-compressing visual tokens on remote sensing images, we analyze the redundancy of remote sensing images from two aspects.

\begin{itemize}[leftmargin=8pt]
\item According to information theory~\cite{shannon1948mathematical}, entropy measures the average uncertainty or information content of an image, while the maximum entropy corresponds to the idealized scenario where pixel values are uniformly distributed (\ie, no redundancy). Thus, from the entropy perspective, the image redundancy can be defined as~\cite{gonzales1987digital}:
\begin{equation}
\begin{aligned}
  R_e = 1 - \frac{-\sum_{l=0}^{L-1} p(l) \log _{2} p(l)}{\log _{2} L},
  \label{eq:entropy}
\end{aligned}
\end{equation}
where $L$ denotes the number of distinct intensity levels (\eg, $L=256$ for an 8-bit grayscale image), and $p(l)$ denotes the probability mass function of the pixel intensity value $l$.
\end{itemize}


\begin{itemize}[leftmargin=8pt]
\item Beyond pixel-level statistical redundancy, structural self-similarity reflects spatial redundancy caused by repetitive patterns (\eg, textures, geometric features). To quantify this, we leverage the Structural Similarity Index Matrix (SSIM)~\cite{wang2004image} to measure inter-patch similarity. For an image partitioned into $N$ patches, the SSIM matrix $\mathbf{M} \in \mathbb{R}^{N \times N}$ is defined as:
\begin{equation}
\begin{aligned}
\mathbf{M}(i,j) = \frac{(2\mu_i\mu_j + C_1)(2\sigma_{ij} + C_2)}{(\mu_i^2 + \mu_j^2 + C_1)(\sigma_i^2 + \sigma_j^2 + C_2)}, \quad \forall i,j \in {1, ..., N}
\label{eq:ssim}
\end{aligned}
\end{equation}
where $\mu_{i}$, $\sigma_{i}$ denote the mean and variance of the $i$-th patch, $\sigma_{ij}$ is the covariance between patches $i$ and $j$, and $ C_{1}$, $C_{2}$ are stability constants. Then, the structural self-similarity redundancy $R_{s}$ is derived by averaging off-diagonal elements of $\mathbf{M}$:
\begin{equation}
\begin{aligned}
R_s = \frac{1}{N(N-1)} \sum_{i \neq j} \mathbf{M}(i,j).
\label{eq:ssim_redundancy}
\end{aligned}
\end{equation}
\end{itemize}


\begin{wrapfigure}{r}{0.6\textwidth} 
  \centering
  \vspace{-5pt}
  \subfloat[pixel-level redundancy]{\label{fig:redundancy_a}
    \includegraphics[width=0.28\textwidth]{figures/redundancy_a.pdf}
  }
  \subfloat[spatial structure redundancy]{\label{fig:redundancy_b}
    \includegraphics[width=0.28\textwidth]{figures/redundancy_b.pdf}
  }
  \caption{\small Redundancy analysis of remote sensing datasets and natural images, and the former exhibits higher redundancy.}
  \label{fig:simg_pair}
  \vspace{-1em}
\end{wrapfigure}

We evaluate six benchmark datasets spanning natural images (COCO~\cite{caesar2018coco}, ADE20K~\cite{zhou2017scene}, PASCAL~\cite{everingham2010pascal}) and remote sensing images (LoveDA~\cite{wang2021loveda}, DeepGlobe~\cite{demir2018deepglobe}, xBD~\cite{gupta2019xbd}) for redundancy analysis. As shown in Figure~\ref{fig:simg_pair}, our analysis reveals two critical findings: 1) Remote sensing images demonstrate 1.9$\sim$3.3$\times$ higher entropic redundancy than natural images, indicating greater pixel-level compressibility. 2) The average self-similarity for remote sensing data exceeds natural images by 42.6\%, confirming the higher prevalence of repetitive textures and geometric patterns. This insight justifies aggressive token compression for semantic-level comprehension in remote sensing images.


\textbf{Token Compression Connector.}
In modern MLLM, connectors such as Q-Former~\cite{li2023blip} and MLP~\cite{liu2023visual} are designed to transform visual tokens into a multi-modal space. However, some works~\cite{cha2024honeybee, yao2024deco} point out that Q-Former may lead to loss of vision information and is difficult to train. Therefore, in SegEarth-R1, we follow the MLP connector fashion in LLaVA~\cite{liu2023visual} and use a simple but effective connector, \ie, stacked convolutional blocks and Layer Normalization (LN). Here, convolutional blocks are used for spatial down-sampling to compress the size of the feature map, and LN is used to stabilize cross-modal training. Specifically, our connector can be formulated as:
\begin{equation}
\begin{aligned}
v_{out}=(Conv \circ LN)^{d}(v_4),
\label{eq:connector}
\end{aligned}
\end{equation}
where $\circ$ denotes the function composition operator, and $d$ denotes the number of stacked layers.


\subsubsection{Text Instruction}


Although the instructions involved in geospatial pixel reasoning are implicit and contain more words than referring segmentation, they still maintain the same data format. Therefore, it is easy to convert them into question-answer pairs using a template like ``\textbf{USER}: This is an image <IMAGE>, please doing geospatial pixel reasoning according to the following instruction: <DESCRIPTION>. \textbf{ASSISTANT}: <ANSWER>''. For referring segmentation task, the task name in the instruction is changed to ``referring segmentation''.


\subsection{Mask Generation with Spatial Correlation}

Some recent LLM-based segmentation models~\cite{zhang2024psalm, wei2024instructseg} use Mask2Former~\cite{cheng2022masked} paradigm as mask generator. They use $T$ learnable mask tokens (typically $T$=100) as queries in the transformer decoder to generate $T$ candidate masks with corresponding scores, which are then assigned to the description embeddings\footnote{embeddings of <DESCRIPTION> in text instruction.} by bipartite matching. Unlike the reasoning segmentation of natural images, which is more inclined to make inferences based on the attributes of the object itself. In geospatial pixel reasoning, the model must understand and extrapolate more based on the spatial layout and inter-object correlations within the image (\eg, identifying earthquake evacuation zones in Figure~\ref{fig:method}, which requires analyzing topological relationships between roads and buildings). In addition, we posit that the mask query mechanism~\cite{cheng2021per, cheng2022masked} is inflexible and redundant in language-guided segmentation, and we only need to generate variable numbers of masks based on the instruction. Motivated by the above, we propose to directly use description embedding as the query for the mask generator and explicitly associate it with the image spatial features.


\begin{wrapfigure}{r}{0.25\textwidth}
    \vspace{-15pt}
    \centering
    \includegraphics[width=\linewidth]{figures/pool.pdf} 
    \vspace{-10pt}
    \caption{\small $D$-Projector.}
    \label{fig:pool}
    \vspace{-10pt}
\end{wrapfigure}


The length of description embeddings varies according to the user's instruction, whereas in our geospatial pixel reasoning or referring segmentation setting, the segmentation result can be represented by a single binary mask. Therefore, we introduce a description projection module ($D$-Projector), which converts the whole description into a single vector as shown in Figure~\ref{fig:pool}. Specifically, the description embeddings are averaged into a global vector, which is then interacted with the flattened multi-scale visual features via a cross-attention operation, and a skip-connection and linear layer maps it into the query vector. Following this, the query vector is fed into the Transformer decoder of Mask2Former, which is composed of stacked masked attention, self-attention and FFN. Notably, since the mask query mechanism has been removed, the number of generated masks is the same as the number of queries, and therefore, score prediction and bipartite matching are also not required. Finally, the predicted mask is supervised with a linear combination of focal loss~\cite{lin2017focal} and dice loss~\cite{milletari2016v}.


\section{Experiments}

\subsection{Settings}

\noindent
\textbf{Datasets and Tasks.} In addition to the geospatial pixel reasoning on EarthReason, we assess the basic geolocation capability with the explicit short query of our model. We employed two benchmark referring image segmentation datasets: RefSegRS~\cite{yuan2024rrsis} and RRSIS-D~\cite{liu2024rotated}. These datasets contain 14 and 20 semantic categories, respectively, with textual descriptions primarily focusing on direct visual attributes such as orientation, color, and size. All models are trained on their own training set and evaluated on their validation and testing sets.

% Notably, to evaluate the model's out-of-domain capability, we placed several categories exclusively in the validation and test sets, making them unseen during training. Further details can be found in the supplementary materials.



\noindent
\textbf{Evaluation metrics.} Following~\cite{lai2024lisa}, we adopt two evaluation metrics: \textbf{gIoU} (per-image IoU average) and cIoU (cumulative intersection over union). The latter is preferred because of its stability.

\noindent
\textbf{Network Architecture.} Unless otherwise specified, SegEarth-R1 use phi-1.5 (1.3B)~\cite{li2023textbooks} as the LLM, and adopt the Swin-B as the visual encoder. The token compression connector is configured with a layer number $d=2$. The mask generator follows the Mask2Former architecture, but removes mask tokens as mentioned above.

\noindent
\textbf{Implementation details.} During training, we use bf16 precision and freeze the visual encoder. The LLM is initialized from Phi-1.5, while both the Swin-B encoder and the mask generator are initialized with pretrained weights from Mask2Former. All images are resized to $1024 \times 1024$, maintaining the original aspect ratio by padding the shorter side. We adopt the AdamW optimizer with an initial learning rate of $1 \times 10^{-4}$, cosine learning rate schedule, and no weight decay. A uniform batch size of 16 is used across datasets, with training steps set to 7,610 (RRSIS-D), 5,400 (RefSegRS), and 2,220 (EarthReason). All experiments are conducted on two NVIDIA A100 80GB GPUs.

\subsection{Geospatial Pixel Reasoning Results}


\begin{wraptable}{l}{0.7\textwidth}
\vspace{-1em}
    \caption{\small Geospatial pixel reasoning results among SegEarth-R1 (ours) and previous related works.}
    \label{tab:earthreason}
    \centering
    \scalebox{0.75}{
    \begin{tabular}{@{}l|c|c|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Method} & \multirow{2}{*}{Visual Encoder} & \multirow{2}{*}{LLM Type} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & & & Val & Test & Val & Test \\
    \midrule[1pt]
    LISA~\cite{lai2024lisa} & CLIP-L & Vicuna-7B~\cite{chiang2023vicuna} & 57.39 & 59.10 & 61.04 & 60.88 \\
    PixelLM~\cite{ren2024pixellm} & CLIP-L & Vicuna-7B~\cite{chiang2023vicuna} & 57.79 & 59.22 & 57.94 & 60.01 \\
    % Text4Seg~\cite{lan2024text4seg} &  &  &  &  &  &  \\
    PSALM~\cite{zhang2024psalm} & Swin-B & phi-1.5 (1.3B)~\cite{li2023textbooks} & 62.03 & 64.61 & 66.61 & 68.30 \\
    \textit{SegEarth-R1} & Swin-B & phi-1.5 (1.3B)~\cite{li2023textbooks} & \textbf{64.13} & \textbf{68.25} & \textbf{68.60} & \textbf{70.75} \\
    \bottomrule[1pt]
\end{tabular}}
\vspace{-1em}
\end{wraptable}

We conduct a comparative evaluation of SOTA LLM-based methods and SegEarth-R1 on the EarthReason dataset. As shown in Table~\ref{tab:earthreason}, all models are trained solely on the training split of EarthReason to ensure a fair comparison. LISA and PixelLM demonstrate comparable performance; however, despite leveraging larger LLM or MLLM, the quality of their predicted segmentation masks remains suboptimal. This can be primarily attributed to their reliance on CLIP as the visual encoder, which tends to diminish the representation of small-scale geospatial targets. As one of the baselines of SegEarth-R1, PSALM achieves notable improvements over LISA and PixelLM. Nevertheless, PSALM does not adequately incorporate LLM-based segmentation and the Mask2Former paradigm, and lacks considerations for overhead images. SegEarth-R1 achieves the best results on both metrics surpassing PSALM by 3.64\% and 2.45\% on the test set. Importantly, SegEarth-R1 uses fewer visual tokens in LLM and reduces the number of queries in the mask generator, thus providing a lower inference cost.


\subsection{Referring Segmentation Results}


\begin{wraptable}{r}{0.55\textwidth}
    \vspace{-1em}
    \caption{\small Referring segmentation results among SegEarth-R1 and previous related works on RRSIS-D dataset.}
    \label{tab:RRSIS_D}
    \centering
    \scalebox{0.65}{
    \begin{tabular}{@{}l|c|cc|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Method} & & \multicolumn{2}{c|}{P@0.5} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & & Val & Test & Val & Test & Val & Test \\
    \midrule[1pt]
    \multicolumn{8}{@{}l}{\textbf{\textit{Traditional method:}}} \\
    RRN~\cite{li2018referring} & {\tiny CVPR'18} & 51.09 & 51.07 & 66.53 & 66.43 & 46.06 & 45.64 \\
    CSMC~\cite{ye2019cross} & {\tiny CVPR'19} & 55.68 & 55.32 & 69.39 & 69.39 & 48.85 & 48.54 \\
    LSCM~\cite{hui2020linguistic} & {\tiny ECCV'20} & 57.12 & 56.02 & 69.05 & 69.28 & 50.36 & 49.92 \\
    CMPC~\cite{huang2020referring} & {\tiny CVPR'20} & 57.93 & 55.83 & 69.22 & 69.39 & 50.41 & 49.24 \\
    BRINet~\cite{hu2020bi} & {\tiny CVPR'20} & 58.79 & 56.90 & 70.73 & 69.88 & 51.14 & 49.65 \\
    CMPC+~\cite{liu2021cross} & {\tiny TPAMI'20} & 59.19 & 57.65 & 70.14 & 68.64 & 51.41 & 50.24 \\
    LGCE~\cite{yuan2024rrsis} & {\tiny TGRS'24} & 68.10 & 67.65 & 76.68 & 76.34 & 60.16 & 59.37 \\
    RIS-DMMI~\cite{hu2023beyond} & {\tiny CVPR'23} & 70.40 & 68.74 & 77.01 & 76.20 & 60.72 & 60.12 \\
    LAVT~\cite{yang2022lavt} & {\tiny CVPR'22} & 69.54 & 69.52 & 77.59 & 77.19 & 61.46 & 61.04 \\
    RMSIN~\cite{liu2024rotated} & {\tiny CVPR'24} & 74.66 & 74.26 & 78.27 & 77.79 & 65.10 & 64.20 \\
    \midrule[1pt]
    \multicolumn{8}{@{}l}{\textbf{\textit{LLM-based method:}}} \\
    LISA~\cite{lai2024lisa} & {\tiny CVPR'24} & 27.07 & 24.51 & - & - & 27.84 & 26.78 \\
    PixelLM~\cite{ren2024pixellm} & {\tiny CVPR'24} & 33.46 & 28.81 & - & - & 33.89 & 31.65 \\
    NEXT-Chat~\cite{zhang2023next} & {\tiny arXiv'23} & 28.97 & 26.37 & - & - & 26.98 & 24.98 \\
    GeoGround~\cite{zhou2024geoground} & {\tiny arXiv'25} & 68.69 & 67.50 & - & - & 61.10 & 60.50 \\
    \midrule[1pt]
    \multicolumn{2}{@{}l|}{\model} & \textbf{78.62} & \textbf{76.96} & \textbf{78.92} & \textbf{78.01} & \textbf{67.56} & \textbf{66.40} \\
    \bottomrule[1pt]
    \end{tabular}}
    % \vspace{-1em}
\end{wraptable}


SegEarth-R1 also supports basic explicit language-guided segmentation. As shown in Table~\ref{tab:RRSIS_D}, we compare its performance with existing SOTA traditional methods (not based on LLM) as well as recent LLM-based methods. Notably, prior to SegEarth-R1, LLM-based methods consistently underperformed in comparison to traditional methods on the referring segmentation task. For instance, the advanced GeoGround~\cite{zhou2024geoground} lags behind RMSIN~\cite{liu2024rotated} by 3.7\% in terms of gIoU on the RRSIS-D dataset. In contrast, SegEarth-R1, as a universal LLM-based language-guided segmentation method, surpasses traditional methods on the referring segmentation task for the first time with a 2.2\% improvement. This result highlights the enhanced generalization capability and practical potential of SegEarth-R1. On the RefSegRS dataset, the improvement of SegEarth-R1 is more significant than the previous method, with an 8.33\% and 9.87\% improvement over RMSIN on the validation and testing sets, respectively, as listed in Table~\ref{tab:RefSegRS}.


\begin{table}[h]
    \caption{\small Referring segmentation results among SegEarth-R1 and previous related works on RefSegRS dataset.}
    \label{tab:RefSegRS}
    \centering
    \scalebox{0.66}{
    \begin{tabular}{@{}l|c|cc|cc|cc|cc|cc|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Method} & & \multicolumn{2}{c|}{P@0.5} & \multicolumn{2}{c|}{P@0.6} & \multicolumn{2}{c|}{P@0.7} & \multicolumn{2}{c|}{P@0.8} & \multicolumn{2}{c|}{P@0.9} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    % \hline
    & & Val & Test & Val & Test & Val & Test & Val & Test & Val & Test & Val & Test & Val & Test \\
    \midrule[1pt]
    % \multicolumn{15}{@{}l}{\textbf{\textit{Traditional method:}}} \\
    % \textbf{\textit{Traditional method:}} & & & & & & & & & & & & & & \\
    BRINet~\cite{hu2020bi} & {\tiny CVPR'20} & 36.86 & 20.72 & 35.53 & 14.26 & 19.93 & 9.87 & 10.66 & 2.98 & 2.84 & 1.14 & 61.59 & 58.22 & 38.73 & 31.51 \\
    LSCM~\cite{hui2020linguistic} & {\tiny ECCV'20} & 56.82 & 31.54 & 41.24 & 20.41 & 21.85 & 9.51 & 12.11 & 5.29 & 2.51 & 0.84 & 62.82 & 61.27 & 40.59 & 35.54 \\
    CMPC~\cite{huang2020referring} & {\tiny CVPR'20} & 46.09 & 32.36 & 26.45 & 14.14 & 12.76 & 6.55 & 7.42 & 1.76 & 1.39 & 0.22 & 63.55 & 55.39 & 42.08 & 40.63 \\
    CMSA~\cite{ye2019cross} & {\tiny CVPR'19} & 39.24 & 28.07 & 38.44 & 20.25 & 20.39 & 12.71 & 11.79 & 5.61 & 1.52 & 0.83 & 65.84 & 64.53 & 43.62 & 41.47 \\
    RRN~\cite{li2018referring} & {\tiny CVPR'18} & 55.43 & 30.26 & 42.98 & 23.01 & 23.11 & 14.87 & 13.72 & 7.17 & 2.64 & 0.98 & 69.24 & 65.06 & 50.81 & 41.88 \\
    EVF-SAM~\cite{zhang2024evf} & {\tiny Arxiv'24} & 57.77 & 35.17 & 37.59 & 22.34 & 16.24 & 9.36 & 4.87 & 2.86 & 1.86 & 0.39 & 59.61 & 55.51 & 46.98 & 36.64 \\
    CMPC+~\cite{liu2021cross} & {\tiny TPAMI'21} & 56.84 & 49.19 & 37.59 & 28.31 & 20.42 & 15.31 & 10.67 & 8.12 & 2.78 & 0.55 & 70.62 & 66.53 & 47.13 & 43.65 \\
    CARIS~\cite{liu2023caris} & {\tiny ACMMM'23} & 68.45 & 45.40 & 47.10 & 27.19 & 25.52 & 15.08 & 14.62 & 8.87 & 3.71 & 1.98 & 75.79 & 69.74 & 54.30 & 42.66 \\
    CRIS~\cite{wang2022cris} & {\tiny CVPR'22} & 53.13 & 35.77 & 36.19 & 24.11 & 24.36 & 14.36 & 11.83 & 6.38 & 2.55 & 1.21 & 72.14 & 65.87 & 53.74 & 43.26 \\
    LAVT~\cite{yang2022lavt} & {\tiny CVPR'22} & 80.97 & 51.84 & 58.70 & 30.27 & 31.09 & 17.34 & 15.55 & 9.52 & 4.64 & 2.09 & 78.50 & 71.86 & 61.53 & 47.40 \\
    RIS-DMMI~\cite{hu2023beyond} & {\tiny CVPR'23} & 86.17 & 63.89 & 74.71 & 44.30 & 38.05 & 19.81 & 18.10 & 6.49 & 3.25 & 1.00 & 74.02 & 68.58 & 65.72 & 52.15 \\
    LGCE~\cite{yuan2024rrsis} & {\tiny TGRS'24} & 90.72 & 73.75 & 86.31 & 61.14 & 71.93 & 39.46 & 32.95 & 16.02 & 10.21 & 5.45 & 83.56 & 76.81 & 72.51 & 59.96 \\
    RMSIN~\cite{liu2024rotated} & {\tiny CVPR'24} & 93.97 & 79.20 & 89.33 & 65.99 & 74.25 & 42.98 & 29.70 & 16.51 & 7.89 & 3.25 & 82.41 & 75.72 & 73.84 & 62.58 \\
    SegEarth-R1 & & \textbf{95.82} & \textbf{86.30} & \textbf{93.27} & \textbf{79.53} & \textbf{88.86} & \textbf{69.57} & \textbf{78.19} & \textbf{48.87} & \textbf{22.04} & \textbf{10.73} & \textbf{85.01} & \textbf{79.00} & \textbf{82.17} & \textbf{72.45} \\
    \bottomrule[1pt]
\end{tabular}}
\end{table}

\subsection{Ablation Study}

\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/res.pdf}
   \caption{\small Qualitative Results of SegEarth-R1 on EarthReason. More results can be found in  Appendix~\ref{sec:appendix_examples}.}
   \label{fig:res}
   \vspace{-1em}
\end{figure}


\begin{minipage}{\textwidth}
\footnotesize
\begin{minipage}[t]{0.55\textwidth}
% 原有左侧表格代码保持不变
\makeatletter\def\@captype{table}
\caption{\small Ablation of SegEarth-R1 components on EarthReason: query description embedding (Query D.E.), description projector ($D$-Projector), token compression connector (T.C. Connector).}
\label{tab:ablation}
\centering
\scalebox{0.7}{
\begin{tabular}{@{}c|c|c|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{Query D.E.} & \multirow{2}{*}{$D$-Projector} & \multirow{2}{*}{T.C. Connector} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & & & Val & Test & Val & Test \\
    \midrule[1pt]
    \ding{55} & \ding{55} & \ding{55} & 62.03 & 64.61 & 66.61 & 68.30 \\
    \Checkmark & \ding{55} & \ding{55} & 63.34 & 66.19 & 67.42 & 69.15 \\
    \ding{55} & \Checkmark & \ding{55} & 63.32 & 66.31 & 67.22 & 69.21 \\
    \ding{55} & \ding{55} & \Checkmark & 63.47 & 65.41 & 68.31 & 69.20 \\
    \Checkmark & \Checkmark & \ding{55} & 64.12 & 66.71 & \textbf{68.61} & 69.61 \\
    \Checkmark & \Checkmark & \Checkmark & \textbf{64.13} & \textbf{68.25} & 68.60 & \textbf{70.75} \\
    \bottomrule[1pt]
\end{tabular}}
\end{minipage}
% \hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
% 右侧改为垂直排列的两个表格
\begin{minipage}[t][0.105\textheight][t]{\textwidth}
\makeatletter\def\@captype{table}
\caption{\small Ablation of LLM type on RRSIS-D.}
\label{table:ablation_llm}
\centering
\scalebox{0.75}{
\begin{tabular}{@{}l|cc|cc@{}}
    \toprule[1pt]
    \multirow{2}{*}{LLM Type} & \multicolumn{2}{c|}{cloU} & \multicolumn{2}{c}{gloU} \\
    & Val & Test & Val & Test \\
    \midrule[1pt]
    phi-1.5 (1.3B) & 78.92 & 78.01 & 67.56 & 66.40 \\
    phi-2 (2B) & \textbf{78.98} & \textbf{78.35} & \textbf{67.91} & \textbf{66.67} \\
    Qwen2.5 (0.5B) & 78.53 & 77.87 & 67.70 & 66.49 \\
    \bottomrule[1pt]
\end{tabular}}
\end{minipage}
% \vspace{0.5cm} % 调整垂直间距
\begin{minipage}[t][0.01\textheight][t]{\textwidth} % 新增第三个表格
\makeatletter\def\@captype{table}
\caption{\small Ablation of $d$ on EarthReason Val set.}
\label{table:ablation_d}
\centering
\scalebox{0.7}{
\begin{tabular}{@{}c|c|c||c|c|c@{}}
    \toprule[1pt]
    $d$ & \#Visual Token & gIoU & $d$ & \#Visual Token & gIoU \\
    \midrule[1pt]
    0 & 1024 & 68.28 & \textbf{2} & \textbf{64} & \textbf{68.60} \\
    1 & 256 & 68.47 & 3 & 16 & 68.22 \\
    \bottomrule[1pt]
\end{tabular}}
\end{minipage}
\end{minipage}
\end{minipage}




\noindent
\textbf{Components.}
We conduct ablation studies on the EarthReason dataset to evaluate the effectiveness of the novel components involved in SegEarth-R1. As listed in Table~\ref{tab:ablation}, the first row shows the results of the PSALM baseline. Each proposed component contributes to performance enhancement, yielding improvements ranging from 0.85\% to 0.9\%. The T.C. Connector and Query D.E. not only enhances performance but also reduces computational overhead. Further, the proposed components can be well coupled, and when they are all activated, \ie, complete SegEarth-R1, all metrics exhibit substantial gains over the baseline, confirming the effectiveness and compatibility of the proposed design. In fact, although these components are initially designed with remote sensing scenarios in mind, their underlying principles offer transferable insights applicable to general image understanding.

\noindent
\textbf{LLM Type.}
Given the limited scale of the dataset, we select some small LLM for comparation, as presented in Table~\ref{table:ablation_llm}. SegEarth-R1 demonstrates consistently high performance across different LLM, indicating the robustness and architectural stability of the overall framework. Notably, with Qwen2.5 (0.5B)~\cite{yang2024qwen2}, it still achieves competitive results, indicating its potential for edge deployment.

\noindent
\textbf{Layer Number of T.C. Connector.}
The layer number $d$ controls the number of visual tokens fed into the LLM. As shown in Table~\ref{table:ablation_d}, increasing token quantity does not improve performance. This observation aligns with our earlier analysis, suggesting that appropriate compression of visual tokens is beneficial for the global understanding of a remote sensing image. In SegEarth-R1, spatial correlations between the image and the instruction are primarily handled by the mask generator, while the LLM is only responsible for relatively semantic correlations. This division of labor allows for more efficient use of computational resources without compromising performance.

\section{Conclusion}

In this paper, we introduce geospatial pixel reasoning, a new task in remote sensing that requires models to infer segmentation masks from implicit natural language queries by reasoning over spatial context and domain knowledge. To enable research in this direction, we present EarthReason, the first large-scale benchmark dataset that emphasises complex reasoning scenarios. To address the distinct challenges inherent in remote sensing, we propose SegEarth-R1, a language-guided segmentation model that integrates a hierarchical visual encoder, an LLM for instruction parsing and semantic correlation, and a tailored mask generator designed for spatial correlation. Extensive experiments validate SegEarth-R1’s superiority, achieving SOTA performance on both geospatial pixel reasoning and referring segmentation tasks. This work pioneers the fusion of natural language reasoning with pixel-level geospatial analysis, offering transformative potential for applications like environmental monitoring and disaster response.


\bibliography{main}
\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\appendix

\section{Data}

\subsection{Annotation of EarthReason}
\label{sec:appendix_ann}

Each sample of the EarthReason benchmark consists of an image, a corresponding mask, and six reasoning queries along with their respective answers. Given that our metadata is derived from classification datasets, we employed GPT-4o and GPT-3.5 to generate textual annotations, and invited multiple remote sensing and vision experts to provide accurate and reliable mask annotations. Overall, our annotation process consists of the following three steps:

\begin{itemize}[leftmargin=1.5em]
    \item Step-1: To fully leverage the powerful multimodal capabilities and extensive geographic knowledge of GPT-4o, we carefully design the prompt, which is then provided alongside images and their corresponding category labels to generate a reasoning question–answer pair. The prompt is illustrated in Figure~\ref{fig:data_gen1}. 
    \item Step-2: To avoid homogeneous question–answer formats under a single prompt, we further employ the textual capabilities of GPT-3.5 to expand each generated question into six variations and each answer into three alternatives. The prompt used for this expansion is shown in Figure~\ref{fig:data_gen2}.
    \item Step-3: Unlike previous methods that rely on semi-automatic mask annotation based on off-the-shelf bounding boxes or masks, we invite multiple remote sensing vision experts to perform accurate and efficient mask annotation guided by the generated questions. To further improve annotation efficiency, we incorporate SAM-H as an auxiliary tool for some simple targets. Subsequently, we perform cross-validation of the annotation results and re-annotate the samples that do not meet the quality standards. As shown in Figure~\ref{fig:dataset_compare}, (a), (b), and (c), derived from the RRSIS-D dataset, illustrate the masks of semi-automatic annotation based on bounding boxes. (a) and (c) exhibit noticeable annotation errors, while in (b), the query does not align with the annotation. (d), (e), and (f) illustrate our high-quality manual annotations.
\end{itemize}



%Our metadata is derived from a remote sensing image classification dataset, which provides category labels corresponding to each image. To obtain high-quality and diverse annotations, we carefully designed prompts to leverage the powerful multimodal capabilities of GPT-4o%

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/data_gen3.pdf}
   \caption{\small The illustration of the prompt construction process for generating question-answer pairs for geospatial pixel reasoning.}
   \label{fig:data_gen1}
   % \vspace{-1em}
\end{figure}

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/data_gen4.pdf}
   \caption{\small The illustration of the prompt construction process for expand question-answer pairs.}
   \label{fig:data_gen2}
   % \vspace{-1em}
\end{figure}

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/quality.pdf}
   \caption{\small Comparison of annotation quality. (a), (b) and (c) are from RRSIS-D dataset, (d), (e) and (f) are from our EarthReason dataset.}
   \label{fig:dataset_compare}
   % \vspace{-1em}
\end{figure}





\subsection{EarthReason Statistics}
\label{sec:appendix_ann_stat}

The EarthReason benchmark comprises 28 categories, and the number of samples in each category is shown in Figure~\ref{fig:category} (a). It can be observed that the distribution of the 28 categories is relatively balanced. Figure~\ref{fig:category} (b), (c), and (d) illustrate the category distributions in the training, validation, and test sets, respectively. To evaluate the model's generalization capability, we specifically excluded four categories—``basketball court'', ``island'', ``lake'', and ``stadium''—from the training set. Moreover, we introduced 119 empty target samples to mitigate potential hallucinations of the model.
% In addition, we also analyze the distribution of question lengths in the benchmark, as shown in Figure 9(b).

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/category.pdf}
   \caption{\small The category distribution of EarthReason.}
   \label{fig:category}
   % \vspace{-1em}
\end{figure}

\section{Additional Implementation Details}
\label{sec:appendix_implementation}

\subsection{Details of Training Hyper-parameters}
Table~\ref{tab:hyperparam_training} presents the hyper-parameter settings used during the training of our model. For training on the referring segmentation datasets, we employ only focal loss and dice loss to supervise mask generation. In contrast, for training on geospatial pixel reasoning task, we additionally incorporate the cross-entropy loss from the large language model to supervise text answer generation.

\begin{table}[h]
\centering
\caption{The hyper-parameters for model training.}
\label{tab:hyperparam_training}
\begin{tabular}{ll}
\toprule
\textbf{Parameters} & \textbf{Value} \\
\midrule
Optimizer & AdamW \\
Learning Rate & $1 \times 10^{-4}$ \\
Batch Size & 16 \\
Number of Iteration & 7,610 / 5,400 / 2,220 \\
Learning Rate Schedule & Cosine Decay \\
Weight Decay & 0.0 \\
Warmup Ratio & 0.03 \\
$\beta_1$ & 0.9 \\
$\beta_2$ & 0.999 \\
Image Size & 1024 $\times$ 1024 \\
Image Processing & \begin{tabular}[t]{@{}l@{}}Resize long edge to 1024 \\ and padding short edge to 1024.\end{tabular} \\
\bottomrule
\end{tabular}
\end{table}

\section{Examples}
\label{sec:appendix_examples}

\subsection{More Qualitative Results on EarthReason}

Figure~\ref{fig:vis_reason} presents a comparison between SegEarth-R1 and other models on the EarthReason dataset. It can be observed that our model demonstrates a better understanding of long reasoning instructions and produces more accurate mask generation.

\subsection{More Qualitative Results on RRSIS-D}
Figure~\ref{fig:vis_ref} presents a comparison between SegEarth-R1 and PSALM on the RRSIS-D dataset. Our model demonstrates a better understanding of direct geographical attributes such as location, color, and size compared to PSALM. This improvement is attributed to the removal of indirect mask prediction using mask tokens, allowing semantic information (description embeddings) to directly interact with image features to generate masks.

\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/vis_reason.pdf}
   \caption{\small Comparison with other models on EarthReason.}
   \label{fig:vis_reason}
   % \vspace{-1em}
\end{figure}


\begin{figure}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/vis_ref.pdf}
   \caption{\small Comparison with PSALM on RRSIS-D.}
   \label{fig:vis_ref}
   % \vspace{-1em}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
% \section*{NeurIPS Paper Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
% limit. 

% Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% \begin{itemize}
%     \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
%     \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
%    % \item {\bf The papers not including the checklist will be desk rejected.}
% \end{itemize}

% {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% IMPORTANT, please:
% \begin{itemize}
%     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist"},
%     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
% \end{itemize} 
 

% %%% END INSTRUCTIONS %%%


% \begin{enumerate}

% \item {\bf Claims}
%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
%         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
%         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
%     \end{itemize}

% \item {\bf Limitations}
%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
%         \item The authors are encouraged to create a separate "Limitations" section in their paper.
%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
%     \end{itemize}

% \item {\bf Theory assumptions and proofs}
%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include theoretical results. 
%         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.
%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
%         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
%     \end{itemize}

%     \item {\bf Experimental result reproducibility}
%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
%         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
%         \begin{enumerate}
%             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
%         \end{enumerate}
%     \end{itemize}


% \item {\bf Open access to data and code}
%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that paper does not include experiments requiring code.
%         \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
%     \end{itemize}


% \item {\bf Experimental setting/details}
%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
%         \item The full details can be provided either with the code, in appendix, or as supplemental material.
%     \end{itemize}

% \item {\bf Experiment statistical significance}
%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
%         \item The assumptions made should be given (e.g., Normally distributed errors).
%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
%     \end{itemize}

% \item {\bf Experiments compute resources}
%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
%         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
%     \end{itemize}
    
% \item {\bf Code of ethics}
%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
%     \end{itemize}


% \item {\bf Broader impacts}
%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that there is no societal impact of the work performed.
%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
%     \end{itemize}
    
% \item {\bf Safeguards}
%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper poses no such risks.
%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
%         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
%     \end{itemize}

% \item {\bf Licenses for existing assets}
%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not use existing assets.
%         \item The authors should cite the original paper that produced the code package or dataset.
%         \item The authors should state which version of the asset is used and, if possible, include a URL.
%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
%     \end{itemize}

% \item {\bf New assets}
%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not release new assets.
%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
%         \item The paper should discuss whether and how consent was obtained from people whose asset is used.
%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
%     \end{itemize}

% \item {\bf Crowdsourcing and research with human subjects}
%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
%         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
%     \end{itemize}

% \item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
%         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
%         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
%     \end{itemize}

% \item {\bf Declaration of LLM usage}
%     \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
%     %this research? 
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.
%         \item Please refer to our LLM policy (\url{https://neurips.cc/Conferences/2025/LLM}) for what should or should not be described.
%     \end{itemize}

% \end{enumerate}


\end{document}